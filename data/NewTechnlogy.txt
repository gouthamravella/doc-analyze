TECH;
How Blockchain Technology Works. Guide for Beginners

Nearly everyone has heard of Blockchain and that it is cool. But not everybody understands how it works. This article shows that Blockchain certainly isnt magic.
What is Blockchain? 
A Blockchain is a diary that is almost impossible to forge.
Hash function 
Let's imagine that 10 people in one room decided to make a separate currency. They have to follow the flow of funds, and one person  let's call him Bob  decided to keep a list of all actions in a diary:

One man  lets call him Jack  decided to steal money. To hide this, he changed the entries in the diary:

Bob noticed that someone had interfered with his diary. He decided to stop this from happening.
He found a program called a Hash function that turns text into a set of numbers and letters as in the table below.

A hash is a string of numbers and letters, produced by hash functions. A hash function is a mathematical function that takes a variable number of characters and converts it into a string with a fixed number of characters. Even a small change in a string creates a completely new hash.
After each record, he inserted a hash. The new diary was as follows:

Jack decided to change entries again. At night, he got to the diary, changed the record and generated a new hash.

Bob noticed that somebody had sifted through the diary again. He decided to complicate the record of each transaction. After each record, he inserted a hash generated from the record+last hash. So each entry depends on the previous.

If Jack tries to change the record, he will have to change the hash in all previous entries. But Jack really wanted more money, and he spent the whole night counting all the hashes.
Nonce 
But Bob did not want to give up. He decided to add a number after each record. This number is called Nonce. Nonce should be chosen so that the generated hash ends in two zeros.

Now, to forge records, Jack would have to spend hours and hours chosing Nonce for each line.
More importantly, not only people, but computers cant figure out the Nonce quickly.
Nodes 
Later, Bob realized that there were too many records and that he couldnt keep the diary like this forever. So when he wrote 5,000 transactions, he converted them to a one page spreadsheet. Mary checked that all transactions were right.
Bob spread his spreadsheet diary over 5,000 computers, which were  all over the world. These computers are called nodes. Every time a transaction occurs it has to be approved by the nodes, each of whom checks its validity. Once every node has checked a transaction there is a sort of electronic vote, as some nodes may think the transaction is valid and others think it is a fraud.
The nodes referred to above are computers. Each node has a copy of the digital ledger or Blockchain. Each node checks the validity of each transaction. If a majority of nodes say that a transaction is valid then it is written into a block.
Now, if Jack change one entry, all the other computers will have the original hash. They would not allow the change to occur.
Block 
This one spreadsheet is called a block .The whole family of blocks is the Blockchain. Every node has a copy of the Blockchain. Once a block reaches a certain number of approved transactions then a new block is formed.
The Blockchain updates itself every ten minutes. It does so automatically. No master or central computer instructs the computers to do this.
As soon as the spreadsheet or ledger or registry is updated, it can no longer be changed. Thus, its impossible to forge it. You can only add new entries to it. The registry is updated on all computers on the network at the same time.
Important points: 
1.	A Blockchain is a type of diary or spreadsheet containing information about transactions.
2.	Each transaction generates a hash.
3.	A hash is a string of numbers and letters.
4.	Transactions are entered in the order in which they occurred. Order is very important.
5.	The hash depends not only on the transaction but the previous transaction's hash.
6.	Even a small change in a transaction creates a completely new hash.
7.	The nodes check to make sure a transaction has not been changed by inspecting the hash.
8.	If a transaction is approved by a majority of the nodes then it is written into a block.
9.	Each block refers to the previous block and together make the Blockchain.
10.	A Blockchain is effective as it is spread over many computers, each of which have a copy of the Blockchain.
11.	These computers are called nodes.
12.	The Blockchain updates itself every 10 minutes.
Wallets, digital signatures, protocols 
Bob gathered the 10 people together. He needed to explain the new coin to them.
Jack had confessed his sins to the group and deeply apologized. To prove his sincerity he gave Ann and Mary their coins back.
With all that sorted, Bob explained why this could never happen again. He decided to implement something called a digital signature to confirm every transaction. But first, he gave everyone a wallet.
What is a wallet? 
A wallet is a string of numbers and letters, such as 18c177926650e5550973303c300e136f22673b74. This is an address that will appear in various blocks within the Blockchain as transactions take place. No visible records of who did what transaction with who, only the number of a wallet. The address of each particular wallet is also a public key.
Read more in the article Bitcoin Wallets: everything you need to know.
Digital signature 
To carry out a transaction you need two things: a wallet, which is basically an address, and a private key. The private key is a string of random numbers, but unlike the address the private key must be kept secret.
When someone decides to send coins to anyone else they must sign the message containing the transaction with their private key. The system of two keys is at the heart of encryption and cryptography, and its use long predates the existence of Blockchain. It was first proposed in the 1970s.
Once the message is sent it is broadcast to the Blockchain network. The network of nodes then works on the message to make sure that the transaction it contains is valid. If it confirms the validity, the transaction is placed in a block and after that no information about it can be changed.

TECH:
How Does the Blockchain Work?
Blockchain technology explained in simple words
Follow
Jun 1, 2016
Credit: Ani_Ka/DigitalVision Vectors/Getty
8.1K
Blockchain technology is probably the best invention since the internet itself. It allows value exchange without the need for trust or a central authority. Imagine you and I bet $50 on tomorrows weather in San Francisco. I bet it will be sunny, you that it will rain. Today we have three options to manage this transaction:
1.	We can trust each other. Rainy or sunny, the loser will give $50 to the winner. If we are friends, this could be a good way of managing it. However, friends or strangers, one can easily not pay the other.
2.	We can turn the bet into a contract. With a contract in place both parties will be more prone to pay. However, should either of the two decide not to pay, the winner will have to pay additional money to cover legal expenses and the court case might take a long time. Especially for a small amount of cash, this doesnt seem like the optimal way to manage the transaction.
3.	We can involve a neutral third party. Each of us gives $50 to a third party, who will give the total amount to the winner. But hey, she could also run away with all our money. So we end up with one of the first two options: trust or contract.
Neither trust nor contract is an optimal solution: We cant trust strangers, and enforcing a contract requires time and money. The blockchain technology is interesting because it offers us a third option which is secure, quick, and cheap.
Blockchain allows us to write a few lines of code, a program running on the blockchain, to which both of us send $50. This program will keep the $100 safe and check tomorrows weather automatically on several data sources. Sunny or rainy, it will automatically transfer the whole amount to the winner. Each party can check the contract logic, and once its running on the blockchain it cant be changed or stopped. This may be too much effort for a $50 bet, but imagine selling a house or a company.
This piece explains how the blockchain works without discussing the technical details in depth, but by digging just enough to give you a general idea of the underlying logic and mechanisms.
________________________________________
The Basics of Bitcoin

Images courtesy of author.
The most known and discussed application of the blockchain technology is bitcoin, a digital currency that can be used to exchange products and services, just like the U.S. dollar, euro, Chinese yuan, and other national currencies. Lets use this first application of the blockchain technology to learn how it works.
Bitcoin gives us, for the first time, a way for one Internet user to transfer a unique piece of digital property to another Internet user, such that the transfer is guaranteed to be safe and secure, everyone knows that the transfer has taken place, and nobody can challenge the legitimacy of the transfer. The consequences of this breakthrough are hard to overstate.
 Marc Andreessen
________________________________________
One bitcoin is a single unit of the Bitcoin (BTC) digital currency. Just like a dollar, a bitcoin has no value by itself; it has value only because we agree to trade goods and services to bring more of the currency under our control, and we believe others will do the same.
To keep track of the amount of bitcoin each of us owns, the blockchain uses a ledger, a digital file that tracks all bitcoin transactions.

Fig. 1 - Bitcoin ledger digital file simplified
The ledger file is not stored in a central entity server, like a bank, or in a single data center. It is distributed across the world via a network of private computers that are both storing data and executing computations. Each of these computers represents a node of the blockchain network and has a copy of the ledger file.
If David wants to send bitcoins to Sandra, he broadcasts a message to the network that says the amount of bitcoin in his account should go down by 5 BTC, and the amount in Sandras account should increase by the same quantity. Each node in the network will receive the message and apply the requested transaction to its copy of the ledger, updating the account balances.

Fig. 2 - Transaction request message simplified
The fact that the ledger is maintained by a group of connected computers rather than by a centralized entity like a bank has several implications:
	In our bank system we only know our own transactions and account balances; on the blockchain everyone can see everyone elses transactions.
	While you can generally trust your bank, the bitcoin network is distributed and if something goes wrong there is no help desk to call or anyone to sue.
	The blockchain system is designed in such a way that no trust is needed; security and reliability are obtained via special mathematical functions and code.
________________________________________
We can define the blockchain as a system that allows a group of connected computers to maintain a single updated and secure ledger. In order to perform transactions on the blockchain, you need a wallet, a program that allows you to store and exchange your bitcoins. Since only you should be able to spend your bitcoins, each wallet is protected by a special cryptographic method that uses a unique pair of distinct but connected keys: a private and a public key.
If a message is encrypted with a specific public key, only the owner of the paired private key can decrypt and read the message. The reverse is also true: If you encrypt a message with your private key, only the paired public key can decrypt it. When David wants to send bitcoins, he needs to broadcast a message encrypted with the private key of his wallet. As David is the only one who knows the private key necessary to unlock his wallet, he is the only one who can spend his bitcoins. Each node in the network can cross-check that the transaction request is coming from David by decrypting the message with the public key of his wallet.
When you encrypt a transaction request with your wallets private key, you are generating a digital signature that is used by blockchain computers to verify the source and authenticity of the transaction. The digital signature is a string of text resulting from your transaction request and your private key; therefore it cannot be used for other transactions. If you change a single character in the transaction request message, the digital signature will change, so no potential attacker can change your transaction requests or alter the amount of bitcoin you are sending.

Fig. 3 - Digital Signature transaction encryption simplified
To send bitcoin you need to prove that you own the private key of a specific wallet as you need the key to encrypt your transaction request message. Since you broadcast the message only after it has been encrypted, you never have to reveal your private key.
Tracking Your Wallet Balance
Each node in the blockchain is keeping a copy of the ledger. So, how does a node know your account balance? The blockchain system doesnt keep track of account balances at all; it only records each and every transaction that is verified and approved. The ledger in fact does not keep track of balances, it only keeps track of every transaction broadcasted within the bitcoin network (Fig. 4). To determine your wallet balance, you need to analyze and verify all the transactions that ever took place on the whole network connected to your wallet.

Fig. 4 - Blockchain Ledger
This balance verification is performed based on links to previous transactions. In order to send 10 bitcoins to John, Mary has to generate a transaction request that includes links to previous incoming transactions that add up to at least 10 bitcoins. These links are called inputs. Nodes in the network verify the amount and ensure that these inputs havent been spent yet. In fact, each time you reference inputs in a transaction, they are deemed invalid for any future transaction. This is all performed automatically in Marys wallet and double-checked by the bitcoin network nodes; she only sends a 10 BTC transaction to Johns wallet using his public key.

Fig. 5 - Blockchain transaction request structure
So, how can the system trust that input transactions are valid? It checks all the previous transactions correlated to the wallet you use to send bitcoins via the input references. To speed up the verification process, a special record of unspent transactions is kept by the network nodes. Thanks to this security check, it is not possible to double-spend bitcoins.
Owning bitcoins means that there are transactions written in the ledger that point to your wallet address and havent been used as inputs yet. All the code to perform transactions on the bitcoin network is open source; this means that anyone with a laptop and an internet connection can operate transactions. However, should there be a mistake in the code used to broadcast a transaction request message, the associated bitcoins will be permanently lost.
Remember that since the network is distributed, there is no customer support to call nor anyone who could help you restore a lost transaction or forgotten wallet password. For this reason, if you are interested in transacting on the bitcoin network, its a good idea to use the open source and official version of bitcoin wallet software (such as Bitcoin Core), and to store your wallets password or private key in a very safe repository.
But Is It Really Safe? And Why Is It Called Blockchain?
Anyone can access the bitcoin network via an anonymous connection (for example, the TOR network or a VPN network), and submit or receive transactions revealing nothing more than his public key. However if someone uses the same public key over and over, its possible to connect all the transactions to the same owner. The bitcoin network allows you to generate as many wallets as you like, each with its own private and public keys. This allows you to receive payments on different wallets, and there is no way for anyone to know that you own all these wallets private keys, unless you send all the received bitcoins to a single wallet.
The total number of possible bitcoin addresses is 6 or 1461501637330902918203684832716283019655932542976.
This large number protects the network from possible attacks while allowing anyone to own a wallet.
With this setup, there is still a major security hole that could be exploited to recall bitcoins after spending them. Transactions are passed from node to node within the network, so the order in which two transactions reach each node can be different. An attacker could send a transaction, wait for the counterpart to ship a product, and then send a reverse transaction back to his own account. In this case, some nodes could receive the second transaction before the first and therefore consider the initial payment transaction invalid, as the transaction inputs would be marked as already spent. How do you know which transaction has been requested first? Its not secure to order the transactions by timestamp because it could easily be counterfeit. Therefore, there is no way to tell if a transaction happened before another, and this opens up the potential for fraud.
If this happens, there will be disagreement among the network nodes regarding the order of transactions each of them received. So the blockchain system has been designed to use node agreement to order transactions and prevent the fraud described above.
The bitcoin network orders transactions by grouping them into blocks; each block contains a definite number of transactions and a link to the previous block. This is what puts one block after the other in time. Blocks are therefore organized into a time-related chain (Fig. 6) that gives the name to the whole system: blockchain.

Fig. 6??The block chain sequence structure simplified
Transactions in the same block are considered to have happened at the same time, and transactions not yet in a block are considered unconfirmed. Each node can group transactions into a block and broadcast it to the network as a suggestion for which block should be next. Since any node can suggest a new block, how does the system agree on which block should be the next?
To be added to the blockchain, each block must contain the answer to a complex mathematical problem created using an irreversible cryptographic hash function. The only way to solve such a mathematical problem is to guess random numbers that, combined with the previous block content, generate a defined result. It could take about a year for a typical computer to guess the right number and solve the mathematical problem. However, due to the large number of computers in the network that are guessing numbers, a block is solved on average every 10 minutes. The node that solves the mathematical problem acquires the right to place the next block on the chain and broadcast it to the network.
And what if two nodes solve the problem at the same time and send their blocks to the network simultaneously? In this case, both blocks are broadcast and each node builds on the block that it received first. However, the blockchain system requires each node to build immediately on the longest blockchain available. So if there is ambiguity about which is the last block, as soon as the next block is solved, each node will adopt the longest chain as the only option.

Fig.7 - End of chain ambiguity logic
Due to the low probability of solving blocks simultaneously, its almost impossible that multiple blocks would be solved at the same time over and over, building different tails, so the whole blockchain stabilizes quickly to one single string of blocks that every node agrees on.
A disagreement about which block represents the end of the chain tail opens up the potential for fraud again. If a transaction happens to be in a block that belongs to a shorter tail (like block B in Fig. 7), once the next block is solved, this transaction, along with all others in its block, will go back to the unconfirmed transactions.
Transactions in the Bitcoin blockchain system are protected by a mathematical race: Any attacker is competing against the whole network.
Lets see how Mary could leverage this end-of-chain ambiguity to perform a double-spending attack. Mary sends money to John, John ships the product to Mary. Since nodes always adopt the longer tail as the confirmed transactions, if Mary could generate a longer tail that contains a reverse transaction with the same input references, John would be out of both his money and his product.

Fig. 8 - Marys double-spending attack
How does the system prevent this kind of fraud? Each block contains a reference to the previous block (see Fig. 6). That reference is part of the mathematical problem that needs to be solved in order to spread the following block to the network. So, its extremely hard to pre-compute a series of blocks due to the high number of random guesses needed to solve a block and place it on the blockchain. Mary is in a race against the rest of the network to solve the math problem that allows her to place the next block on the chain. Even if she solves it before anyone else, its very unlikely she could solve two, three, or more blocks in a row, since each time she is competing against the whole network.
Could Mary use a super fast computer to generate enough random guesses to compete with the whole network in solving blocks? Yes, but even with a very, very fast computer, due to the large number of members in the network, its highly unlikely Mary could solve several blocks in a row at the exact time needed to perform a double-spending attack.
She would need control of 50 percent of the computing power of the whole network to have a 50 percent chance of solving a block before some other node does??and even in this case, shed only have a 25 percent chance of solving two blocks in a row. The more blocks to be solves in a row, the lower the probability of her success. Transactions in the bitcoin blockchain system are protected by a mathematical race: Any attacker is competing against the entire network.
Therefore, transactions grow more secure with time. Those included in a block confirmed one hour ago, for example, are more secure than those in a block confirmed in the last 10 minutes. Since a block is added to the chain every 10 minutes on average, a transaction included in a block for the first time an hour ago has most likely been processed and is now irreversible.

Fig. 9 - Blockchain transactions security
Mining Bitcoin
In order to send bitcoins, you need to reference an incoming transaction to your own wallet. This applies to every single transaction across the network. So, where do bitcoins come from in the first place?
As a way to balance the deflationary nature of bitcoin due to software errors and wallet password loss, a reward is given to those who solve the mathematical problem of each block. The activity of running the bitcoin blockchain software in order to obtain these bitcoin rewards is called mining??and its very much like mining gold.
Rewards are the main incentive for private people to operate the nodes, thus providing the necessary computing power to process transactions and stabilize the blockchain network.
Because it takes a long time for a typical computer to solve a block (about one year on average), nodes band together in groups that divide up the number of guesses to solve the next block. Working as a group speeds up the process of guessing the right number and getting the reward, which is then shared among group members. These groups are called mining pools.
Some of these mining pools are very large, and represent more than 20 percent of the total network computing power. This has clear implications for network security, as seen in the double-spend attack example above. Even if one of these pools could potentially gain 50 percent of the network computing power, the further back along the chain a block goes, the more secure the transactions within it become.
However, some of these mining pools with substantial computing power have decided to limit their members in order to safeguard overall network security.
Since the overall network computing power is likely to increase over time due to technological innovation and the increasing number of nodes, the blockchain system recalibrates the mathematical difficulty of solving the next block to target 10 minutes on average for the entire network. This ensures the networks stability and overall security.
Moreover, every four years the block reward is cut in half, so mining bitcoin (running the network) gets less interesting over time. To encourage nodes to keep operating, small reward fees can be attached to each transaction; these rewards are collected by the node that successfully includes such transactions in a block and solves its mathematical problem. Due to this mechanism, transactions associated with a higher reward are usually processed faster than those associated with a low reward. What this means is that, when sending a transaction, you can decide if youd like to process it faster (more expensive) or cheaper (takes more time). Transaction fees in the bitcoin network are currently very small compared with what banks charge, and theyre not associated with the transaction amount.
Blockchain Benefits and Challenges
Now that you have a general understanding of how the blockchain works, lets take a quick look at why its so interesting.
Using blockchain technology has remarkable benefits:
	You have complete control of the value you own; there is no third party that holds your value or can limit your access to it.
	The cost to perform a value transaction from and to anywhere on the planet is very low. This allows micropayments.
	Value can be transferred in a few minutes, and the transaction can be considered secure after a few hours, rather than days or weeks.
	Anyone at any time can verify every transaction made on the blockchain, resulting in full transparency.
	Its possible to leverage the blockchain technology to build decentralized applications that would be able to manage information and transfer value fast and securely.
However, there are a few challenges that need to be addressed:
	Transactions can be sent and received anonymously. This preserves user privacy, but it also allows illegal activity on the network.
	Though many exchange platforms are emerging, and digital currencies are gaining popularity, its still not easy to trade bitcoins for goods and services.
	Bitcoin, like many other cryptocurrencies, is very volatile: There arent many bitcoins available in the market and the demand is changing rapidly. Bitcoin price is erratic, changing based on large events or announcements in the cryptocurrencies industry.
Overall, the blockchain technology has the potential to revolutionize several industries, from advertising to energy distribution. Its main power lies in its decentralized nature and ability to eliminate the need for trust.
New use cases are arising all the time??like the possibility of creating a fully decentralized platform that runs smart contracts like Ethereum. But its important to remember that the technology is still in its infancy. New tools are being developed every day to improve blockchain security while offering a broader range of features, tools, and services.
________________________________________
TECH:
What is Blockchain Technology? A Step-by-Step Guide For Beginner
Is blockchain technology the new internet?
The blockchain is an undeniably ingenious invention  the brainchild of a person or group of people known by the pseudonym,  Satoshi Nakamoto. But since then, it has evolved into something greater, and the main question every single person is asking is: What is Blockchain?
By allowing digital information to be distributed but not copied, blockchain technology created the backbone of a new type of internet. Originally devised for the digital currency, Bitcoin,  (Buy Bitcoin) the tech community is now finding other potential uses for the technology.
Bitcoin has been called digital gold, and for a good reason. To date, the total value of the currency is close to $9 billion US. And blockchains can make other types of digital value. Like the internet (or your car), you dont need to know how the blockchain works to use it. However, having a basic knowledge of this new technology shows why its considered revolutionary. So, we hope you enjoy this, What Is Blockchain Guide. And if you already know what blockchain is and want to become a blockchain developer (2018  currently in high demand!) please check out our in depth blockchain tutorial and create your very first blockchain.
 
________________________________________

What is Blockchain Technology?

The blockchain is an incorruptible digital ledger of economic transactions that can be programmed to record not just financial transactions but virtually everything of value.
Don & Alex Tapscott, authors Blockchain Revolution (2016)
 
 
 
A distributed database
Picture a spreadsheet that is duplicated thousands of times across a network of computers. Then imagine that this network is designed to regularly update this spreadsheet and you have a basic understanding of the blockchain.
Train to Become A Blockchain Developer
Start Your Free Trial Today!
Information held on a blockchain exists as a shared  and continually reconciled  database. This is a way of using the network that has obvious benefits. The blockchain database isnt stored in any single location, meaning the records it keeps are truly public and easily verifiable. No centralized version of this information exists for a hacker to corrupt. Hosted by millions of computers simultaneously, its data is accessible to anyone on the internet.
To go in deeper with the Google spreadsheet analogy, I would like you to read this piece from a blockchain specialist.
Blockchain as Google Docs
 
________________________________________

The traditional way of sharing documents with collaboration is to send a Microsoft Word document to another recipient, and ask them to make revisions to it. The problem with that scenario is that you need to wait until receiving a return copy before you can see or make other changes because you are locked out of editing it until the other person is done with it. Thats how databases work today. Two owners cant be messing with the same record at once.Thats how banks maintain money balances and transfers; they briefly lock access (or decrease the balance) while they make a transfer, then update the other side, then re-open access (or update again).With Google Docs (or Google Sheets), both parties have access to the same document at the same time, and the single version of that document is always visible to both of them. It is like a shared ledger, but it is a shared document. The distributed part comes into play when sharing involves a number of people.
Imagine the number of legal documents that should be used that way. Instead of passing them to each other, losing track of versions, and not being in sync with the other version, why cant *all* business documents become shared instead of transferred back and forth? So many types of legal contracts would be ideal for that kind of workflow.You dont need a blockchain to share documents, but the shared documents analogy is a powerful one.
William Mougayar, Venture advisor, 4x entrepreneur, marketer, strategist and blockchain specialist 
Blockchain Durability and robustness
Blockchain technology is like the internet in that it has a built-in robustness. By storing blocks of information that are identical across its network, the blockchain cannot:
1.	Be controlled by any single entity.
2.	Has no single point of failure.
Bitcoin was invented in 2008. Since that time, the Bitcoin blockchain has operated without significant disruption. (To date, any of problems associated with Bitcoin have been due to hacking or mismanagement. In other words, these problems come from bad intention and human error, not flaws in the underlying concepts.)
The internet itself has proven to be durable for almost 30 years. Its a track record that bodes well for blockchain technology as it continues to be developed.
________________________________________

As revolutionary as it sounds, Blockchain truly is a mechanism to bring everyone to the highest degree of accountability. No more missed transactions, human or machine errors, or even an exchange that was not done with the consent of the parties involved. Above anything else, the most critical area where Blockchain helps is to guarantee the validity of a transaction by recording it not only on a main register but a connected distributed system of registers, all of which are connected through a secure validation mechanism. 
 Ian Khan, TEDx Speaker | Author | Technology Futurist
 
Transparent and incorruptible
The blockchain network lives in a state of consensus, one that automatically checks in with itself every ten minutes.  A kind of self-auditing ecosystem of a digital value, the network reconciles every transaction that happens in ten-minute intervals. Each group of these transactions is referred to as a block. Two important properties result from this:
1.	Transparency data is embedded within the network as a whole, by definition it is public.
2.	It cannot be corrupted altering any unit of information on the blockchain would mean using a huge amount of computing power to override the entire network.
 
In theory, this could be possible. In practice, its unlikely to happen. Taking control of the system to capture Bitcoins, for instance, would also have the effect of destroying their value.
________________________________________

Blockchain solves the problem of manipulation. When I speak about it in the West, people say they trust Google, Facebook, or their banks. But the rest of the world doesnt trust organizations and corporations that much  I mean Africa, India, the Eastern Europe, or Russia. Its not about the places where people are really rich. Blockchains opportunities are the highest in the countries that havent reached that level yet.
Vitalik Buterin, inventor of Ethereum
 
 
A network of nodes
A network of so-called computing nodes make up the blockchain.
 

Node
(computer connected to the blockchain network using a client that performs the task of validating and relaying transactions) gets a copy of the blockchain, which gets downloaded automatically upon joining the blockchain network.
 
 
 
 
Together they create a powerful second-level network, a wholly different vision for how the internet can function.
Every node is an administrator of the blockchain, and joins the network voluntarily (in this sense, the network is decentralized). However, each one has an incentive for participating in the network: the chance of winning Bitcoins.
Nodes are said to be mining Bitcoin, but the term is something of a misnomer. In fact, each one is competing to win Bitcoins by solving computational puzzles. Bitcoin was the raison detre of the blockchain as it was originally conceived. Its now recognized to be only the first of many potential applications of the technology.
There are an estimated 700 Bitcoin-like cryptocurrencies (exchangeable value tokens) already available. As well, a range of other potential adaptations of the original blockchain concept are currently active, or in development.
 

TECH:
How Does Blockchain Work?


For investors new to the cryptocurrency world, one of the most overwhelming and confusing aspects can be blockchain. Blockchain technology is what powers and supports the digital currency space, and many analysts believe that it contains numerous viable applications and uses beyond cryptocurrencies as well. You may have heard about financial institutions and even mainstream corporations around the world beginning to explore ways that they can integrate blockchain technology into their traditional practices. Beyond that, though, it can be a bit of a mystery as to what blockchain is exactly and as to how it works. Below, we'll explore the ins and outs of blockchain, providing an overview of this technology, how it works with regard to cryptocurrencies and other potential applications and why it may be one of the most revolutionary inventions since the internet.
The Three Primary Components of Blockchain
Blockchain can actually be thought of as the combination of several different existing technologies. While these technologies themselves aren't new, it is the ways in which they are combined and applied which brought about blockchain. According to CoinDesk, these three component technologies are:
	Private key cryptography
	A distributed network that includes a shared ledger
	Means of accounting for the transactions and records related to the network
To illustrate the technology of private cryptographic keys, it helps to envision two individuals who wish to conduct a transaction online. Each of these individuals holds two keys: One of these is private and one is public. By combining the public and private keys, this aspect of cryptography allows individuals to generate a secure digital identity reference point. This secure identity is a major component of blockchain technology. Together, a public and a private key generate a digital signature, which is a useful tool for certifying and controlling ownership.
The digital signature of the cryptography element is then combined with the distributed network technology component. Blockchain technology acts as a large network of individuals who can act as validators to reach a consensus about various things, including transactions. This process is certified by mathematical verification and is used to secure the network. By combining the use of cryptographic keys with a distributed network, blockchain allows for new types of digital interactions.
Process of Confirmation
One of the most important aspects of blockchain technology is the way that it confirms and validates transactions. In the example above, in which two individuals wish to conduct a transaction online, each with a private and a public key, blockchain allows the first person (person A) to use their private key to attach information regarding the transaction to the public key of the second person (person B). This information together forms part of a block, which contains a digital signature as well as a timestamp and other relevant information about the transaction, but not the identities of the individuals involved in that transaction. That block is then transmitted across the blockchain network to all of the nodes, or other component parts of the network, which will then act as validators for the transaction.
All of this sending of information and validating of blocks requires huge amounts of computing power. In practical terms, it may seem unrealistic to expect millions of computers around the world to all be willing to dedicate computing power and other resources to this endeavor. One solution to this issue for the blockchain network is mining. Mining is related to a traditional economic issue called the "tragedy of the commons." Put simply, this concept summarizes a situation in which individuals who each act independently in their own self interests tend to behave in ways contrary to the common good of all users as a result of depleting a resource through their action at a collective level. In the process of blockchain validation, an individual who gives up a small portion of his or her computational power in order to provide a service to the network thereby earns a reward. By acting out of self-interest (aiming to earn the reward: in this case, a small amount of a cryptocurrency), that person has been incentivized to help serve the needs of the broader network.
Chains of Blocks
Why go through this complicated process of validation anyway? For blockchain networks, this is a crucial step toward insuring that cryptocurrencies cannot be spent in multiple transactions at the same time, a concept known as double-spending. In order to protect against double-spending, blockchain networks have to ensure that cryptocurrencies are both uniquely owned and imbued with value. One way of providing this service is to have the nodes within the blockchain network act as components of the ledger system itself, maintaining a history of transactions for each coin in that network by working to solve complicated mathematical problems. These nodes serve to confirm or reject blocks representing bits of information about transactions. If a majority of node operators arrive at the same solution to a problem, the block is confirmed and it is added to the chain of blocks that exist before it. This new block is timestamped and is likely to contain information about various aspects of past transactions. This is where there is room for variation depending upon the particular network: some blockchain networks include certain types of information in their blocks, while others include different sets of information.
It is this last aspect of blockchain that some people believe provides the most potential for future applications in the future. The data making up blocks in a blockchain such as the one corresponding to bitcoin, for example, is linked with the past transactions that have taken place between different individuals, acting as a public record of all past transactions. But the data included in blocks could be essentially anything. For governments, for example, aspects of blockchain technology might prove useful when it comes to authorizing transactions, which is normally done through compliance regimes. Blockchain technology could be useful for providing audit trails or to foster new connections between different financial institutions and potential partners. For other aspects of the financial world, blockchain may be able to streamline the process of clearing and settlement, which has traditionally taken days. This technology could also help to automate regulatory compliance by translating legal prose into code, for example, or by permitting certain types of transactions and blocking others. There are wide-ranging possibilities for blockchain technology both within and outside of the financial world.
As with any new technology, however, it's not entirely clear how to best make use of the powerful capabilities of blockchain. As time goes on, it's likely that continued experimentation will unveil new ways of utilizing blockchain for a variety of different purposes, as well as new methods of utilizing blockchain in order to make it more effective, efficient, secure and powerful. In the meantime, the largest blockchain networks, such as those for digital currencies like bitcoin, are only continuing to grow. (See also: 5 Ways to Invest in the Blockchain Boom.)


TECH:
How does blockchain verification work?
Ad by Hyperion Fund

2018's most innovative project is here.

75% of the hard cap reached.

Learn more at invictuscapital.com
8 Answers
 
Alexandra Goldberg, Social Media Manager (2017-present)
Answered Nov 6, 2017  Author has 256 answers and 180.5k answer views
Blockchain settlement, as outlined in the original whitepaper by Satoshi Nakamoto , enables a digital cash-like system to transmit scarce units of value reliably without the need for intermediation or peer to peer trust. The bitcoin blockchain is valued for its robustness as it is a highly redundant decentralized system, designed to resist political corruption thanks to a low reliance on legacy banking and payment infrastructure.
When it comes to commerce use cases, however, the blockchain natively fails to provide much of the services traditionally demanded by consumers and merchants. Among these deficiencies are instant payments, for example bitcoin payments can take on average minutes to hours , and there is no complete mechanism for handling fraud or ensuring customer satisfaction.
The blockchain by design is a low level protocol that never intended to directly solve all the matters in payments and, rather humbly provides the foundation upon which many features of the traditional financial system can be rebuilt. For blockchain technology to truly propagate to the payment space, services like credit and mediation must eventually be included.
Blockchains have an inherent limitation with respect to the sphere of data with which smart contracts can rely on. Blockchains can only see the activity of their networks and have no senses enlightening reliable insight to the outside world. The blockchain does not know about temperature changes, elections, sporting events or whether or not a consumer was shipped the correct goods they ordered.
For the blockchain then to factor outside information, the data must be mapped from the real world to the blockchain by some mechanism. This process of integrating data outside the blockchain network is known as oraclization. An oracle provides a service which integrates outside data onto the blockchain and is required for much of the most interesting use cases of smart contracts.
Payments, a key utility of digital currencies, requires more than native blockchain technology to account for the most common payment needs. In payments, a consumer usually purchases goods or services and pays with currency - this is the action digital currencies focus on. The completion of the payment, however, requires not just the receipt of currency but also the rendering of the goods and services as originally agreed. True payment settlement, therefore, only occurs once both parties, the consumer and merchant, are settled.
Capitalistic trade is built on the ideal that two parties who voluntarily trade both benefit from the transaction. This assumes customer Alice values merchant Bobs goods or services more than Bob values Alices currency, thereby justifying the trade. The non-zero sum outcome desired here can, then, only be guaranteed if we account for both legs of the transaction - payment and delivery.
Blockchains have an inherent limitation with respect to the sphere of data with which smart contracts can rely on. Blockchains can only see the activity of their networks and have no senses enlightening reliable insight to the outside world. The blockchain does not know about temperature changes, elections, sporting events or whether or not a consumer was shipped the correct goods they ordered.
For the blockchain then to factor outside information, the data must be mapped from the real world to the blockchain by some mechanism. This process of integrating data outside the blockchain network is known as oraclization. An oracle provides a service which integrates outside data onto the blockchain and is required for much of the most interesting use cases of smart contracts.
Payments, a key utility of digital currencies, requires more than native blockchain technology to account for the most common payment needs. In payments, a consumer usually purchases goods or services and pays with currency - this is the action digital currencies focus on. The completion of the payment, however, requires not just the receipt of currency but also the rendering of the goods and services as originally agreed. True payment settlement, therefore, only occurs once both parties, the consumer and merchant, are settled.
Capitalistic trade is built on the ideal that two parties who voluntarily trade both benefit from the transaction. This assumes customer Alice values merchant Bobs goods or services more than Bob values Alices currency, thereby justifying the trade. The non-zero sum outcome desired here can, then, only be guaranteed if we account for both legs of the transaction - payment and delivery.
Would like to know more about advanced Blockchain technology based solution? Please visit: https://coti.io/
3.3k Views  View Upvoters  Answer requested by Chris Barton

Promoted by Blockchain Council

Verification in blockchain is a consensus mechanism based process.Since blockchain acts as a decentralized ledger with multiple nodes in the network, the transaction must be verified by all nodes. All blockchains are continuous chains starting from a genesis block upto to the current block.
During the process of creating a new block, the transactions are hashed and a unique output or result is created. A hash function, takes input, and produces a specific output of a fixed size. It acts a digital fingerprint which cannot be modified.
The successive block in the chain contains a hash of the previous block and each block refers to its predecessor block for authentication. The transactions in a blockchain are referred to by their hash. For authentication of a transaction, all nodes must receive same output by running the hash. If different output is received then the transaction is considered invalid and is not verified.
1k Views
 
Shruti Singh, Write about Technologies like Blockchain and IoT
Answered May 28, 2018  Author has 157 answers and 185k answer views
Blockchain technology is one of the valuable gift of technology because of its security and verification feature. Blockchain, as you might know, holds all the digital information in different blocks which are linked to each other via a hash codes.
 
Each block holds the hash code of the previous linked blog which is generated via the values stored within the block. Whenever someone tries to delete or change the values of any particular block, its has code changes which informs the connected blocks that something is not right.
Besides, whenever a new transaction is made or an existing one is updated, the majority of nodes within the blockchain systems gets notified. If they accept the changes, the changes are implemented. Otherwise, the request is denied.
Hope this helps!
 
Brian Schuster, Head of Founder Solutions at Ark Capital
Answered Apr 27, 2017  Author has 1.4k answers and 6m answer views
Verification (simplified) is the process of running transactions through one way mathematical equations called hashes. When a new block is created, all of its transactions are hashed (in some cases, hashed many times), producing a unique result.
These results are verified by other nodes on the network by re-running the same mathematical equations with the same inputs and confirming that the output matches exactly. If the output doesn't match what that tells the node is that the inputs don't match, either due to an error or fraud, and the transaction should be ignored.
This process works similarly for validating single transactions as well as validating new blocks.
 


Multi-user access feature.

Our new multi-user access feature provides everything you possibly need to achieve your ASO goals.

Learn more at asodesk.com
 
Oleg Sergeykin, PhD
Updated May 16, 2018  Author has 2k answers and 3.9m answer views
There are also an another process and tools to verify smart contracts, it is also could be considered as a form of blockchain verification (cause smart contracts are based on blockchain).
DeployMyContract is smart contract deploy and verification tool. It allows your contract will be compiled, deployed on the Ethereum network and verified on the Etherscan. You can select not only the mainnet Ethereum network but test networks also.
 
So all you have to do is start the application and choose the Ethereum Network. This is followed by pasting the source code which then starts processing and then asks for the main contract and wallet key file. You might have to fill in some constructor parameters if they exist but it still remains to be the most hassle-free way to deploy and verify a contract: How to verify a smart contract
553 Views
 
Robert Dsouza, lives in Bengaluru, Karnataka, India (2016-present)
Answered Mar 24, 2018
I was looking out for the same thing on Google and found this short but amazing article about the same topic:
How does Blockchain Verification Work? - TechVariable
Hope it helps you :)
 
Related Questions
	TECH: 

Pulling the Blockchain apart.. The transaction life-cycle
Popping the hood of something magical
Unraveling what the blockchain is, how it works and what the benefits are is pretty difficult. It took me many weeks to only get a rough idea on what is going on.
Therefore I will share my journey and understanding, to help others to get a bit more knowledge on this subject. So that we will not get sold snakeoil when talking to a blockchain consultant.
The ideas behind the Blockchain is a combination of various complex ideas and systems. While this combination provides significant value, its consequent complexity makes it difficult to grasp in a short period of time.

https://medium.com/the-mission/a-brief-history-of-blockchain-an-investors-perspective-e9b6605aad68
PULLING IT APART

https://bitsonblocks.net/2015/09/09/a-gentle-introduction-to-blockchain-technology/
In the previous blog I stated that the blockchain is nothing more then a collection of transactions. Those transactions are sealt together in packages of X number of transaction. Those packages are called blocks that are linked in a specific order, hence the name blockchain. A blockchain is comparable to a daisy chain :)

https://commons.wikimedia.org/wiki/File:Daisy_chain.JPG
To pull stuff apart it is important to know what the overall picture is, so that we can start pulling at the various parts of the whole. Lets start with looking at the life cycle of a transaction on a blockchain.
THE TRANSACTION LIFE CYCLE
Below we have a few Blockchain (and Bitcoin) transactions visualised.
Overall the sequence of steps are;
1.	Someone Requests a Transaction via something called a wallet.
2.	The transaction is send (broadcast) to all participation computers in the specific blockchain network.
3.	Every computer in the network checks (validate) the transaction against some validation rules that are set by the creators of the specific blockchain network.
4.	Validated transactions are stored into a block and are sealt with a lock (hash).
5.	This block becomes part of the blockchain when other computers in the network validate if the lock on the block is correct.
6.	Now the transaction is part of the blockchain and can not be altered in any way.
In the following graphics the steps are illustrated and I have written down the steps also. You will see that everybody is using a different way of naming the steps but overall the content is the same.
Insurance Funda

http://insurancefunda.in/bitcoin-cryptocurrency/
This is the most clear and easy to understand overview of the steps in the blockchain.
1.	Someone requests a transaction
2.	Transaction broadcasted to P2P computers (nodes).
3.	Validation, miners verify the transaction.
4.	Transactions combined to form a data block.
5.	New block added to existing Blockchain.
6.	The transaction is complete.
Deloitte

https://dupress.deloitte.com/dup-us-en/focus/tech-trends/2016/blockchain-applications-and-trust-in-a-global-economy.html
The image has a high resolution when you click on the source. The steps or elements depicted are
1.	Transaction
2.	Verification
3.	Structure
4.	Validation
5.	Blockchain Mining
6.	The chain
7.	Built-in Defense.
Wall Street Analyst

http://wallstanalyst.com/bitcoin-progression-towards-world-acceptance/
The description in this article is pretty clear. The steps identified here are.
1.	Send payment destination address.
2.	Payment destination address is published to the network for all to see.
3.	Payment transaction is done securely from origin address to destination address.
4.	The transaction is confirmed, processed and secured by the network and blockchain.
i-Scoop & Accenture

http://usblogs.pwc.com/emerging-technology/a-primer-on-blockchain-infographic/ https://www.i-scoop.eu/fintech/blockchain-distributed-ledger-technology/
This image is a cut out from a larger infographic created by Accenture. The steps identified here are
1.	Request of transaction is submitted to the network.
2.	The transaction is validated by the network.
3.	The verified transaction is combined with other verified transactions into a block in the blockchain.
4.	Transaction is complete.
Bambora

https://www.bambora.com/en/ca/blog/bitcoin-explained/
1.	Open your wallet and scan the address to which you want to send money.
2.	Select the amount of money and send the transaction.
3.	The wallet secures the payment so you know the sender of the money.
4.	The transaction is validated by the network and made part of the mining process.
5.	Mining is in progress and is done when a minder earns a bitcoin.
6.	The network validates the result of the mining process.
7.	The receiver of the money gets a confirmation of the successful transaction.
Techno Llama

http://www.technollama.co.uk/blockchains-and-the-challenges-of-decentralization
This blog is IMHO to abstract, but then this can also give new insights. The steps that are defined are.
1.	There is intention to send money from A to B.
2.	The transaction is put into a block.
3.	The block is send to all members of the network.
4.	The network validates the block.
5.	The block is added to the chain.
6.	The money is moved from A to B.
Zero hedge

http://www.zerohedge.com/news/2013-05-12/visualizing-how-bitcoin-transaction-works
This is a great roadmap, but to extensive for just someone without any knowledge to read.
1.	Wallets & Addresses
The sending and receiving party of the transaction have a wallet that contains addresses with money and can create new addresses when needed.
2.	Creating a new (receiving) address
create a new address for receiving the money for a transaction.
3.	Submitting a Payment
here the sender tells the wallet how much money is to be send to the receiving address and this is translated into a transaction.
4.	Verifying the transaction 
in this step the transaction if being verified by computers in the network and bundled in transaction blocks
5.	Cryptographic Hashes
the blocks are locked together with cryptographic hashes.
6.	Transaction verified
The transaction is verified and part of the chain of blocks and therefor is it not possible to change this ever.

TECH:
How do Bitcoin Transactions Work?
	

Last updated: 29th January 2018
Simple version:
If I want to send some of my bitcoin to you, I publish my intention and the nodes scan the entire bitcoin network to validate that I 1) have the bitcoin that I want to send, and 2) haven't already sent it to someone else. Once that information is confirmed, my transaction gets included in a "block" which gets attached to the previous block - hence the term "blockchain." Transactions can't be undone or tampered with, because it would mean re-doing all the blocks that came after.
Getting a bit more complicated:
My bitcoin wallet doesn't actually hold my bitcoin. What it does is hold my bitcoin address, which keeps a record of all of my transactions, and therefore of my balance. This address  a long string of 34 letters and numbers  is also known as my "public key."  I don't mind that the whole world can see this sequence. Each address/public key has a corresponding "private key" of 64 letters and numbers. This is private, and it's crucial that I keep it secret and safe. The two keys are related, but there's no way that you can figure out my private key from my public key.
That's important, because any transaction I issue from my bitcoin address needs to be "signed" with my private key. To do that, I put both my private key and the transaction details (how many bitcoins I want to send, and to whom) into the bitcoin software on my computer or smartphone.
With this information, the program spits out a digital signature, which gets sent out to the network for validation.
This transaction can be validated  that is, it can be confirmed that I own the bitcoin that I am transferring to you, and that I haven't already sent it to someone else  by plugging the signature and my public key (which everyone knows) into the bitcoin program. This is one of the genius parts of bitcoin: if the signature was made with the private key that corresponds to that public key, the program will validate the transaction, without knowing what the private key is. Very clever.
The network then confirms that I haven't previously spent the bitcoin by running through my address history, which it can do because it knows my address (= my public key), and because all transactions are public on the bitcoin ledger.
Even more complicated:
Once my transaction has been validated, it gets included into a "block," along with a bunch of other transactions.
A brief detour to discuss what a "hash" is, because it's important for the next paragraph: a hash is produced by a "hash function," which is a complex math equation that reduces any amount of text or data to 64-character string. It's not random  every time you put in that particular data set through the hash function, you'll get the same 64-character string. But if you change so much as a comma, you'll get a completely different 64-character string. This whole article could be reduced to a hash, and unless I change, remove or add anything to the text, the same hash can be produced again and again. This is a very effective way to tell if something has been changed, and is how the blockchain can confirm that a transaction has not been tampered with.
Back to our blocks: each block includes, as part of its data, a hash of the previous block. That's what makes it part of a chain, hence the term "blockchain." So, if one small part of the previous block was tampered with, the current block's hash would have to change (remember that one tiny change in the input of the hash function changes the output). So if you want to change something in the previous block, you also have to change something (= the hash) in the current block, because the one that is currently included is no longer correct. That's very hard to do, especially since by the time you've reached half way, there's probably another block on top of the current one. You'd then also have to change that one. And so on.
This is what makes Bitcoin virtually tamper-proof. I say virtually because it's not impossible, just very very, very, very, very difficult and therefore unlikely.
Fun
And if you want to indulge in some mindless fascination, you can sit at your desk and watch bitcoin transactions float by. Blockchain.info is good for this, but if you want a hypnotically fun version, try BitBonkers.

TECH:
39 new apps and sites that will change your life
The sites, apps and tools to change the way you work, rest and play
	
 
	
 
	
 
	

You don't need us to tell you how great the internet is, or how a couple of apps or services can completely revolutionise your life both on and offline. But how can you be sure you're kitted out with the right ones?
It's difficult to search for something you don't even know exists, and the sheer number of new toys to play with is growing at an exponential rate. Are you wasting time that could be better spent kicking back, watching movies, or even - if you're desperate - being out in the sun? Almost certainly!
Over the following pages, you'll find our carefully selected recommendations to help upgrade your entertainment, work and social life with the best the internet has to offer - a few new programs, because sometimes you can't beat a client for heavy lifting, a few apps to load onto your smartphone, and websites that will give you great new features you never knew you could live without for the low, low cost of ignoring a couple of adverts. From the top of the cloud down, they're what you need.
What should you look for in a new service? The feature you can't do without is the ability to export your data should the service either shut down or be swallowed up. The days of a company like Google or Yahoo swooping in to guarantee its future are long gone. Services purchased by the giants are now routinely closed down, with the point of their acquisition being the talent behind them, and this can happen very quickly.
If you can't extract your data, be it photos, music, backed up files or anything else, you're best off avoiding the service until it adds that feature. Never assume it'll be added later. It may be, but you shouldn't take that gamble.
This is why we recommend Dropbox as our file backup tool of choice. It works great right now, and may it continue to do so, but should it close overnight, you have a copy of everything on your hard drive. Next, the service should offer some value beyond social connection, and in most cases, the option to be as antisocial as you like.
Anything that offers Facebook integration means that you can be up and running in a single click, but you never know what it will broadcast to your friends. If you don't want them to know what you're watching, listening to or clicking, be very careful before you grant access - especially if it's a new service that has yet to prove itself.
Finally, of course, never give any information to a site you're not sure you can trust. Scammers are everywhere, and some of them make websites. It's worth putting any site's name into Google just to check for skeletons in the closet. In most cases you won't find anything, and that's great. If there's more to it than met the eye though, you'll be glad you took the time to check.
Tools for work are the most compelling and the most risky. On one hand, stumbling across something that makes life easier is to be savoured. On the other, none of us want to start relying on something that gets taken away - and the price of failure can be serious.
This is one of the main drawbacks of tools like Google Docs. Yes, they're great - as long as you have internet access. If it goes down, you're potentially in trouble. Better safe than sorry.
Cloud computing should still be a core part of your working environment though, even if you're not ready to give up your local applications. In fact, in most cases, you probably shouldn't do that at all. Web apps still pale in comparison to a good local client for most functions, from email to image editing to basic word processing. They can offer handy features like multi-user editing, but by technology or design, they usually feel written to be 'good enough' rather than 'great'.
More to the point, they lack power. No serious spreadsheet user is going to use Google Docs when they could be using Excel for instance, and it'll likely be years before that changes - if it ever does. No, at the moment, the best work use for the cloud is accessing your files and computers from a distance, with web apps acting as stand-ins. Google Docs doesn't care if it's your primary word-processor, or simply what you fire up on your old netbook. It'll still edit and export all of your most important files - and if they're available via the web, you'll always be able to feed them into it.
Best of all, doing it this way also gives you access to older versions of every document you work on. That's not something you get on many home/office PCs, and is impossible if you work on multiple systems.
That's not all though. The cloud offers immediate access to all your devices as well as data, keeping you up to date with everything. On iPhone, Apple's iCloud is frankly lousy for most document syncing, but you can't beat it for contacts and calendars.
Most other phones will also offer the ability to connect to either dedicated address book software or Google Contacts, ensuring you stay up to date.
No distractions
But what of the offline world? If you don't use Microsoft Office, you can find tools that can replicate its functions  OpenOffice.org being the best of the free alternatives. For actually writing your documents though, you can often do better.
For example, there's been a slew of 'distraction-free' environments over the last couple of years, and at first glance, they may appear stupid. They blank out your screen, give you a cursor, and that's about it. No formatting, no fancy layout - just text. Try them however - Q10 is a good starting point - and you'll soon see the appeal.
If you have a long document to write, not having new mail icons popping up or the siren's call of an internet browser can be just what you need to focus on the task at hand. For best results, open rainymood.com for some free atmospheric noise to can break the silence without breaking your flow.
Finally, don't forget what your tablet and smartphone can contribute to your day. The benefi ts of their apps and web browsers go without saying, but don't think you have to spend a fortune to get the data you need to take advantage of them.
Visit giffgaff.com and you can get 500MB of 3G data for 5 a month on tablets, or unlimited data on phones for 10. Both these deals are available as PAYG rather than contracts, and run on the O2 network. Note though that if you have an iPad or iPhone, you'll need to order a special micro-SIM instead of the standard model, or cut it down to size yourself.
Websites for work
1. Remote PC access
TeamViewer 
www.teamviewer.com
Advertisement
 
Don't be put off by the price (419 for a lifetime business licence) - it's free for non-commercial use. TeamViewer givers you complete access to your home PC (as long as it's switched on, obviously) from anywhere. You'll never be without that essential file or application again. Just don't tell the boss you're using it, especially if you're planning to pull the old "Oh, but I definitely emailed it in!" dodge next Monday morning.
2. Distraction busters
AntiSocial/Freedom
www.macfreedom.com
 
Freedom switches off your internet access for a set number of minutes, while AntiSocial lets you stay online, but blocks time-wasting sites like Facebook. You can restore access if you must, but it involves rebooting your PC. It's just annoying enough that you won't do it on a whim, while still giving you a get-out clause if you absolutely need one.
3. Cloud storage
Dropboxwww.dropbox.com
 
Not just the best cloud storage option out there, Dropbox is also the most convenient. It's the de facto standard for mobile apps, you get 2GB of space for free, and if you send out a few referral links, you can easily boost that to 7/8GB. Add support for file versioning and easy web access, and it's the one cloud tool you definitely need.
4. Writing tool
Scrivener 
www.scrivenerforwindows.com
 
Do you ever write long documents? Reports, essays, novels, anything? You need Scrivener  the best writing tool in the world. Endlessly configurable, as happy to help you plan as to help you write, and capable of reformatting your document to a perfect exported copy whenever you like. You'll never go back to Microsoft Word again.
5. Aggregator
Instapaper 
www.instapaper.com
 
Many articles on the web deserve to be read, but you don't always have the time to go through them right then and there. Instapaper lives as a bookmark in your browser, stripping out all the adverts and graphical cruft in favour of storing a simple, well-formatted text version that's perfect for reading on your smartphone or tablet. When you finally have a moment to spare, it's all there and waiting for you.
6. Voicemail
HulloMail 
www.hullomail.com
 
Visual voicemail refined. With HulloMail, you can get a handy iPhone-style answering machine on an Android handset - or if you don't pay for an O2 contract, with the bonus of having any messages emailed to you so that you can pick them up anywhere. It's not compatible with all services unfortunately, especially phones on pay-as-you-go payment plans, although does work fine with a Giffgaff SIM.
Apps for work
7. Soundnote - 2.99
Download from iTunes
This little app is the ultimate dictaphone. Hit 'Record' and you get everything that happens around you, with the option to scribble on the screen. Tap the scribbles and you go straight to that point in the recording - perfect for transcribing interviews.
8. TapToDo - Free
Download from iTunes
A to-do list that syncs with your Google Tasks list, keeping you up to date with the rest of the day's assignments whether you're at your PC or out on the road. It provides an extra level of security by adding its own password, too.
9. Reeder - 1.99
Download from iTunes
Hands down the best RSS reader for iOS. Reeder is the perfect way to track any number of websites and news feeders via Google Reader. Google Apps users beware though - you'll need to switch two-step authentication on.
10. Evernote - Free
Your personal notebook. Clip websites, copy images, and record notes wherever you are. The free version does a lot, the premium edition (4 a month or 35 a year) gives you extra space, collaborative note-making tools and offiine access to your collection.

TECH:
12 Massive Changes That Will Transform The Software Industry Within 5 Years
Julie Bort
 
May 7, 2012, 7:36 PM

GOOG Alphab Non Vtg-C
 1,239.12 -10.18 (-0.80 %)
DisclaimerGet real-time GOOG charts here 
MSFT Microsoft
 111.95 -0.07 (-0.10 %)
DisclaimerGet real-time MSFT charts here 
ORCL Oracle
 48.38 -0.51 (-1.00 %)
DisclaimerGet real-time ORCL charts here 
The software industry is going through the kind of perfect storm that only happens once ever 30 years.
The last time this happened, it created the giants of today, like Microsoft, Oracle and SAP. It also created massive new industries  like computer security software.
The same thing is happening now and in the next five years, the next crop of industry giants and new technologies will arise.
Cloud computing changes how companies buy software.
Flickr / The Toad
What's changing: Instead of spending a chunk of cash buying software licenses every three to five years, companies are buying subscriptions and getting their software delivered over the Internet. Most (but not all) enterprise software will eventually be delivered this way.
Who wins: Companies with cloud in their DNA like Salesforce.com, Google Apps and startups who deliver cloud-based point products (OnLive, Workday, Zendesk, etc.)
Who loses: Traditional software players like Oracle, Microsoft, and SAP. They are all investing heavily in cloud computing, but so far, none has made the transition in a significant way. We'll see if they can.
Bottoms-up marketing tactics change which software companies buy.
Orin Zebest via flickr
What's changing: Software is being bought in by the employees who use it instead of by IT professionals. IT folks call it "rogue" software, but those selling it that way call it "bottoms up" marketing.
When enough people use it, a company is "forced" to negotiate for an enterprise subscription to save money or get features that will make the product more secure and manageable.
Who wins: Any company selling directly to employees. This is how Microsoft SharePoint got so popular. Employees were buying it themselves. But services that make it easy for employees to share stuff, like Dropbox, Asana, Hootsuite, Evernote are also big winners.
Who loses: IT departments and sometimes their companies. There are no controls in place to make sure that employees aren't breaking rules like sharing confidential information, or downloading software that has viruses. Expect new technologies like desktop virtualization to rise in reaction (see next slide).
Desktop virtualization changes how companies deliver software.

What's changing: People want to work on their own devices and use software they like. Companies need to control software and protect important data. Desktop virtualization does this. It is like your entire hard drive  operating system, software, settings  delivered as a service over the Internet. It is often called Desktop as a Service (DaaS).
Who wins: Companies like Dell, Wipro, VMware, Red Hat, Citrix, and a whole bunch of new DaaS companies like Wanova, Dizzion, countless others.
Who loses: No one. Microsoft licenses Windows per device, so it doesn't lose money  though if enterprises aren't careful, it can cost them more than using a regular PC. However, DaaS requires a full-time network connection so isn't good for many employees, especially if they travel.
Windows 8's 'Metro' interface will change how people use Windows.
Screenshot
What's changing: Everything about Windows. Microsoft has introduced a new Metro interface that won't work with older Windows apps and has a totally new user interface geared for touch.
Who wins: PC makers, who will be able to use Windows 8 to make Windows-based tablets that are more competitive with the iPad. New Windows developers, as Microsoft is making it easier for them to write applications for Windows. Consumers, who will get more choice in tablets.
Who loses: Microsoft's faithful who hate to relearn how to use Windows. Potentially Microsoft if there's a big backlash against Windows 8 like there was against Vista.
The move to Web apps is breaking Microsoft's monopoly.
Ellis Hamburger, Business Insider
What's changing: New web technologies like HTML5 mean that great software can be written for the browser and not a particular operating system like Windows.
Who wins: Cloud/web companies like Google, software developers, and Linux vendors like Red Hat and Canonical.
Who loses: Microsoft and its most established Windows software developers, but maybe not permanently. Microsoft is reworking Windows to make it more relevant to the Web and introducing cloud software like Office 365 and cloud services like Azure. Don't count Microsoft out.
Mobile computing changes where software is used.
Flickr/Toby Simkin
What's changing: Software needs to be accessible anywhere on any device and can't be stuck in one place anymore. People want to be able to access work while at the beach or soccer field.
Who wins: Just about everyone. Employees. They get the freedom to work anywhere. Device makers like Apple, HTC, LG and Android owner Google because people own multiple devices. Also network companies like Cisco, Verizon, AT&T.
Who loses: Only companies stupid enough not to see this giant wave of mobile coming. We can't think of any. But we can't predict if giants like Microsoft, Oracle, or SAP will do a good job with their mobile apps and that's the risk.
App stores change how much people are willing to pay for software.
Larry He's So Fine
What's changing: Apple's app store has trained consumers to pay a lot less for software than we used to. We're forking out $0-$10 instead of $30-$100.
Who wins: Consumers. Startups with alternative business models: Rovio, Zynga, and so on.
Who loses: Traditional desktop software makers who focused on Windows apps.
The death of the mouse changes the software user interface.
Associated Press
What's changing: Touch and gesture interface is changing how we interact with software.
Who wins: Our wrists. People with disabilities. Plus gesture-based computing has created a whole new class of cool apps like these Kinect Hacks.
Who loses: Trackball makers and mice makers like HP, Logitech, Microsoft, and Apple. Traditional PC makers who haven't yet built any tablets competitive with Apple's iPad.
Open source changes how software is written.

What's changing: Software is being written by a crowd  anyone who wants to contribute  not controlled by a company.
Who wins: The people and companies using the software. Open source foundations like the Linux Foundation, Mozilla, Apache Foundation, Outercurve. Open source projects from companies like Red Hat, Opscode, and Rapid 7.
Who loses: Traditional proprietary software makers who have to do all the work themselves including Microsoft, Apple, and Oracle. All of these companies have had love-hate relationships with the open source community.
Open source changes who controls software.
PC World
What's changing: Many open source projects require that the software must always remain open and available for anyone to see the code and change it.
Who wins: The people using and contributing to the software. Open source companies like Red Hat and Canonical.
Who loses: Proprietary software vendors like Microsoft, Oracle, SAP, or CA. They have to be very careful not to accidentally use open source software in their products or they could wind up having to share the code with the world.
Big data changes what can be done with software.

What's changing: New technologies like noSQL databases and Hadoop allow massive amounts of data to be collected, stored, and analyzed inexpensively. This allows all kinds of new software to be created.
Who wins: Cloud software companies that service consumers and have to scale big: Zynga, eBay, AOL. Enterprises that want to analyze trends but couldn't afford old fashioned data warehouses. Storage equipment makers like EMC who will sell a lot more disk drives. Data warehouse companies with new products like Teradata. Hadoop startups like Cloudera and MapR
Who loses: For the most part, big data doesn't really replace anything, it augments it. But some forms of it, like noSQL databases, are taking a chunk away from traditional databases built by Oracle, IBM, and Microsoft.
Social media changes how software is used.
Dan Frommer, Business Insider
What's changing: Social media is being embedded into everything, from our spreadsheets to our televisions. People want to work together, discuss, find experts, get immediate feedback and so on.
Who wins: Facebook, LinkedIn, Google, Yammer, file-sharing services like Box, Salesforce, social media monitoring tools like Lithium, Microsoft with social tools like Lync, and any other company adding social features to their software.
Who loses: Most companies are finding ways to cash in.

TECH:
21 technologies transforming software development
The very nature of programming is evolving faster than you might think, thanks to these powerful tools
	 
 
	 
 
	 
 
	 
 
	 
 
	 
 
	 
By Peter Wayner
Contributing Editor, InfoWorld | AUG 3, 2017
Become An Insider
 
Why gaming AI wont help make AI work in the real worldbut...
	
Whats new in NativeScript
See all Insider
A long time ago, developers wrote assembly code that ran fast and light. On good days, they had enough money in their budget to hire someone to toggle all those switches on the front of the machine to input their code. On bad days, they flipped the switches themselves. Life was simple: The software loaded data from memory, did some arithmetic, and sent it back. That was all.
Today, developers must work with teams spread across multiple continents where people speak different languages with different character sets and  this is the bad part  use different versions of the compiler. Some of the code is new, and some may be from decade-old libraries that may or may not come with source code. Building team spirit and slogging through the mess is only the beginning of what it means to be a programmer today.
[ The art of programming moves rapidly. Don't miss: 21 hot programming trends  and 21 going cold. | 9 cutting-edge programming languages worth learning now. | Keep up with hot topics in programming with InfoWorld's App Dev Report newsletter. ]
The work involved in telling computers what to do is markedly different than it was even five years ago, and its quite possible that any Rip Van Winkle-like developer who slept through the past 10 years would be unable to function in the todays computing world. Everything seems to be changing faster than ever.
Here are 20 technologies transforming the very nature of programming. Theyre changing how we work with fellow developers, how we interact with our customers, and how we code. Dont get caught asleep at the console.

TECH:

The Real Meaning of Software Transformation for Businesses Today
OCTOBER 5, 2015 SIOBHAN MCFEENEY
This blog is part of joint work performed by Stephen Casale and Siobhan McFeeney.
As companies and industries embrace the full logic of open communities, automation, and services-oriented architectures, software creates a trajectory that has and will continue to radically transform how we do and consume things, and how we run our lives and businesses.
In this new calculus, protecting legacy strengths by adding complexity, or squeezing more from well trod sales channels no longer works for many enterprises. Software and the legions of open communities who wield it, adapt it daily, and can run around and through traditional barriers to entering markets, changing the ways of making, selling, and running companies. Software companies have fundamentally and permanently changed customer expectations to the extent that all companies must become digital or digitally enabled.
We know why good companies with good intentions come to Pivotal. They want to become great at building softwareto make it their talisman that helps them become tomorrows great company, or to tap into and preserve the domain expertise that they have worked so hard to achieve.
Software is deeply connected to change. But Ive talked to customers at Pivotal who cite how theyre retooling how they do software, as a time-to-market exercise. And I ask, is this what its really about, does it stop there? It doesnt.
Its more than a software platform that ties in a great data fabric and the analytics, more than microservices, and more than developers and IT teams who embrace DevOps, agile, and twelve factor app processes. Its all of that. But the decentralization and autonomy this creates affect how the connected parts of the enterprise will function to make the rate of app development, new workflows, and new processes succeed in what will be a reinvented company. At the end of all this technology, processes, reward systems, are clients who have customers who buy and renew services. Digital transformation puts these customers at the beginning of the conversation, not at the end.
Get Everyone Curious
Many companies tick through their compliance, overhead, and governance processes. It is rare when anyone questions why these things are in place and for what purpose. Curiosity across an organization is a big part of success in this new calculus. When an entire organization becomes curious, youre on the path to change. One of the things that triggers people across organizations to be curious is that they feel included. They think, hey, we can connect to this change and I understand my role in it. They also learn to frame every question with the customer or end user in mind. When we work with our clients we too need to be curious and ask the right questions. One of our early transformation clients struggled for months to build out their engineering team despite our support and guidance. For months the lack of progress baffled us until we asked some different questions, took a different approach than before and realised they had outsourced their sourcing and recruiting and were struggling with a new approach. Taking time to sometimes ask the obvious question of our clients can be the most efficient path to solving problems. Dont assume.
Shipping code every day gives developers and IT teams a tangible sense that theyre doing profound work. And once new code cycle is set in motion, it impacts working relationships across the organization. There is more demand for facilitators over lone heros, leadership and coaching over command and control.
There needs to the same sense of reward for an executive sponsor, or line of business owner, or for those who have to retrofit facilities to allow for a agile development. And security and compliance needs to become a true friend and ally. When we reinvent the HR, Finance, and ops roles, and deeply connect this new work to the overall transformation, we give a sense of purpose and inspiration to an entire company. Having said that, it is hard work and a long road but the beauty of the agile way is that while the road is long you get to see progress at every turn.
Its Really Not About You
When I started 11 years ago in AAAknown for its towing serviceswe built built an entire software operation around speed and volume, keeping extra tow trucks in reserve so we could hit the speed metric. The only problem with this was our sophisticated Net Promoter scoresour key tool and process to measure customer satisfactionwent from average to average.
Someone finally asked: Is it really about speed? It turns out that we were all about speed. The customer, as we found out through an intensive listening process, was about accurate time of arrival. Long story short, I led a big shift in software and operations around accurate times of arrival. It was cheaper, but more importantly it changed us entirely as a company. And our Net Promoter scores went way up.
This is a classic hurdle. A lot of big customers believe they have a sophisticated understanding about what customers need. The lesson for transformation and any company is to unearth the real thing that is driving the company and business, and not to drive what you think it is.
The lesson for transformation and any company is to unearth the real thing that is driving the company and business, and not to drive what you think it is.
The difference between doing what customers want, and listening and getting to what they need is worlds apart. If you can articulate this, and be religious about it, you have something special. In Pivotal Labs we have wonderful fail-safes to avoid going down rabbit holes of what our teams could do as opposed to what they should do. Most companies dont have these fail-safes, to remind them that its not about them. Show me an executive team who does not believe they are key to the success of the company. They are in fact a part of it, but if they really empower and educate their teams they become less critical. In my last role as a COO the executive team, while attempting to introduce a new culture tried a few surprising approaches. The most effective experiment was shutting down email for all VPs and above for two weeks. The amount of planning and risk assessment around this activity was insane and the results were..yes, nothing broke, in fact things moved just a little faster.
It starts with a simple user story. At the end of the day our purpose is to ensure the customer has a good experience, and thats true whether youre producing software or selling Tide. As we say at Pivotal, make the right thingnot what is dear to you or makes your head spin, but the right thing for the end user.
In the end, if you strip down all the things that surround us, the processes and the titles, theyre essentially comforts. The minute you flatten organizations, you become vulnerable, and that understandably makes many people uncomfortable. Its very brave, and honorable to make leaps to do the right thing. Over time it always pays off.
This holds true for software transformation. It requires multiple shifts in order to stay coordinated through change, but people have been doing industrial-size change for hundreds of years. Most people do find the path to reinvention surprisingly refreshing. Lets make sure the changes that software transformation relates to are practical things, where folks are included, and have a basic roadmap.

TECH:
The Real Meaning of Software Transformation for Businesses Today
OCTOBER 5, 2015 SIOBHAN MCFEENEY
This blog is part of joint work performed by Stephen Casale and Siobhan McFeeney.
As companies and industries embrace the full logic of open communities, automation, and services-oriented architectures, software creates a trajectory that has and will continue to radically transform how we do and consume things, and how we run our lives and businesses.
In this new calculus, protecting legacy strengths by adding complexity, or squeezing more from well trod sales channels no longer works for many enterprises. Software and the legions of open communities who wield it, adapt it daily, and can run around and through traditional barriers to entering markets, changing the ways of making, selling, and running companies. Software companies have fundamentally and permanently changed customer expectations to the extent that all companies must become digital or digitally enabled.
We know why good companies with good intentions come to Pivotal. They want to become great at building softwareto make it their talisman that helps them become tomorrows great company, or to tap into and preserve the domain expertise that they have worked so hard to achieve.
Software is deeply connected to change. But Ive talked to customers at Pivotal who cite how theyre retooling how they do software, as a time-to-market exercise. And I ask, is this what its really about, does it stop there? It doesnt.
Its more than a software platform that ties in a great data fabric and the analytics, more than microservices, and more than developers and IT teams who embrace DevOps, agile, and twelve factor app processes. Its all of that. But the decentralization and autonomy this creates affect how the connected parts of the enterprise will function to make the rate of app development, new workflows, and new processes succeed in what will be a reinvented company. At the end of all this technology, processes, reward systems, are clients who have customers who buy and renew services. Digital transformation puts these customers at the beginning of the conversation, not at the end.
Get Everyone Curious
Many companies tick through their compliance, overhead, and governance processes. It is rare when anyone questions why these things are in place and for what purpose. Curiosity across an organization is a big part of success in this new calculus. When an entire organization becomes curious, youre on the path to change. One of the things that triggers people across organizations to be curious is that they feel included. They think, hey, we can connect to this change and I understand my role in it. They also learn to frame every question with the customer or end user in mind. When we work with our clients we too need to be curious and ask the right questions. One of our early transformation clients struggled for months to build out their engineering team despite our support and guidance. For months the lack of progress baffled us until we asked some different questions, took a different approach than before and realised they had outsourced their sourcing and recruiting and were struggling with a new approach. Taking time to sometimes ask the obvious question of our clients can be the most efficient path to solving problems. Dont assume.
Shipping code every day gives developers and IT teams a tangible sense that theyre doing profound work. And once new code cycle is set in motion, it impacts working relationships across the organization. There is more demand for facilitators over lone heros, leadership and coaching over command and control.
There needs to the same sense of reward for an executive sponsor, or line of business owner, or for those who have to retrofit facilities to allow for a agile development. And security and compliance needs to become a true friend and ally. When we reinvent the HR, Finance, and ops roles, and deeply connect this new work to the overall transformation, we give a sense of purpose and inspiration to an entire company. Having said that, it is hard work and a long road but the beauty of the agile way is that while the road is long you get to see progress at every turn.
Its Really Not About You
When I started 11 years ago in AAAknown for its towing serviceswe built built an entire software operation around speed and volume, keeping extra tow trucks in reserve so we could hit the speed metric. The only problem with this was our sophisticated Net Promoter scoresour key tool and process to measure customer satisfactionwent from average to average.
Someone finally asked: Is it really about speed? It turns out that we were all about speed. The customer, as we found out through an intensive listening process, was about accurate time of arrival. Long story short, I led a big shift in software and operations around accurate times of arrival. It was cheaper, but more importantly it changed us entirely as a company. And our Net Promoter scores went way up.
This is a classic hurdle. A lot of big customers believe they have a sophisticated understanding about what customers need. The lesson for transformation and any company is to unearth the real thing that is driving the company and business, and not to drive what you think it is.
The lesson for transformation and any company is to unearth the real thing that is driving the company and business, and not to drive what you think it is.
The difference between doing what customers want, and listening and getting to what they need is worlds apart. If you can articulate this, and be religious about it, you have something special. In Pivotal Labs we have wonderful fail-safes to avoid going down rabbit holes of what our teams could do as opposed to what they should do. Most companies dont have these fail-safes, to remind them that its not about them. Show me an executive team who does not believe they are key to the success of the company. They are in fact a part of it, but if they really empower and educate their teams they become less critical. In my last role as a COO the executive team, while attempting to introduce a new culture tried a few surprising approaches. The most effective experiment was shutting down email for all VPs and above for two weeks. The amount of planning and risk assessment around this activity was insane and the results were..yes, nothing broke, in fact things moved just a little faster.
It starts with a simple user story. At the end of the day our purpose is to ensure the customer has a good experience, and thats true whether youre producing software or selling Tide. As we say at Pivotal, make the right thingnot what is dear to you or makes your head spin, but the right thing for the end user.
In the end, if you strip down all the things that surround us, the processes and the titles, theyre essentially comforts. The minute you flatten organizations, you become vulnerable, and that understandably makes many people uncomfortable. Its very brave, and honorable to make leaps to do the right thing. Over time it always pays off.
This holds true for software transformation. It requires multiple shifts in order to stay coordinated through change, but people have been doing industrial-size change for hundreds of years. Most people do find the path to reinvention surprisingly refreshing. Lets make sure the changes that software transformation relates to are practical things, where folks are included, and have a basic roadmap.




TECH:
AI is Transforming Software Development
The world is advancing rapidly. Various computer programs and software are coming up on a daily basis and have a significant impact on how people go along their regular businesses. However, Artificial intelligence (AI) ranks high as one for the advancements which are radically transforming the technological space.
Artificial intelligence (AI) is a branch of computer science which pays particular attention to the development of intelligent machines which have the capability of reacting like human beings. Such tools and software can engage in activities such as speech recognition, learning and problem solving among others. The development of this area has a significant impact on the field of software developers. Artificial intelligence (AI) is changing your work as a software developer in the following ways.
Testing process
The process of testing software has been made fast by the advancement of Artificial intelligence (AI). The algorithms used by AI are so powerful that developers will have fewer tasks. They will not need to use all the testing scripts. Additionally dealing with vast amounts of data will be a thing of the past. AI tools have the potential of going through these large files, sort them efficiently and increase the accuracy of these program within the shortest time possible. As such, they will eliminate guesswork and human errors. The AI tools will show you and those involved in the mobile app development process where the flaws are and show them how to correct such mistakes.

Enhancing the Creativity of Human Beings
`The brains involved in all forms of software need to be creative in a bid to become competitive in the ever-changing world of technology. Users want software, applications and programs which they can efficiently use and interact with. It is at this point that coding becomes extremely important to these developers. AI tools improve the functionality of such applications by shortening the long coding process. These tools can be used from the inception of an innovative idea until the launching of the mobile application or the software. The tools have significantly assisted in moving the technology sector forward.
Testing Process
Due to the growing competition and limited time, software developers are launching new products into the market without testing them adequately. It is such a big mistake. Additionally, budget constraint is a major contributing factor to these unfortunate happenings. However, with the AI tools, this is a story of the past. AI tools eliminate the need for manual testing. They achieve this crucial task by employing specific data sets to the software and single out where errors are likely to occur. When you use AI testing tools, over 80% of testing job can be done over a short duration of time.
Dealing with bugs
Bugs are a significant interruption to the process of app development and software testing. When they enter the system, developers cast doubt into their capabilities. With the entry of AI into the software space, this question comes to an end. These tools deal with bugs and have the capacity of detecting and eliminating small errors which can interfere with the coding process. You can use the information they generate from these tools to know the modifications needed to keep away bugs from the process of mobile app development and software development. It is a huge in this field which is transforming lives.

TECH:
HOW IS DIGITAL TRANSFORMING BANKING?
Its been a decade since the financial crisis, but you still live the pain. Cost-cutting, compliance with tight new regulations and new digital-first competitors have changed the market  and ratcheted up the pace of innovation. The industry has never faced such radical change and monumental challenges.
Aggressively seeking to catch up and reinvent themselves, banks are embracing digital transformation to grow, compete and win. With customer expectations being wildly different than they were just a few years ago  banks are accelerating their digitalization and putting the spotlight on critical digital business demands, including:
	Transparency
	Automation across the entire IT infrastructure
	End-to-end business processes
	Improved UX and customer experience
	Delivering new, innovative services to differentiate
How do you know where to start and how can you accelerate your digital transformation? Software AG can help you overcome obstacles around modernization and strategically position for the future to ensure your business is poised for growth. Our open, independent software leverages your existing systems and readies you for future tech. Use software as a competitive advantage to help align IT and the needs of the business, optimize automated processes and business logic, leverage data for real-time decision-making and action, and transform into a cost-effective, agile digital bank.

TECH:
How to remove software as a barrier to digital transformation
When software doesnt work, it makes routine tasks and processes more difficult. Heres how to turn technology from a barrier to an enabler of your digital business
Research shows a consistent majority of business users have significant issues with enterprise software
 
Software is now central to the lives of every consumer. In the early days, a less-than-perfect software experience was tolerated. But today  whether it be song downloads, video calls of food deliveries  consumers expect technology to just work.
So why are expectations lower in the office? The words enterprise software often conjure thoughts of complex systems that are clunky, slow and difficult to use.
Instead of the software working for the business, users often work around the software. Multiply that experience across many software applications and the headache can get substantially bigger.
Research shows a consistent majority of business users have significant issues with software  from insufficient capabilities to applications that dont meet requirements. Software-as-a-barrier (SaaB) has become a common model in enterprise software.
SaaB makes routine business tasks and processes more difficult because of technology that doesnt work well together. It complicates, rather than simplifies. And it is in many corners of the enterprise, from HR to accounting and financial systems to ERP.
When software doesnt work as intended, the business rejects it and people dont use it. Its why so many life sciences organisations still resort to spreadsheets to track content, quality changes, regulatory information, and clinical trial data.
Focusing on the business
When SaaB is removed, people become empowered. Take the dreaded expense report. Hunting down receipts to create and submit an expense report is burdensome, but cloud software changes the entire process by automatically tracking expenses in real time. The routine task of creating expense reports is now fully automated and simple.
Many organisations are starting to remove SaaB to take advantage of new digital opportunities. For example, commercial operations are using digital to gain better intelligence of market dynamics, holistically manage commercial content globally to stay compliant, and create closer connections with customers.
The word of the decade will be digital, a recurring theme. In life sciences, breaking down SaaB presents a significant opportunity, for example, to transform engagement and collaboration with healthcare professionals and create a more efficient digital supply chain.
Digital is disrupting every industry. In a survey of almost 1,200 c-suite decision makers, 90% agreed their business must to evolve to thrive in a digital world, while 98% said their business or organisation has already been disrupted.
In life sciences, the commercial business model is still pre-internet and has yet to be disrupted. Companies approach digital channels differently  theyre often disconnected or theyre use is limited. This severely impacts doctors experiences with pharmaceutical companies, especially in relation to how they interact with companies in other industries.
Consequently, its difficult for busy healthcare providers to get the new drug or product information they need, when and where they want it. Engagement is becoming more digital, but SaaB is preventing life sciences from making the transition to running a digital business.
The changing expectations of healthcare providers and their patients will drive transformation. With more drug innovations, life sciences organisations will need to create more targeted and flexible content that better informs physicians and patients.
Additionally, healthcare providers will want to connect and engage with life sciences companies through digital channels for the information they want, when and how they want it. These dynamics are prompting many companies to change quickly and replace SaaB.
Enabling digital transformation
It is clear that software will drive innovation in commercial and enable the new digital business in life sciences. But in order for that to happen, companies need to remove software as a barrier to innovation. Here are a few steps you can take now.
1. Get more efficient in the cloud
Start by modernising your operations in the cloud. By 2020, 67% of all enterprise IT infrastructure and software spending will be for cloud-based offerings. Additionally, the use of industry-specific cloud applications is expected to increase, with half of the respondents of the Future of Cloud Computer Survey indicating they are or will use an industry cloud offering within the next 24 months. Focus on areas that are most strategic to your business and that have the most opportunity to provide a competitive advantage, such as customer-facing applications or content management systems. If these systems are creating complications today, its an area ripe to take out SaaB and move to the cloud.
2. Streamline processes across your digital supply chain
As companies transition to operate in a digital marketplace, commercial organisations face the daunting task of creating, reviewing, approving, distributing and withdrawing commercial content at internet speed. Typically, the root cause of an ineffective digital supply chain is too many fragmented processes and systems. As a result, the wheel is reinvented in many areas  every brand across the company figures out and implements its own website or digital engagement strategy. Everyone pays for this inefficiency.
Removing SaaB can transform how brands operate and drive new, repeatable processes across the digital supply chain. When the digital ecosystem is simplified, a significant amount of time, cost, effort and compliance risk can be reduced when getting content to market and, ultimately, help the business capitalize on new market opportunities.
3. Create a partnership with IT
Fundamentally, the proliferation of SaaB happens when people with competing priorities make decisions that impact the purchasing process. The end result is software that doesnt work for anyone. So partnership between the business and IT is critical.
Many CIOs now have a cloud first mandate in the life sciences industry and their role is shifting to leading their organisations digital transformation. The opportunity for commercial teams is to align on priorities with IT and identify technology limitations where SaaB is holding the business back.
 
Sourced from Matt Wallach, co-founder and president, Veeva
Sign up for our weekly email and get the latest tech news straight to your inbox.
Whitepapers
 
From ECM to Content Services: Evolution or Revolution
The move from ECM to Content Services has been well debated - but what does it actually mean for end-users?	
Practical Techniques for Data Preparation
Practical Techniques for Data Preparation, the first how-to guide on data wrangling.	
TECH:
From refugee aid to CPR: Changing lives with mobile apps powered by Microsoft technology
Mile Pettersson was in his office in Gothenberg, Sweden, when his cell phone alerted him that someone was in cardiac arrest nearby and needed help. A former firefighter, Pettersson jumped in his car, drove 2 miles to a crowded shopping area and found an older man on the ground, surrounded by people who didnt know what to do. He quickly performed CPR until an ambulance arrived about 10 minutes later.
The man survived, thanks to Pettersson, medics and Heartrunner, a mobile app that alerts CPR-trained volunteers of nearby cardiac-arrest victims and shows the closest automated external defibrillators. Available in three regions  two in Sweden, one in Denmark  Heartrunner has over 50,000 trained and registered volunteers and increases the rate of bystander CPR, a significant factor in survival rates.
An ambulance can take 10 to 20 minutes [to arrive], so someone coming in 2 to 5 minutes means a bigger chance to survive. This is why I do it, says Pettersson, who retired from a 30-year firefighting career and now teaches construction safety.
Heartrunner is among a growing number of innovative, mission-driven, mobile apps that are changing lives and solving global problems with the help of Microsoft developer tools and cloud services. They include Visual Studio Tools for Xamarin, which enable developers to build native iOS, Android and Windows apps from a shared codebase and Visual Studio App Center, which can automate an entire app lifecycle, helping developers ship powerful apps faster.
Organizations of all sizes and industries use Microsoft technology to efficiently build, test, monitor and update their apps, but for nonprofits and small teams focused on social good, the benefits are compounded. The technologies help them save time, reach many people, build quality apps and focus limited resources on helping others  instead of managing multiple codebases or spending cycles on manual work.
Tragedy inspired a new idea to save lives
Eirik Lie, left, Heartrunner lead application architect, and Tommy Nilsen, Heartrunner software developer. Photo by Monica Beate Tvedt.
The inspiration for Heartrunner began in 2007, when a cardiologists morning bus commute stalled in traffic. He later learned the reason for the delay: A woman had suffered a cardiac arrest at a bus stop and died.
He was thinking you can order a pizza and find the closest laundromat with a cell phone. But why couldnt anyone match his life-saving skills with her need for help, when he was so close? says David Fredman, operations manager of Heartrunner Sweden, which owns and distributes Heartrunner. The app was built by Norwegian company UMS, a firm specializing in citizen alerts in various cities and industries.
Researchers at Swedens prestigious Karolinska Institutet studied the cardiologists idea, finding that phone alerts, triggered by emergency dispatch calls, drove a 30 percent increase in citizen bystander CPR in cardiac arrest cases. Their study was published in 2015 in the New England Journal of Medicine.
The same year, UMS rebuilt the app for Android and iOS with Xamarin.Forms, which enabled the first-time mobile developers to share 80 percent of code across platforms and release the apps in just six months. Quality is critical to UMS, and the three-member Heartrunner team relies on automation, including continuous, automated builds and UI testing with Visual Studio App Center. That helps the team reach more volunteers and people in need, and meet users expectations for how the apps should look, feel and behave.
Heartrunner Sweden, which wants to make emergency care as on-demand as hailing a ride, has dispatched 12,000 CPR-trained volunteers in thousands of cardiac-arrest cases  an early-intervention movement thats saving lives.
We get feedback from people who have been saved because of our app, says Eirik Lie, Heartrunners lead application architect. Its a good feeling.
Helping urban refugees find aid
From left, Urban Refuge Jordan liaison Vicky Kelberer and assistant professor Noora Lori work with Boston Universitys Software & Application Innovation Lab members Andrei Lapets, San Tran and Frederick Jansen. Photo by Erika Sidor.
A continent away in Boston, the desire to make a difference also drives the all-volunteer team Urban Refuge, whose aid locator apps help urban refugees find humanitarian resources in Amman, Jordan. The team grew out of a Boston University class on forced migration that covered Jordans 2.7 million refugees and included an assignment to create a practical, digital solution to help people.
Students focused on urban refugees  refugees who dont live in a camp  who often struggle to find essential services like housing, job placement and cash assistance, relying only on word of mouth.
The class developed a solution for smartphones, something most refugees in Jordan have readily accessible. Students partnered with Microsoft technical evangelists to create a mobile experience that makes it fast and easy to search for aid by category, see a map of geotagged service providers and get directions to aid offices.
As a nonprofit with limited funds and resources, Urban Refuge needs to be prepared to scale, while getting the most out of every moment. Xamarin.Forms helped the small team maximize its development resources and time, share code across platforms to deliver on Android and iOS apps, and ensure content was localized in Arabic and English. Team members have curated a database of nearly 200 Amman-based aid organizations and are in a second round of beta-testing, automatically distributing new builds to on-the-ground testers with Visual Studio Team Services and HockeyApp, now part of Visual Studio App Service.

Most of the students have graduated and have demanding, full-time jobs, but none of us are stopping, says the class teacher, Noora Lori, an assistant professor at the Pardee School of Global Studies at Boston University. The ultimate goal is helping people.
For Michelle Abou-Raad, Urban Refuges director of operations and a former student, the work is about ensuring that the class project will make a difference and benefit those in need. She has traveled twice to Jordan for research, where she met refugee women who inspired her and the entire Urban Refuge team.
Despite their struggles, theyre trying to make ends meet and support their families, she says. I want to keep my promise that I will have an impact on them in some way.
Supporting charities and personal causes
Technology also plays a central role in the mission of London-based JustGiving, whose social-giving platform drives donations for thousands of charities around the world. More than 23 million people have used JustGiving to raise $4.5 billion in donations since its founding in 2001, by creating and sharing pages for causes ranging from fundraising for cancer research to crowdfunding aid for recent bombing victims in Manchester, England.
The vision of the founders was to make sure no good cause in the world goes unfunded, says Mark Gibaud, lead developer of mobile apps for JustGiving.
With Microsoft technology, including Visual Studio Tools for Xamarin and Visual Studio App Centers Test service, the team shipped its iOS app, followed by an Android version, built by a single developer in a mere three months. Sharing code across platforms streamlines any app updates and automated testing means faster deployment, better apps and more giving.
The JustGiving team. Photo courtesy of JustGiving.
Great app, spun up my page yesterday, shared on Facebook, Messenger and WhatsApp and emailed link to my dinosaur friends, an iOS customer recently wrote in a review. Already got 10 percent of my target in less than 24 hours.
Gibaud says the review is typical, with fundraisers who use JustGivings mobile apps raising 10 percent more money than fundraisers who use the companys desktop and mobile web platforms. The apps provide a richer engagement and moments of delight, including the sound of a coin clanking in a jar when someone donates. That triggers a reinforcement loop of fundraisers sharing pages more often, which leads to more donations.
As a testament to the teams success, Blackbaud  a U.S.-based software company that serves nonprofits around the world  recently acquired JustGiving. The success also motivates JustGiving to want to achieve more.
Were fully aware that apps are the future, Gibaud says. Theres so much potential for a richer exchange with your user base that were in a rush to explore it more.


TECH:
Legacy System Modernization: How to Transform the Enterprise for Digital Future
Share: 
1. The state of software
Dining at a fancy restaurant, you want to spend some quality time, enjoying tasty food and drinks. When choosing the latter, the chances are, you will prefer a glass of good wine, the older the better. For that matter, we all know old wine and old friends are the best.
The problem is, unlike wine and friends, software doesnt get better with age.
Just think about it. There is a good reason your new computer runs Windows 10 instead of Windows XP.
Obviously, your current computer is more powerful and more capable than the one you owned 10 years ago. Similarly, the business you run is not the same as it was when you started it. Therefore, using outdated and underperforming software to manage it is analogous to running Windows XP on your new ZenBook.
1.1 What is legacy system?
Do you remember the last time you used a pager? Probably, it was in the late 90s. But the technology is not as dead as you might have thought. In fact, your own life might depend on it as 90 percent of hospitals surveyed by HIMSS Analytics rely on pagers as a cost of doing business.[1]
Aside from being outdated, pager technology is a huge source of expense, costing a hospital on average around $180,000 per year.[1] So why is it still so widely used?
The answer is simple: Some systems are just hard to replace  especially the ones that handle vital business processes within an organization.
Pagers in health care are not the only example of such a phenomenon. Commonly referred to as a legacy system/technology, it is relatively widespread in a number of other industries, including banking, finance, insurance, and transportation.
As defined by Gartner, legacy application is an information system that may be based on outdated technologies, but is critical to day-to-day operations.
A number of examples of such legacy systems can be found across some major federal organizations. For example, a system, used by the Department of Defense to operate the US nuclear forces, including intercontinental ballistic missiles, nuclear-capable bombers, and tanker support aircraft, runs on an IBM Series/1 Computer (a 1970s computing system) and uses 8-inch floppy disks.[2]
However, a legacy system is not always defined by its age. It might be due to the lack of support or its inability to meet the needs of a business or organization that a system is considered to be legacy. Such software is usually difficult (or impossible) to maintain, support, improve, or integrate with the new systems due to its architecture, underlying technology, or design.
1.2 The hidden costs of legacy software
Many companies continue using outdated systems, regardless of its age or the quality of underlying technologies. The software has been working just fine for decades and is still able to cope with most of its tasks, they argue. Indeed, why fix it if it aint broke?
Actually, there are quite a lot of reasons to fix your legacy systems. The real cost of running such software is the major one among them.
According to some estimates, about 71 percent of the IT budget slated for US federal civilian agencies will be spent on maintaining legacy systems next year. This equals more than $34 billion. To put that into context, only $3.1 billion are assigned for the IT modernization in 2017.[3]

Yet, these numbers represent just the tip of the iceberg. The hidden costs not stated in any budget are even bigger. Namely, there are several sources of legacy software expenditure.
1.2.1 Maintenance and support
The costs of legacy system maintenance operations include the following:
	Updates and changes
Legacy systems are typically quite large in terms of code base as well as functionality. Taking into account their monolithic nature, you cannot just change or replace one system module. A small update might result in multiple conflicts across the system. Thus, any change or update to the legacy system requires time and effort, neither of which come cheap. Additionally, legacy systems usually have vast amounts of documentation as well as a number of undocumented features. So, there is always a certain risk involved when interfering with the source code.
	Infrastructure
Just like software itself, the underlying infrastructure becomes harder and more expensive to maintain as it ages. Legacy systems often require a specific technical environment, including hardware. Thus, the infrastructure maintenance spending remains high, as compared to modern cloud-based solutions. Legacy data represents another significant infrastructure issue. Being scattered across several databases and storage resources, it is hard to reorganize for increased storage space optimization. Gathering and systematizing legacy data manually to further transfer it to the new database is a time- and cost-intensive task.
	Staff training
Depending on obsolete technologies, the legacy system support and maintenance requires a specific set of skills and expertise. While the developers who have built the software might retire or switch to other technologies, it becomes increasingly harder to find and retain the right talent. Dedicated staff training might be an even bigger source of expense.
1.2.2 Integration and compliance
Modern software platforms often rely on third-party APIs to access a few capabilities, such as geolocation, user authentication, data sharing, and transactions. For example, Uber relies on the data provided through the Google Maps API for its core functionality  an ability to find and track cars nearby.
Indeed, why reinvent the wheel, when you can use the existing, tried and true solution at a fraction of the cost?
Modern technologies are integration-ready by default. API vendors typically provide support for most of the programming languages and frameworks out of the box. Yet, obsolete or rare technologies typically lack the compatibility.
Connecting a legacy software to a third-party tool or service often requires a significant amount of custom code. And there is still a chance that the final integration wont work as well as intended or that it will work at all.
Another aspect of legacy systems that comes at a high cost is compliance. This is especially true for heavily-regulated industries, such as finance or health care. Similar to the introduction of the HIPAA omnibus rule back in 2013, when many providers were unprepared to change their systems in order to comply with the new regulations, any new standards set by the government could lead to millions in penalties, if you fail to adapt to them.
1.2.3 Security
According to the Better Data, Better Decisions (BDNA) State of the Enterprise Report, out-of-date software and hardware assets represent one of the core vulnerabilities that typically plagues almost all corporate and government networks.[5] Indeed, legacy systems are usually less resistant to cyber attacks, harmful programs and malware, which is only logical. If the software solution had been around for years, the attackers most likely had enough time to get familiar with the code and find its vulnerabilities.
Another reason for this is that outdated software might no longer be supported by the vendor. This means that no patches are provided and no one keeps the system compliant with the latest security requirements.
Every piece of hardware and version of software that is not maintained regularly could be a gateway for unauthorized access.[5]
 BDNA State of the Enterprise Report (Q2 2016)
Even if your system is custom-built and you have the resources to maintain it, adding more patches means additional investment in security. In the worst case, this might resemble a leaky bucket, where you get a new breach as soon as the previous one is fixed.
1.2.4 Lost business opportunities
By investing in legacy software support and maintenance, you leave less room for innovations. Instead of adopting new technologies and business models, you are stuck with your old software, letting new opportunities in your industry go unnoticed. This leaves your competitors more openings to outperform you and take over your market share.
A study found that mobile banking solutions might result in 72 percent higher revenue for banks.[4] Thus, a lightweight mobile application provided by a competing bank can lure away some of your customers. In this case, the cost of a missed opportunity equals the revenue your competitor will get from newly acquired customers.
Additionally, Javelin Strategy & Research found that an average cost per mobile banking transaction equals just 10 cents to process, while processing an offline transaction at a physical bank costs $4.25.[6]

Such overhead is another significant element that adds to the cost of missed opportunity. While mobile banking providers save more on efficient and low-cost ways to process transactions, you might still be using outdated POS terminals and manual assistance to do the same job.
1.2.5 Organizational agility and efficiency
Talking about business opportunities, timing proves to be crucial. How fast can you respond to the market challenges? Will it take weeks to adopt new technologies and solutions? Or rather several months? The truth is, in most cases, businesses bound to legacy systems lack organizational agility to adapt to the upcoming challenges.
Indeed, 73 percent of the business leaders surveyed by IDG and Dell list business agility as a critical or very important objective of IT modernization.[7]
Partially due to their inability to be updated and modified, legacy systems can hold back innovation, resulting in significant losses. Moreover, outdated, cumbersome software is less efficient, which has a negative impact on the employee productivity.
Machine-reengineering, the business processes automation with the help of machine learning, is a great example of how innovation can optimize business performance. Based on Harvard Business Review research, more than a third of the early adopters saw gains in bottom-line performance using machine-reengineering to slash 15% to 70% of costs from certain processes. At the same time, some saw a tenfold improvement in workforce effectiveness or value creation.[8]
For example, the resource lists a number of successful cases of processes reengineering. Namely, a financial services provider implemented a biometrics solution based on voice recognition. The company was able to eliminate a four-step authentication process by using customers voices as passwords. This resulted in 50 percent more efficient call routing, the improvement achieved largely due to the use of advanced technology.
 
2. Preparing for the Digital Future
Despite the problems and risks related to the outdated software, a majority of the companies surveyed by the University of Surrey are still lacking legacy-modernization initiatives. The research shows that most of them would only consider reengineering the current solution in case of an emergency, such as a complete system outage.[9]
Although many core systems have become efficient and stable over the many decades of their existence [], they are now seen as a key obstacle in the rapid evolution to a fully integrated digital business.[9]
 University of Surrey report
To bridge the gap between the current offerings and customer expectations, companies need to rethink their business models, making them digital-ready. Yet, the legacy software is only one aspect of the problem. Sometimes, a far bigger issue is the mindset that comes with it. That is why proving a business case for software modernization is the first challenge to be faced by the initiating party.
2. 1 The business case for software modernization
The following benefits prove that legacy-system modernization is a vital part of the overall business digitization.
 
Yet, despite all the benefits, the resistance to modernization is often well-grounded.
2.2 Considering the risks
Two major arguments are typically used when talking about a software modernization initiative. Those are the time and cost involved. Indeed, a solution that took a team of developers years to implement cannot be re-created in a week, even if you hire twice as many developers to handle the task. Thus, in some cases software reengineering cost might exceed the initial investments.
Yet, there are more pitfalls to avoid. Some of them have been described by a group of  Carnegie Mellon University researchers back in 1999. The report Why Reengineering Projects Fail lists the following reasons for legacy reengineering effort failure:
1.	The organization inadvertently adopts a flawed or incomplete reengineering strategy.
2.	The organization makes inappropriate use of outside consultants and outside contractors.
3.	The workforce is tied to old technologies with inadequate training programs.
4.	The organization does not have its legacy system under control.
5.	There is too little elicitation and validation of requirements.
6.	Software architecture is not a primary reengineering consideration.
7.	There is no notion of a separate and distinct reengineering process.
8.	There is inadequate planning or inadequate resolve to follow the plans.
9.	Management lacks long-term commitment.
10.	Management predetermines technical decisions. [10]
Therefore, successful software reengineering requires a solid modernization strategy and great attention to detail. In this regard, we can share some of the best practices and approaches we have developed at AltexSoft.
3. Legacy Software Modernization Best Practices
There are two approaches to dealing with the legacy problem: revolutionary (big-bang) and evolutionary. Yet, both have benefits as well as drawbacks.
The revolutionary method revolves around developing and carrying out a legacy system replacement strategy. Its implementation requires shutting down the old system and building a new one from scratch. The approach might be considered extreme, but sometimes it is better to retire the system completely to avoid some serious damage, such as security breaches, lost data, system downtime. Or it can be applied in a case when the original product cannot solve the existing business problems anymore, so it makes no sense to reengineer or port it to the new technologies.
The evolutionary approach presupposes a systematic, step-by-step software modernization process. It is usually less painful: It does not disrupt the major business processes and implies significantly lower risks for the company. Yet, it often turns into a band-aid approach, where you focus on solving the problems instead of removing the factors that cause them.
3.1 Legacy system assessment framework
Often dealing with legacy systems, we at AltexSoft have developed our own approach to choosing an appropriate way to modernize business-critical software. Namely, we take several steps first to assess the existing solution.
Technologies Analysis
The first step in our plan is to identify and analyze the technology stack of the existing product. Thus, we know if the programming language or frameworks used are still relevant and supported by the vendors. If the product relies completely on outdated technologies, the chances are we would need to completely rewrite it in the process of modernization.
Architecture Audit
In case the tech stack (or some parts of it) is still relevant, it is necessary to conduct an architecture audit. This will help you define the system elements which are functioning well and focus on the ones that need modernization. Plus, you will be able to see how different parts of the system interrelate, so that your future changes wont affect the whole product.
Code Review
Legacy software usually has an excessive codebase, requiring regular reviews and refactoring. If not treated properly, the software tends to rot. This might lead to more design flaws and conflicts as you try to introduce new features or update some parts of the system. That is why, as a part of any modernization or changes, we typically conduct a complete code review, assessing the quality and updateability of the systems source code.
UI/UX Review
The same principle applies to the UI and UX design. A thorough design review is required to understand which parts of the system interface need a facelift.
Performance Testing
Performance testing aims at uncovering further potential issues with the legacy systems. Poor performance or major flaws can serve as a reason for a complete system reengineering as well as selective improvements.
Current Requirements and Opportunities for Future Growth
While considering the current business needs and requirements articulated by the client, we also focus on opportunities for the future growth. Thus, we help you make an informed decision by providing a well-grounded and unbiased opinion on the software modernization options.
Considering the results of the above-listed evaluation stages, we can offer three possible ways to modernize legacy software.
 
3.2 Legacy modernization approaches
 
3.2.1 Migration & Enhancements
This is one of the most popular approaches to application modernization and the easiest way to make sure your product will keep serving your needs for years to come. It presupposes the system migration (typically re-hosting, using cloud solutions) and some minor enhancements. This includes UI/UX updates, performance optimization, and database migration.
Yet, this method has a number of limitations. Namely, the core business logic and architecture mostly remain unchanged, as this type of changes require a more invasive approach.
3.2.2 Correction & Growth
If the product technology stack is relatively modern and does not represent a threat for future product growth, modernization can involve some minor enhancements/corrections. This might be architecture optimization or code refactoring, UX updates or performance optimization without significant changes in product business logic.
As soon as the product is up to date, you can add more features on top of it. These might be third-party integrations or custom-built modules.
 
3.2.3 Complete Software Reengineering
Considered the most extreme method, features extraction relies on your business strategy and growth outlook. This means, in order to reengineer the product, you need to identify the features that are still crucial to your business and the ones that are no longer used or required. After that, the required features are prioritized and modified if needed.
Taking the legacy system as a basis, the team creates an up-to-date product with matching capabilities, but better performance, look and feel, modern technologies and scalable architecture. Depending on the functionality analysis and prioritization, the new product might 100% match the previous version in terms of functionality, or lack some features that are no longer required/used.
4. Checklist for successful application modernization
Aside from adopting the best-suited strategy for your technology modernization initiative, there is more you can do to make sure the process runs smoothly. Here is our checklist of 7 things to consider for a successful software modernization project:
1. Assess the current state of legacy systems.
Legacy software does not always fall under old or outdated definitions. There are more aspects to assess when identifying the legacy. That is why you need to assess all systems in place to uncover the current and potential issues it can bring up in the near future. The assessment should be systematic and detailed: Study all aspects of your technology, from code and architecture to visual look and feel, taking into account your future business plans for product growth.
2. Select the modernization method that would be the fastest to deliver value.
Based on the assessment conducted at the first phase, choose the modernization approach that best fits your needs and will help you deliver results fast. Aside from the three approaches we have listed above, consider existing products you can use instead. There is no need to reinvent the wheel if there is an SaaS solution available at a fraction of cost. Yet, if your system solves rather specific tasks or you want to be able to build more features on top of it, custom product development services might be right for you. In this case, adopting agile software development practices can help you speed up the process and deliver value fast.
3. Rethink the architecture and prioritize for simplicity.
Legacy systems often fail to perform as needed due to their overly complex structure. When modernizing your system, less is more in terms of both architecture and functionality. Start by implementing only the most important features. Consider a microservices architecture approach to make your product scalable. Additionally, make sure the newly released application will work well with the rest of the tools used in your business by default. If you plan to change any of the tools soon, consider several possible options and keep their requirements in mind when building your application.
4. Choose the technology stack to deliver optimal performance and user experience.
When reengineering your system, make sure you use a solid and future-ready technology stack. The choice of technologies should completely depend on the product specifics. Consult with your internal IT staff or address a professional tech consultancy. The right tech stack contributes to building a performant, reliable and efficient product. Adopt a solid quality assurance and testing process to deliver the best results.
5. Document for future system growth.
To avoid the same mistakes that made you reengineer your current solution, introduce (or adopt best practices used by other companies) a set of coding standards and internal processes. Orderly documented and clean code make your software easy to understand, extend and maintain in the future.
6. Create a separate support and retirement schedule for your legacy system.
Even if you have a brand-new system running like a clockwork, you will still need your legacy software, just in case. So, dont kill it all at once. Document and archive your solutions so you can easily access and refer to them when needed. Therefore, you need to support your legacy system for some time and plan for retiring your legacy system only when your new product is up and running.
7. Budget for training and system updates.
Working with the old systems for years, your employees might need some time and guidance to master the new software. So be ready to invest in staff training for better performance and efficiency. Additionally, plan for regular system updates. If you fail to keep your product up to date, you will soon face another modernization challenge.
Conclusion
Regardless of the chosen approach, software modernization is a complex, labor intensive and risky process. Yet, the results seem well-worth the risk.
According to a recent Gartner survey, 45 percent of respondents state that one of the current top 5 IT project priorities is application modernization of installed on-premises core enterprise applications and a further 41 percent say that extending capabilities of core enterprise applications is a top five priority.[11]


TECH:



Is simple always better? And if so, how simple? Its already understood that unnecessary complexity is counterproductive and undesirable, but then so is oversimplification. After all, too much of a good thing can be a bad thing.
In the development of business software, this is a frequent conundrum. When the underlying process is an intricate one, can the technology be easy to use and still retain the necessary features to perform effectively? In essence, can simplicity and complexity (usability and functionality) coexist in the same software system?
Our new Insight IT piece, Simplify: Combating Software Complexity to Achieve a Transformation in Productivity, identifies three crucial aspects of digital strategy that CIOs can target for simplification to boost workplace acceptance without sacrificing functionality. Read it today  its got some simple advice for addressing this complex problem.


TECH:
3 Insights on Digital Business Transformation and Software Support
1. Software is a major enabler of digital business transformation
According to ZK Research, there are 3 primary drivers of digital transformation: transform business models, create new customer experiences, and empower workforce innovation. These drivers all require a much higher level of business agility. Software is much more dynamic and agile than hardware, so it better enables digital transformation.
As Zeus aptly pointed out, software plays a key role at each step in the journey of becoming a digital organization.
For starters, the agility of software enables companies to align IT with business outcomes.
The automation capabilities of software can help improve productivity and empower the workforce. For instance, automating processes such as expense reporting and audits reduces the time spent on data entry.
A software-driven approach to product development and marketing can deliver new insights on customer behavior, which can help improve service and customer experience.
2. Dont overlook software testing, QA, and support
Several chat participants stressed the importance of good software testing and quality assurance during the development phase to ensure that software performs up to standard. Even for packaged apps, testing shouldnt be overlooked. A few suggested best practices for testing and QA, include:
	Dont over customize. Too much customization leaves more room for coding errors and bugs
	Consider the benefits of SaaS. The QA and testing is moved to the cloud.provider, which drives new features faster
	Shift to a DevOps model to help ensure long lasting, good quality software
Many of our chatters also agreed that once software is deployed, having the right support is also critical. Keeping up with the latest updates, version upgrades, and bug fixes is key to making sure the software continues to perform reliably and reduces the risk of security breaches.
3. Downtime can be catastrophic. Choose software support wisely
No matter the industry vertical, downtime can have a catastrophic impact on any business. Zeus shared some alarming figures from ZK Research on the cost of downtime by vertical. Even one hour can be incredibly damaging ($5.2 million an hour for a brokerage firm!).
Downtime not only costs money, but also can damage a companys brand reputation and cause customers to churn. If a company is bleeding customers, in the end the amount of money lost doesnt really matter. The bottom line is that downtime can be catastrophic, so have the right software support in place to minimize the risks of downtime.
When choosing the right software support, what should you consider? Here are a few recommendations from the chat:
	Understand your internal skills and map them out against your future requirements. Align support services to fill any gaps you have
	Determine what level of risk you can tolerate and align Service Level Agreements to make sure the severity of the issues are addressed accordingly by the subject matter experts you need, when you need them
	If your IT team is unable to keep up with software updates, choose a vendor that can also help you keep up with the routine updates and software upgrades. Maintenance goes a long way toward preventing downtime
	Choose a vendor based on the reputation of its support organization and one that can support your future growth needs
Heres some final food for thought. As one of our chat participants reminded us, software is eating the world. As business and technology require more agility and flexibility that only software can enable, we need to feed and nourish the software with the right support. If you are evaluating any of our Cisco ONE or Collaboration products, learn more on how Cisco can support you.
If you didnt get a chance to join our #CiscoChat, leave me a comment. What do you think is important to consider when choosing the right software support?

TECH:
Machine Learning
What it is and why it matters
Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.
Evolution of machine learning
Because of new computing technologies, machine learning today is not like machine learning of the past. It was born from pattern recognition and the theory that computers can learn without being programmed to perform specific tasks; researchers interested in artificial intelligence wanted to see if computers could learn from data. The iterative aspect of machine learning is important because as models are exposed to new data, they are able to independently adapt. They learn from previous computations to produce reliable, repeatable decisions and results. Its a science thats not new  but one that has gained fresh momentum.
While many machine learning algorithms have been around for a long time, the ability to automatically apply complex mathematical calculations to big data  over and over, faster and faster  is a recent development. Here are a few widely publicized examples of machine learning applications you may be familiar with:
	The heavily hyped, self-driving Google car? The essence of machine learning.
	Online recommendation offers such as those from Amazon and Netflix? Machine learning applications for everyday life.
	Knowing what customers are saying about you on Twitter? Machine learning combined with linguistic rule creation.
	Fraud detection? One of the more obvious, important uses in our world today.
 
Play Video
Machine Learning and Artificial Intelligence
While artificial intelligence (AI) is the broad science of mimicking human abilities, machine learning is a specific subset of AI that trains a machine how to learn. Watch this video to better understand the relationship between AI and machine learning. You'll see how these two technologies work, with useful examples and a few funny asides.
Why is machine learning important?
Resurging interest in machine learning is due to the same factors that have made data mining and Bayesian analysis more popular than ever. Things like growing volumes and varieties of available data, computational processing that is cheaper and more powerful, and affordable data storage.
All of these things mean it's possible to quickly and automatically produce models that can analyze bigger, more complex data and deliver faster, more accurate results  even on a very large scale. And by building precise models, an organization has a better chance of identifying profitable opportunities  or avoiding unknown risks.
 
What's required to create good machine learning systems?
	Data preparation capabilities.
	Algorithms  basic and advanced.
	Automation and iterative processes.
	Scalability.
	Ensemble modeling.

Did you know?
	In machine learning, a target is called a label.
	In statistics, a target is called a dependent variable.
	A variable in statistics is called a feature in machine learning.
	A transformation in statistics is called feature creation in machine learning.
Machine learning in today's world
By using algorithms to build models that uncover connections, organizations can make better decisions without human intervention. Learn more about the technologies that are shaping the world we live in.

Opportunities and challenges for machine learning in business
This O'Reilly white paper provides a practical guide to implementing machine-learning applications in your organization.

Machine learning powers credit scoring
How can machine learning make credit scoring more efficient? Find out credit scoring agencies can use it to evaluate consumer activity to provide better results for creditors.  

Will machine learning change your organization?
This Harvard Business Review Insight Center report looks at how machine learning will change companies and the way we manage them.   


Applying machine learning to IoT
Machine learning can be used to achieve higher levels of efficiency, particularly when applied to the Internet of Things. This article explores the topic.
Who's using it?
Most industries working with large amounts of data have recognized the value of machine learning technology. By gleaning insights from this data  often in real time  organizations are able to work more efficiently or gain an advantage over competitors.
Financial services
Banks and other businesses in the financial industry use machine learning technology for two key purposes: to identify important insights in data, and prevent fraud. The insights can identify investment opportunities, or help investors know when to trade. Data mining can also identify clients with high-risk profiles, or use cybersurveillance to pinpoint warning signs of fraud.
Government
Government agencies such as public safety and utilities have a particular need for machine learning since they have multiple sources of data that can be mined for insights. Analyzing sensor data, for example, identifies ways to increase efficiency and save money. Machine learning can also help detect fraud and minimize identity theft.
Health care
Machine learning is a fast-growing trend in the health care industry, thanks to the advent of wearable devices and sensors that can use data to assess a patient's health in real time. The technology can also help medical experts analyze data to identify trends or red flags that may lead to improved diagnoses and treatment. 
Marketing and sales
Websites recommending items you might like based on previous purchases are using machine learning to analyze your buying history  and promote other items you'd be interested in. This ability to capture data, analyze it and use it to personalize a shopping experience (or implement a marketing campaign) is the future of retail.
Oil and gas
Finding new energy sources. Analyzing minerals in the ground. Predicting refinery sensor failure. Streamlining oil distribution to make it more efficient and cost-effective. The number of machine learning use cases for this industry is vast  and still expanding.
Transportation
Analyzing data to identify patterns and trends is key to the transportation industry, which relies on making routes more efficient and predicting potential problems to increase profitability. The data analysis and modeling aspects of machine learning are important tools to delivery companies, public transportation and other transportation organizations.
Learn More About Industries Using This Technology
Select industry
What are some popular machine learning methods?
Two of the most widely adopted machine learning methods are supervised learning and unsupervised learning  but there are also other methods of machine learning. Here's an overview of the most popular types.
Supervised learning algorithms are trained using labeled examples, such as an input where the desired output is known. For example, a piece of equipment could have data points labeled either F (failed) or R (runs). The learning algorithm receives a set of inputs along with the corresponding correct outputs, and the algorithm learns by comparing its actual output with correct outputs to find errors. It then modifies the model accordingly. Through methods like classification, regression, prediction and gradient boosting, supervised learning uses patterns to predict the values of the label on additional unlabeled data. Supervised learning is commonly used in applications where historical data predicts likely future events. For example, it can anticipate when credit card transactions are likely to be fraudulent or which insurance customer is likely to file a claim.
Unsupervised learning is used against data that has no historical labels. The system is not told the "right answer." The algorithm must figure out what is being shown. The goal is to explore the data and find some structure within. Unsupervised learning works well on transactional data. For example, it can identify segments of customers with similar attributes who can then be treated similarly in marketing campaigns. Or it can find the main attributes that separate customer segments from each other. Popular techniques include self-organizing maps, nearest-neighbor mapping, k-means clustering and singular value decomposition. These algorithms are also used to segment text topics, recommend items and identify data outliers.
Semisupervised learning is used for the same applications as supervised learning. But it uses both labeled and unlabeled data for training  typically a small amount of labeled data with a large amount of unlabeled data (because unlabeled data is less expensive and takes less effort to acquire). This type of learning can be used with methods such as classification, regression and prediction. Semisupervised learning is useful when the cost associated with labeling is too high to allow for a fully labeled training process. Early examples of this include identifying a person's face on a web cam.
Reinforcement learning is often used for robotics, gaming and navigation. With reinforcement learning, the algorithm discovers through trial and error which actions yield the greatest rewards. This type of learning has three primary components: the agent (the learner or decision maker), the environment (everything the agent interacts with) and actions (what the agent can do). The objective is for the agent to choose actions that maximize the expected reward over a given amount of time. The agent will reach the goal much faster by following a good policy. So the goal in reinforcement learning is to learn the best policy.


TECH:
Machine Learning for Humans, Part 3: Unsupervised Learning
Clustering and dimensionality reduction: k-means clustering, hierarchical clustering, principal component analysis (PCA), singular value decomposition (SVD)
This series is available as a full-length e-book! Download here. Free for download, contributions appreciated (paypal.me/ml4h)
How do you find the underlying structure of a dataset? How do you summarize it and group it most usefully? How do you effectively represent data in a compressed format? These are the goals of unsupervised learning, which is called unsupervised because you start with unlabeled data (theres no Y).
The two unsupervised learning tasks we will explore are clustering the data into groups by similarity and reducing dimensionality to compress the data while maintaining its structure and usefulness.
Examples of where unsupervised learning methods might be useful:
- An advertising platform segments the U.S. population into smaller groups with similar demographics and purchasing habits so that advertisers can reach their target market with relevant ads.
- Airbnb groups its housing listings into neighborhoods so that users can navigate listings more easily. 
- A data science team reduces the number of dimensions in a large data set to simplify modeling and reduce file size.
In contrast to supervised learning, its not always easy to come up with metrics for how well an unsupervised learning algorithm is doing. Performance is often subjective and domain-specific.
Clustering
An interesting example of clustering in the real world is marketing data provider Acxioms life stage clustering system, Personicx. This service segments U.S. households into 70 distinct clusters within 21 life stage groups that are used by advertisers when targeting Facebook ads, display ads, direct mail campaigns, etc.

A selection of Personicx demographic clusters
Their white paper reveals that they used centroid clustering and principal component analysis, both of which are techniques covered in this section.
You can imagine how having access to these clusters is extremely useful for advertisers who want to (1) understand their existing customer base and (2) use their ad spend effectively by targeting potential new customers with relevant demographics, interests, and lifestyles.

You can actually find out which cluster you personally would belong to by answering a few simple questions in Acxioms Whats My Cluster? tool.
Lets walk through a couple of clustering methods to develop intuition for how this task can be performed.
k-means clustering
And k rings were given to the race of Centroids, who above all else, desire power.
The goal of clustering is to create groups of data points such that points in different clusters are dissimilar while points within a cluster are similar.
With k-means clustering, we want to cluster our data points into k groups. A larger k creates smaller groups with more granularity, a lower k means larger groups and less granularity.
The output of the algorithm would be a set of labels assigning each data point to one of the k groups. In k-means clustering, the way these groups are defined is by creating a centroid for each group. The centroids are like the heart of the cluster, they capture the points closest to them and add them to the cluster.
Think of these as the people who show up at a party and soon become the centers of attention because theyre so magnetic. If theres just one of them, everyone will gather around; if there are lots, many smaller centers of activity will form.
Here are the steps to k-means clustering:
1. Define the k centroids. Initialize these at random (there are also fancier algorithms for initializing the centroids that end up converging more effectively).
2. Find the closest centroid & update cluster assignments. Assign each data point to one of the k clusters. Each data point is assigned to the nearest centroids cluster. Here, the measure of nearness is a hyperparameter  often Euclidean distance.
3. Move the centroids to the center of their clusters. The new position of each centroid is calculated as the average position of all the points in its cluster.
Keep repeating steps 2 and 3 until the centroid stop moving a lot at each iteration (i.e., until the algorithm converges).
That, in short, is how k-means clustering works! Check out this visualizationof the algorithm??read it like a comic book. Each point in the plane is colored according the centroid that it is closest to at each moment. Youll notice that the centroids (the larger blue, red, and green circles) start randomly and then quickly adjust to capture their respective clusters.

Another real-life application of k-means clustering is classifying handwritten digits. Suppose we have images of the digits as a long vector of pixel brightnesses. Lets say the images are black and white and are 64x64 pixels. Each pixel represents a dimension. So the world these images live in has 64x64=4,096 dimensions. In this 4,096-dimensional world, k-means clustering allows us to group the images that are close together and assume they represent the same digit, which can achieve pretty good results for digit recognition.
Hierarchical clustering
Lets make a million options become seven options. Or five. Or twenty? Meh, we can decide later.
Hierarchical clustering is similar to regular clustering, except that youre aiming to build a hierarchy of clusters. This can be useful when you want flexibility in how many clusters you ultimately want. For example, imagine grouping items on an online marketplace like Etsy or Amazon. On the homepage youd want a few broad categories of items for simple navigation, but as you go into more specific shopping categories youd want increasing levels of granularity, i.e. more distinct clusters of items.
In terms of outputs from the algorithm, in addition to cluster assignments you also build a nice tree that tells you about the hierarchies between the clusters. You can then pick the number of clusters you want from this tree.
Here are the steps for hierarchical clustering:
1. Start with N clusters, one for each data point.
2. Merge the two clusters that are closest to each other. Now you have N-1 clusters.
3. Recompute the distances between the clusters. There are several ways to do this (see this tutorial for more details). One of them (called average-linkage clustering) is to consider the distance between two clusters to be the average distance between all their respective members.
4. Repeat steps 2 and 3 until you get one cluster of N data points. You get a tree (also known as a dendrogram) like the one below.
5. Pick a number of clusters and draw a horizontal line in the dendrogram. For example, if you want k=2 clusters, you should draw a horizontal line around distance=20000. Youll get one cluster with data points 8, 9, 11, 16 and one cluster with the rest of the data points. In general, the number of clusters you get is the number of intersection points of your horizontal line with the vertical lines in the dendrogram.

Source: Solver.com. For more detail on hierarchical clustering, you can check this video out.
Dimensionality reduction
It is not the daily increase, but the daily decrease. Hack away at the unessential.??Bruce Lee
Dimensionality reduction looks a lot like compression. This is about trying to reduce the complexity of the data while keeping as much of the relevant structure as possible. If you take a simple 128 x 128 x 3 pixels image (length x width x RGB value), thats 49,152 dimensions of data. If youre able to reduce the dimensionality of the space in which these images live without destroying too much of the meaningful content in the images, then youve done a good job at dimensionality reduction.
Well take a look at two common techniques in practice: principal component analysis and singular value decomposition.
Principal component analysis (PCA)
First, a little linear algebra refresher??lets talk about spaces and bases.
Youre familiar with the coordinate plane with origin O(0,0) and basis vectors i(1,0) and j(0,1). It turns out you can choose a completely different basis and still have all the math work out. For example, you can keep O as the origin and choose the basis to vectors i=(2,1) and j=(1,2). If you have the patience for it, youll convince yourself that the point labeled (2,2) in the i, j coordinate system is labeled (6, 6) in the i, j system.

Plotted using Mathisfuns Interactive Cartesian Coordinates
This means we can change the basis of a space. Now imagine much higher-dimensional space. Like, 50K dimensions. You can select a basis for that space, and then select only the 200 most significant vectors of that basis. These basis vectors are called principal components, and the subset you select constitute a new space that is smaller in dimensionality than the original space but maintains as much of the complexity of the data as possible.
To select the most significant principal components, we look at how much of the datas variance they capture and order them by that metric.
Another way of thinking about this is that PCA remaps the space in which our data exists to make it more compressible. The transformed dimension is smaller than the original dimension.
By making use of the first several dimensions of the remapped space only, we can start gaining an understanding of the datasets organization. This is the promise of dimensionality reduction: reduce complexity (dimensionality in this case) while maintaining structure (variance). Heres a fun paper Samer wrote on using PCA (and diffusion mapping, another technique) to try to make sense of the Wikileaks cable release.
Singular value decomposition (SVD)
Lets represent our data like a big A = m x n matrix. SVD is a computation that allows us to decompose that big matrix into a product of 3 smaller matrices (U=m x r, diagonal matrix S=r x r, and V=r x n where r is a small number).
Heres a more visual illustration of that product to start with:

The values in the r*r diagonal matrix S are called singular values. Whats cool about them is that these singular values can be used to compress the original matrix. If you drop the smallest 20% of singular values and the associated columns in matrices U and V, you save quite a bit of space and still get a decent representation of the underlying matrix.
To examine what that means more precisely, lets work with this image of a dog:

Well use the code written in Andrew Gibianskys post on SVD. First, we show that if we rank the singular values (the values of the matrix S) by magnitude, the first 50 singular values contain 85% of the magnitude of the whole matrix S.

We can use this fact to discard the next 250 values of sigma (i.e., set them to 0) and just keep a rank 50 version of the image of the dog. Here, we create a rank 200, 100, 50, 30, 20, 10, and 3 dog. Obviously, the picture is smaller, but lets agree that the rank 30 dog is still good. Now lets see how much compression we achieve with this dog. The original image matrix is 305*275 = 83,875 values. The rank 30 dog is 305*30+30+30*275=17,430??almost 5 times fewer values with very little loss in image quality. The reason for the calculation above is that we also discard the parts of the matrix U and V that get multiplied by zeros when the operation USV is carried out (where S is the modified version of S that only has the first 30 values in it).

TECH:
Supervised and Unsupervised Machine Learning Algorithms

What is supervised machine learning and how does it relate to unsupervised machine learning?
In this post you will discover supervised learning, unsupervised learning and semis-supervised learning. After reading this post you will know:
	About the classification and regression supervised learning problems.
	About the clustering and association unsupervised learning problems.
	Example algorithms used for supervised and unsupervised problems.
	A problem that sits in between supervised and unsupervised learning called semi-supervised learning.
Lets get started.

Supervised and Unsupervised Machine Learning Algorithms
Photo by US Department of Education, some rights reserved.
Supervised Machine Learning
The majority of practical machine learning uses supervised learning.
Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output.
Y = f(X)
The goal is to approximate the mapping function so well that when you have new input data (x) that you can predict the output variables (Y) for that data.
It is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process. We know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.

.
 
 
Supervised learning problems can be further grouped into regression and classification problems.
	Classification: A classification problem is when the output variable is a category, such as red or blue or disease and no disease.
	Regression: A regression problem is when the output variable is a real value, such as dollars or weight.
Some common types of problems built on top of classification and regression include recommendation and time series prediction respectively.
Some popular examples of supervised machine learning algorithms are:
	Linear regression for regression problems.
	Random forest for classification and regression problems.
	Support vector machines for classification problems.
Unsupervised Machine Learning
Unsupervised learning is where you only have input data (X) and no corresponding output variables.
The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.
These are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data.
Unsupervised learning problems can be further grouped into clustering and association problems.
	Clustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.
	Association:  An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy X also tend to buy Y.
Some popular examples of unsupervised learning algorithms are:
	k-means for clustering problems.
	Apriori algorithm for association rule learning problems.
Semi-Supervised Machine Learning
Problems where you have a large amount of input data (X) and only some of the data is labeled (Y) are called semi-supervised learning problems.
These problems sit in between both supervised and unsupervised learning.
A good example is a photo archive where only some of the images are labeled, (e.g. dog, cat, person) and the majority are unlabeled.
Many real world machine learning problems fall into this area. This is because it can be expensive or time-consuming to label data as it may require access to domain experts. Whereas unlabeled data is cheap and easy to collect and store.
You can use unsupervised learning techniques to discover and learn the structure in the input variables.
You can also use supervised learning techniques to make best guess predictions for the unlabeled data, feed that data back into the supervised learning algorithm as training data and use the model to make predictions on new unseen data.
Summary
In this post you learned the difference between supervised, unsupervised and semi-supervised learning. You now know that:
	Supervised: All data is labeled and the algorithms learn to predict the output from the input data.
	Unsupervised: All data is unlabeled and the algorithms learn to inherent structure from the input data.
	Semi-supervised: Some data is labeled but most of it is unlabeled and a mixture of supervised and unsupervised techniques can be used.
TECH:
The Value of Machine Learning: Benefits and Best Practices
By Paramita Ghosh  /  June 8, 2017  /  0 Comments
TwitterFacebook18Google+LinkedIn
Machine Learning (ML) is a specialized sub-field of Artificial Intelligence (AI) where algorithms can learn and improve themselves by studying high volumes of available data. In this field, traditional programming rules do not operate; very high volumes of data alone can teach the algorithms to create better computing models.
Given the unique attributes of Machine Learning algorithms, they work best with Big Data because the volumes and complexity of such data are so high. In other words, Machine Learning aspires to mimic the human brain  learning by observing.
During the 1930s and 1940s, distinguished AI enthusiasts like Alan Turing began to explore the immense possibilities in neural networks, which finally paved the way for the phenomenal growth of Machine Learning as we know it today. You will get an insightful introduction to this fascinating field of Data Science in An Executives Guide to Machine Learning. This article warns that if ML algorithms are merely used as quick solutions for finite business problems, then this vastly untapped filed will remain as a bunch of business tools instead of moving forward into currently untapped areas.
With the growing popularity of Artificial Intelligence and AI-enabled smart solutions for businesses, ML is also gaining rapid prominence in the global business world. With the promise of an algorithm economy that is destined to take over the normal business environment in a few years, it is imperative that more AI or ML students, enthusiasts, and practitioners take the time to review the supplementary literature available around us to fully understand the scope of Machine Learning applications for the business world. This article takes the readers through a quick review of ML algorithms, use cases, and best practices.
The Business of Machine Learning
Today, the Data Scientists or Machine Learning experts stand between the truckloads of data and the generalist business users by extracting the meaning or intelligence out of the data with proven algorithms. The actual goal of AI practitioners is to bring ML to a point where average business users will not need the help of Data Scientists to use ML algorithms. In an algorithm economy, the pre-made algorithms will be smart enough to study and learn from data to improve the algorithmic models for more effective use later.  Some recent examples of such self-teaching ML models may be Facebooks face recognition tool or IBM Watsons Oncology tool for predicting cancer cases.
What Does this Mean for Future Businesses?
The Data Science community hopes that one day, ML solutions will aid the shop-floor managers to make quick and accurate decisions on the fly without the intervention of C-Level executives. The top line business leaders will only be called in for exceptional situations. ML has even proven to generate accurate solutions when perfect data did not exist. Now, given the growth of smart algorithms, Business Analytics will move beyond Descriptive and Predictive, and gradually launch Prescriptive Analytics as the sole preoccupation of business leaders.
Machine Learning Algorithms
The power of Machine Learning is hidden in the self-teaching algorithms, which when exposed to huge amounts of data, can study and learn for improved results. In Rules of Machine Learning, the technical experts can discover the modus operandi of ML, but for the most effective way to explore ML is to talk to experts on neural networks or Deep Learning, or unsupervised learning. ML algorithms have been categorized as of the following types of learning models: supervised, unsupervised, and reinforcement. A common ML solution known to everyone is Netflixs recommender tool that suggests new movies based on a viewers past movie selections. If you wish to find out how the three types of algorithms mentioned above differ from each other, then check out the 10 Algorithms that Every Machine Learning Engineer Should Know.
While exploring Machine Learning algorithms, if you ever face a situation where you cannot decide which algorithm is best suited to your problem, then refer to Microsofts Choice of Machine Learning Algorithms . Although the answer will depend on a combination of factors like the size and nature of available data, the intended result, and the available time.
Machine Learning Use Cases
This section discusses some common Machine Learning Use Cases. It also helps to skim over the article titled the Top 10 Machine Learning Algorithms, where the use cases mentioned here are explained in details.  This article takes each of these algorithms and describes the usage environment with case illustrations.  Some of the standard ML Use Cases include Apriori Algorithm, Logistic Regression, Random Forests, K Means Clustering Algorithm, Nave Bayes Classifier Algorithm, and many more. Also read the Forbes post titled What Are the Top 10 Use Cases for Machine Learning and AI to get a first-hand review of industry applications.
Another post from Forbes, Uses of AI and Machine Learning in Business digs deep into actual AI and ML market applications. The sample cases included here are Expedia that reigns the travel tourism world, by constantly learning from its BFS search tool based on ML; Auto Trader that offers useful apps to consumers; or Oacdo that proposes an alternative to barcode scanning.
Machine Learning Best Practices
The article titled Machine Learning Checklist offers the ML enthusiasts a handy checklist that they can use while working with ML projects. The author of this post claims that this checklist helps to structure the problem in a manner so that the ML project can reliably deliver a good solution. For reference purposes, you can review Machine Learning Quick Ref Best Practices
For all the ML enthusiasts and experts out there, the practical aspects of project execution through utilizing ML are really only just beginning to be understood. The benefits of studying various Machine Learning Use Cases and Best Practices are considerable and will remain important as more enterprises develop and implement Machine Learning throughout the varied Data Management systems.
 
TECH:
Supervised Learning vs Unsupervised Learning - Best 7 Useful Comparison

Introduction to Supervised Learning and Unsupervised Learning
Supervised learning and Unsupervised learning are machine learning tasks.
Supervised learning is simply a process of learning algorithm from the training dataset. Supervised learning is where you have input variables and an output variable and you use an algorithm to learn the mapping function from the input to the output. The aim is to approximate the mapping function so that when we have new input data we can predict the output variables for that data.
Unsupervised learning is modeling the underlying or hidden structure or distribution in the data in order to learn more about the data. Unsupervised learning is where you only have input data and no corresponding output variables.
Training dataset: A set of examples used for learning, where the target value is known.

TECH;
What are some real world examples of supervised and unsupervised learning?
Ad by ASOdesk

Multi-user access feature.


Answered Jul 3, 2018
#supervised learning example : predicting housing price from a given dataset of some or lot of given existing houses price,recommendation system
#unsupervised learning example : predicting or understanding handwritten digits,clustering websites based on particular words count on each webpage to understand what those web sites are actually talking about etc etc

TECH:
3 Examples of Unsupervised Learning
        posted by John Spacey, May 03, 2017


Unsupervised learning is an approach to machine learning whereby software learns from data without being given correct answers. It is an important type of artificial intelligence as it allows an AI to self-improve based on large, diverse data sets such as real world experience. The following are illustrative examples.
Visual Recognition
An unsupervised learner processes 10 million videos together with related textual data such as descriptions and comments. The learner models images in the videos using statistical analysis that allows it to identify visual patterns. These patterns can then be correlated with text to develop theories about the visual traits of various things. For example, such a learner might be able to build a solid model that can identify skateboards in videos. The learner is never given the right answer but can gain confidence based on a large number of samples. Likewise, the learner will discard a large number of models that don't appear to be correct.
Human Behavior
A learner that possesses visual highly developed visual and speech recognition capabilities could watch a large number of television shows to learn about human behavior. For example, a learner might be able to build a model that detects when people are smiling based on correlation of facial patterns and words such as "what are you smiling about?"
Robotics
A highly developed AI that serves as a housekeeping robot develops a theory that there is usually dust under a sofa. Each week, the theory is confirmed as the robot often finds dust under sofas. Nobody explicitly tells the robot the theory is correct but it is able to develop confidence in it nonetheles
	
	
Data scientists use many different kinds of machine learning algorithms to discover patterns in big data that lead to actionable insights. At a high level, these different algorithms can be classified into two groups based on the way they learn about data to make predictions: supervised and unsupervised learning.
Supervised machine learning is the more commonly used between the two. It includes such algorithms as linear and logistic regression, multi-class classification, and support vector machines. Supervised learning is so named because the data scientist acts as a guide to teach the algorithm what conclusions it should come up with. Its similar to the way a child might learn arithmetic from a teacher. Supervised learning requires that the algorithms possible outputs are already known and that the data used to train the algorithm is already labeled with correct answers. For example, a classification algorithm will learn to identify animals after being trained on a dataset of images that are properly labeled with the species of the animal and some identifying characteristics.
On the other hand, unsupervised machine learning is more closely aligned with what some call true artificial intelligence  the idea that a computer can learn to identify complex processes and patterns without a human to provide guidance along the way. Although unsupervised learning is prohibitively complex for some simpler enterprise use cases, it opens the doors to solving problems that humans normally would not tackle. Some examples of unsupervised machine learning algorithms include k-means clustering, principal and independent component analysis, and association rules.
While a supervised classification algorithm learns to ascribe inputted labels to images of animals, its unsupervised counterpart will look at inherent similarities between the images and separate them into groups accordingly, assigning its own new label to each group. In a practical example, this type of algorithm is useful for customer segmentation because it will return groups based on parameters that a human may not consider due to pre-existing biases about the companys demographic.
Choosing to use either a supervised or unsupervised machine learning algorithm typically depends on factors related to the structure and volume of your data and the use case of the issue at hand. A well-rounded data science program will use both types of algorithms to build predictive data models that help stakeholders make decisions across a variety of business challenges.
Want to learn more? There's more to this story  many commonly used machine learning algorithms actually fall into the category of semi-supervised learning, which has it's own post on our blog.


TECH:
Reinforcement Learning

	Introduction						

  Introduction
The general aim of Machine Learning is to produce intelligent programs, often called agents, through a process of learning and evolving. Reinforcement Learning (RL) is one approach that can be taken for this learning process. An RL agent learns by interacting with its environment and observing the results of these interactions. This mimics the fundamental way in which humans (and animals alike) learn. As humans, we have a direct sensori-motor connection to our environment, meaning we can perform actions and witness the results of these actions on the environment. The idea is commonly known as "cause and effect", and this undoubtedly is the key to building up knowledge of our environment throughout our lifetime.
The "cause and effect" idea can be translated into the following steps for an RL agent:
1.	The agent observes an input state
2.	An action is determined by a decision making function (policy)
3.	The action is performed
4.	The agent receives a scalar reward or reinforcement from the environment
5.	Information about the reward given for that state / action pair is recorded
By performing actions, and obersving the resulting reward, the policy used to determine the best action for a state can be fine-tuned. Eventually, if enough states are observed an optimal decision policy will be generated and we will have an agent that performs perfectly in that particular environment.

  Supervised and Unsupervised Learning
Supervised and Unsupervised learning are two quite different techniques of learning. As the names suggest, supervised learning involves learning with some supervision from an external source (i.e. a teacher) whereas unsupervised learning does not. An example of supervised learning is a student taking an exam, having it marked and then being shown which questions they answered incorrectly. After being shown the correct answers, the student should then learn to answer those questions successfully as well. An example of unsupervised learning is someone learning to juggle by themselves. The person will start by throwing the balls and attempting to catch them again. After dropping most of the balls initially, they will gradually adjust their technique and start to keep the balls in the air.
How does this relate to Reinforcement Learning? As described in the steps above, an RL agent learns by receiving a reward or reinforcement from its environment, without any form of supervision other than its own decision making policy. So, RL is a form of unsupervised learning. What this means is that an agent can learn by being set loose in its environment, without the need for specific training data to be generated and then used to teach the agent.

  Exploration and Exploitation
One of the interesting problems that arises when using Reinforcement Learning is the tradeoff between exploration and exploitation. If an agent has tried a certain action in the past and got a decent reward, then repeating this action is going to reproduce the reward. In doing so, the agent is exploiting what it knows to receive a reward. On the other hand, trying other possibilities may produce a better reward, so exploring is definitely a good tactic sometimes. Without a balance of both exploration and exploitation the RL agent will not learn successfully. The most common way to achieve a nice balance is to try a variety of actions while progessively favouring those that stand out as producing the most reward.

  Uses for Reinforcement Learning
A variety of different problems can be solved using Reinforcement Learning. Because RL agents can learn without expert supervision, the type of problems that are best suited to RL are complex problems where there appears to be no obvious or easily programmable solution. Two of the main ones are:
Game playing - determining the best move to make in a game often depends on a number of different factors, hence the number of possible states that can exist in a particular game is usually very large. To cover this many states using a standard rule based approach would mean specifying an also large number of hard coded rules. RL cuts out the need to manually specify rules, agents learn simply by playing the game. For two player games such as backgammon, agents can be trained by playing against other human players or even other RL agents.
Control problems - such as elevator scheduling. Again, it is not obvious what strategies would provide the best, most timely elevator service. For control problems such as this, RL agents can be left to learn in a simulated environment and eventually they will come up with good controlling policies. Some advantages of using RL for control problems is that an agent can be retrained easily to adapt to environment changes, and trained continuously while the system is online, improving performance all the time.
A detailed analysis of using RL for playing backgammon can be found here on the IBM research website. If card games appeal to you, here is a nice applet that learns to play blackjack.


TECH:

Simple Beginners guide to Reinforcement Learning & its implementation
FAIZAN SHAIKH, JANUARY 19, 2017

Introduction
One of the most fundamental question for scientists across the globe has been  How to learn a new skill?. The desire to understand the answer is obvious  if we can understand this, we can enable human species to do things we might not have thought before. Alternately, we can train machines to do more human tasks and create true artificial intelligence.
While we dont have a complete answer to the above question yet, there are a few things which are clear. Irrespective of the skill, we first learn by interacting with the environment. Whether we are learning to drive a car or whether it an infant learning to walk, the learning is based on the interaction with the environment. Learning from interaction is the foundational underlying concept for all theories of learning and intelligence.
 
Re-inforcement Learning
Today, we will explore Reinforcement Learning  a goal-oriented learning based on interaction with environment. Reinforcement Learning is said to be the hope of true artificial intelligence. And it is rightly said so, because the potential that ReinforcementLearning possesses is immense.
Reinforcement Learning is growing rapidly, producing wide variety of learning algorithms for different applications. Hence it is important to be familiar with the techniques of reinforcement learning. If you are not familiar with reinforcement learning, I will suggest you to go through my previous article on introduction to reinforcement learning and the open source RL platforms.
Once you have an understanding of underlying fundamentals, proceed with this article. By the end of this article you will have a thorough understanding of Reinforcement Learning and its practical implementation.
 
P.S. For implementation, we assume that you have basic knowledge of Python. If you dont know Python, you should first go through this tutorial
 
Table of Content
1.	Formulating a reinforcement learning problem
2.	Comparison with other machine learning methodologies
3.	Framework for solving Reinforcement learning problems
4.	An implementation of Reinforcement Learning
5.	Increasing the complexity
6.	Peek into recent RL advancements
7.	Additional Resources
 
1. Formulating a Reinforcement Learning Problem
Reinforcement Learning is learning what to do and how to map situations to actions. The end result is to maximize the numerical reward signal. The learner is not told which action to take, but instead must discover which action will yield the maximum reward. Lets understand this with a simple example below.
Consider an example of a child learning to walk.

Here are the steps a child will take while learning  to walk:
1.	The first thing the child will observe is to notice how you are walking. You use two legs, taking a step at a time in order to walk. Grasping this concept, the child tries to replicate you.
2.	But soon he/she will understand that before walking, the child has to stand up! This is a challenge that comes along while trying to walk. So now the child attempts to get up, staggering and slipping but still determinant to get up.
3.	Then theres another challenge to cope up with. Standing up was easy, but to remain still is another task altogether! Clutching thin air to find support, the child manages to stay standing.
4.	Now the real task for the child is to start walking. But its easy to say than actually do it. There are so many things to keep in mind, like balancing the body weight, deciding which foot to put next and where to put it.
Sounds like a difficult task right? It actually is a bit challenging to get up and start walking, but you have become so use to it that you are not fazed by the task. But now you can get the gist of how difficult it is for a child.
Lets formalize the above example, the problem statement of the example is to walk, where the child is an agent trying to manipulate the environment (which is the surface on which it walks) by taking actions (viz walking) and he/she tries to go from one state (viz each step he/she takes) to another. The child gets a reward (lets say chocolate) when he/she accomplishes a submodule of the task (viz taking couple of steps)and will not receive any chocolate (a.k.a negative reward) when he/she is not able to walk. This is a simplified description of a reinforcement learning problem
. Comparison with other machine learning methodologies
Reinforcement Learning belongs to a bigger class of machine learning algorithm. Below is the description of types of machine learning methodologies.

 
Lets see a comparison between RL and others:
	Supervised vs Reinforcement Learning: In supervised learning, theres an external supervisor, which has knowledge of the environment and who shares it with the agent to complete the task. But there are some problems in which there are so many combinations of subtasks that the agent can perform to achieve the objective. So that creating a supervisor is almost impractical. For example, in a chess game, there are tens of thousands of moves that can be played. So creating a knowledge base that can be played is a tedious task. In these problems, it is more feasible to learn from ones own experiences and gain knowledge from them. This is the main difference that can be said of reinforcement learning and supervised learning. In both supervised and reinforcement learning, there is a mapping between input and output. But in reinforcement learning, there is a reward function which acts as a feedback to the agent as opposed to supervised learning.
	Unsupervised vs Reinforcement Leanring: In reinforcement learning, theres a mapping from input to output which is not present in unsupervised learning. In unsupervised learning, the main task is to find the underlying patterns rather than the mapping. For example, if the task is to suggest a news article to a user, an unsupervised learning algorithm will look at similar articles which the person has previously read and suggest anyone from them. Whereas a reinforcement learning algorithm will get constant feedback from the user by suggesting few news articles and then build a knowledge graph of which articles will the person like.
There is also a fourth type of machine learning methodology called semi-supervised learning, which is essentially a combination of supervised and unsupervised learning. It differs from reinforcement learning as similar to supervised and semi-supervised learning has direct mapping whereas reinforcement does not.
 
3. Framework for solving Reinforcement Learning Problems
To understand how to solve a reinforcement learning problem, lets go through a classic example of reinforcement learning problem  Multi-Armed Bandit Problem. First, we would understand the fundamental problem of exploration vs exploitation and then go on to define the framework to solve RL problems.
 


Suppose you have many slot machines with random payouts. A slot machine would look something like this.
Now you want to do is get the maximum bonus from the slot machines as fast as possible. What would you do?
One naive approach might be to select only one slot machine and keep pulling the lever all day long. Sounds boring, but it may give you some payouts. With this approach, you might hit the jackpot (with a probability close to 0.00000.1) but most of the time you may just be sitting in front of the slot machine losing money. Formally, this can be defined as a pure exploitation approach. Is this the optimal choice? The answer is NO.
Lets look at another approach. We could pull a lever of each & every slot machine and pray to God that at least one of them would hit the jackpot. This is another naive approach which would keep you pulling levers all day long, but give you sub-optimal payouts. Formally this approach is a pure exploration approach.
Both of these approaches are not optimal, and we have to find a proper balance between them to get maximum reward. This is said to be exploration vs exploitation dilemma of reinforcement learning.
First, we formally define the framework for reinforcement learning problem and then list down the probable approaches to solve the problem.
Markov Decision Process:
The mathematical framework for defining a solution in reinforcement learning scenario is called Markov Decision Process. This can be designed as:
	Set of states, S
	Set of actions, A
	Reward function, R
	Policy, p
	Value, V
We have to take an action (A) to transition from our start state to our end state (S). In return getting rewards (R) for each action we take. Our actions can lead to a positive reward or negative reward.
The set of actions we took define our policy (p) and the rewards we get in return defines our value (V). Our task here is to maximize our rewards by choosing the correct policy. So we have to maximize for all possible values of S for a time t.
 
Shortest Path Problem
Let me take you through another example to make it clear.

This is a representation of a shortest path problem. The task is to go from place A to place F, with as low cost as possible. The numbers at each edge between two places represent the cost taken to traverse the distance. The negative cost are actually some earnings on the way. We define Value is the total cumulative reward when you do a policy.
Here,
	The set of states are the nodes, viz {A, B, C, D, E, F}
	The action to take is to go from one place to other, viz {A -> B, C -> D, etc}
	The reward function is the value represented by edge, i.e. cost
	The policy is the way to complete the task, viz {A -> C -> F}
Now suppose you are at place A, the only visible path is your next destination and anything beyond that is not known at this stage (a.k.a observable space).
You can take a greedy approach and take the best possible next step, which is going from {A -> D} from a subset of {A -> (B, C, D, E)}. Similarly now you are at place D and want to go to place F, you can choose from {D -> (B, C, F)}. We see that {D -> F} has the lowest cost and hence we take that path.
So here, our policy was to take {A -> D -> F} and our Value is -120.
Congratulations! You have just implemented a reinforcement learning algorithm. This algorithm is known as epsilon greedy, which is literally a greedy approach to solving the problem. Now if you (the salesman) want to go from place A to place F again, you would always choose the same policy.
Other ways of travelling?
Can you guess which category does our policy belong to  i.e. (pure exploration vs pure exploitation)?
Notice that the policy we took is not an optimal policy. We would have to explore a little bit to find the optimal policy. The approach which we took here is policy based learning, and our task is to find the optimal policy among all the possible policies. There are different ways to solve this problem, Ill briefly list down the major categories
	Policy based, where our focus is to find optimal policy
	Value based, where our focus is to find optimal value, i.e. cumulative reward
	Action based, where our focus is on what optimal actions to take at each step
I would try to cover in-depth reinforcement learning algorithms in future articles. Till then, you can refer to this paper on a survey of reinforcement learning algorithms.
 
4. An implementation of Reinforcement Learning
We will be using Deep Q-learning algorithm. Q-learning is a policy based learning algorithm with the function approximator as a neural network. This algorithm was used by Google to beat humans at Atari games!
Lets see a pseudocode of  Q-learning:
1.	Initialize the Values table Q(s, a).
2.	Observe the current state s.
3.	Choose an action a for that state based on one of the action selection policies (eg. epsilon greedy)
4.	Take the action, and observe the reward  r as well as the new state  s.
5.	Update the Value for the state using the observed reward and the maximum reward possible for the next state. The updating is done according to the formula and parameters described above.
6.	Set the state to the new state, and repeat the process until a terminal state is reached.
 
A simple description of Q-learning can be summarized as follows:


TECH
An introduction to Reinforcement Learning

Some of the environments youll work with
This article is part of Deep Reinforcement Learning Course with Tensorflow ???. Check the syllabus here
Were making a video version of the Deep Reinforcement Learning Course with Tensorflow ?? where we focus on the implementation part with Tensorflow, you can see the the playlist here
Reinforcement learning is an important type of Machine Learning where an agent learn how to behave in a environment by performing actions and seeing the results.
In recent years, weve seen a lot of improvements in this fascinating area of research. Examples include DeepMind and the Deep Q learning architecturein 2014, beating the champion of the game of Go with AlphaGo in 2016, OpenAI and the PPO in 2017, amongst others.
DeepMind DQN
In this series of articles, we will focus on learning the different architectures used today to solve Reinforcement Learning problems. These will include Q -learning, Deep Q-learning, Policy Gradients, Actor Critic, and PPO.
In this first article, youll learn:
	What Reinforcement Learning is, and how rewards are the central idea
	The three approaches of Reinforcement Learning
	What the Deep in Deep Reinforcement Learning means
Its really important to master these elements before diving into implementing Deep Reinforcement Learning agents.
The idea behind Reinforcement Learning is that an agent will learn from the environment by interacting with it and receiving rewards for performing actions.

Learning from interaction with the environment comes from our natural experiences. Imagine youre a child in a living room. You see a fireplace, and you approach it.

Its warm, its positive, you feel good (Positive Reward +1). You understand that fire is a positive thing.

But then you try to touch the fire. Ouch! It burns your hand (Negative reward -1). Youve just understood that fire is positive when you are a sufficient distance away, because it produces warmth. But get too close to it and you will be burned.
Thats how humans learn, through interaction. Reinforcement Learning is just a computational approach of learning from action.
The Reinforcement Learning Process

Lets imagine an agent learning to play Super Mario Bros as a working example. The Reinforcement Learning (RL) process can be modeled as a loop that works like this:
	Our Agent receives state S0 from the Environment (In our case we receive the first frame of our game (state) from Super Mario Bros (environment))
	Based on that state S0, agent takes an action A0 (our agent will move right)
	Environment transitions to a new state S1 (new frame)
	Environment gives some reward R1 to the agent (not dead: +1)
This RL loop outputs a sequence of state, action and reward.
The goal of the agent is to maximize the expected cumulative reward.
The central idea of the Reward Hypothesis
Why is the goal of the agent to maximize the expected cumulative reward?
Well, Reinforcement Learning is based on the idea of the reward hypothesis. All goals can be described by the maximization of the expected cumulative reward.
Thats why in Reinforcement Learning, to have the best behavior, we need to maximize the expected cumulative reward.
The cumulative reward at each time step t can be written as:

Which is equivalent to:

Thanks to Pierre-Luc Bacon for the correction
However, in reality, we cant just add the rewards like that. The rewards that come sooner (in the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.

Let say your agent is this small mouse and your opponent is the cat. Your goal is to eat the maximum amount of cheese before being eaten by the cat.
As we can see in the diagram, its more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).
As a consequence, the reward near the cat, even if it is bigger (more cheese), will be discounted. Were not really sure well be able to eat it.
To discount the rewards, we proceed like this:
We define a discount rate called gamma. It must be between 0 and 1.
	The larger the gamma, the smaller the discount. This means the learning agent cares more about the long term reward.
	On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).
Our discounted cumulative expected rewards is:

Thanks to Pierre-Luc Bacon for the correction
To be simple, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less probable to happen.
Episodic or Continuing tasks
A task is an instance of a Reinforcement Learning problem. We can have two types of tasks: episodic and continuous.
Episodic task
In this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and New States.
For instance think about Super Mario Bros, an episode begin at the launch of a new Mario and ending: when youre killed or youre reach the end of the level.

Beginning of a new episode
Continuous tasks
These are tasks that continue forever (no terminal state). In this case, the agent has to learn how to choose the best actions and simultaneously interacts with the environment.
For instance, an agent that do automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop him.

Monte Carlo vs TD Learning methods
We have two ways of learning:
	Collecting the rewards at the end of the episode and then calculating the maximum expected future reward: Monte Carlo Approach
	Estimate the rewards at each step: Temporal Difference Learning
Monte Carlo
When the episode ends (the agent reaches a terminal state), the agent looks at the total cumulative reward to see how well it did. In Monte Carlo approach, rewards are only received at the end of the game.
Then, we start a new game with the added knowledge. The agent makes better decisions with each iteration.

Lets take an example:

If we take the maze environment:
	We always start at the same starting point.
	We terminate the episode if the cat eats us or if we move > 20 steps.
	At the end of the episode, we have a list of State, Actions, Rewards, and New States.
	The agent will sum the total rewards Gt (to see how well it did).
	It will then update V(st) based on the formula above.
	Then start a new game with this new knowledge.
By running more and more episodes, the agent will learn to play better and better.
Temporal Difference Learning : learning at each time step
TD Learning, on the other hand, will not wait until the end of the episode to update the maximum expected future reward estimation: it will update its value estimation V for the non-terminal states St occurring at that experience.
This method is called TD(0) or one step TD (update the value function after any individual step).

TD methods only wait until the next time step to update the value estimates. At time t+1 they immediately form a TD target using the observed reward Rt+1 and the current estimate V(St+1).
TD target is an estimation: in fact you update the previous estimate V(St) by updating it towards a one-step target.
Exploration/Exploitation trade off
Before looking at the different strategies to solve Reinforcement Learning problems, we must cover one more very important topic: the exploration/exploitation trade-off.
	Exploration is finding more information about the environment.
	Exploitation is exploiting known information to maximize the reward.
Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, we can fall into a common trap.

In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze there is a gigantic sum of cheese (+1000).
However, if we only focus on reward, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation).
But if our agent does a little bit of exploration, it can find the big reward.
This is what we call the exploration/exploitation trade off. We must define a rule that helps to handle this trade-off. Well see in future articles different ways to handle it.
Three approaches to Reinforcement Learning
Now that we defined the main elements of Reinforcement Learning, lets move on to the three approaches to solve a Reinforcement Learning problem. These are value-based, policy-based, and model-based.
Value Based
In value-based RL, the goal is to optimize the value function V(s).
The value function is a function that tells us the maximum expected future reward the agent will get at each state.
The value of each state is the total amount of the reward an agent can expect to accumulate over the future, starting at that state.

The agent will use this value function to select which state to choose at each step. The agent takes the state with the biggest value.

In the maze example, at each step we will take the biggest value: -7, then -6, then -5 (and so on) to attain the goal.
Policy Based
In policy-based RL, we want to directly optimize the policy function p(s)without using a value function.
The policy is what defines the agent behavior at a given time.

action = policy(state)
We learn a policy function. This lets us map each state to the best corresponding action.
We have two types of policy:
	Deterministic: a policy at a given state will always return the same action.
	Stochastic: output a distribution probability over actions.


As we can see here, the policy directly indicates the best action to take for each steps.
Model Based
In model-based RL, we model the environment. This means we create a model of the behavior of the environment.
The problem is each environment will need a different model representation. Thats why we will not speak about this type of Reinforcement Learning in the upcoming articles.
Introducing Deep Reinforcement Learning
Deep Reinforcement Learning introduces deep neural networks to solve Reinforcement Learning problems??hence the name deep.
For instance, in the next article well work on Q-Learning (classic Reinforcement Learning) and Deep Q-Learning.
Youll see the difference is that in the first approach, we use a traditional algorithm to create a Q table that helps us find what action to take for each state.
In the second approach, we will use a Neural Network (to approximate the reward based on state: q value).

Schema inspired by the Q learning notebook by Udacity
Congrats! There was a lot of information in this article. Be sure to really grasp the material before continuing. Its important to master these elements before entering the fun part: creating AI that plays video games.
Important: this article is the first part of a free series of blog posts about Deep Reinforcement Learning. For more information and more resources, check out the syllabus.
Next time well work on a Q-learning agent that learns to play the Frozen Lake game.

FrozenLake
If you liked my article, please click the ?? below as many time as you liked the article so other people will see this here on Medium. And dont forget to follow me!
If you have any thoughts, comments, questions, feel free to comment below or send me an email: hello@simoninithomas.com, or tweet me @ThomasSimonini.



Cheers!
________________________________________
TECH:
Cool Companies in Cognitive Computing for 2018
Aug 8, 2018
Joyce Wells
Page 1 of 4 next >>


			inShare

 
Experts may disagree on the precise definitions of artificial intelligence (AI), cognitive computing, machine learning (ML), or natural language processing. However, there is no debate about whether the proliferation of sensors and mobile devices, the rapid increase in data volume, and the heightened need for rapid decision making is fueling a demand for smarter solutions and greater automation.
According to a 2018 McKinsey Global Institute report, the current projected global impact of adopting what it calls AI is $3.5 trillion to $5.8 trillion annually globally. The McKinsey Institute has identified 400 use cases for AI across 19 industries.
Furthermore, Deloitte Global predicts that by the end of 2018, the number of implementations and pilot projects using machine learning will double compared with 2017, and the number will have doubled again by 2020. In addition, with enabling technologies such as ML application program interfaces (APIs) and specialized hardware available in the cloud, these advances will be generally available to small as well as large companies.
Worldwide spending on cognitive and artificial intelligence systems is forecast to reach $57.6 billion in 2021, according to a recent update of the Worldwide Semiannual Cognitive Artificial Intelligence Systems Spending Guide from IDC. With many industries aggressively investing in cognitive and AI solutions, spending is expected to achieve a compound annual growth rate (CAGR) of 50.1% over the 2016-2021 forecast period, the company says.
And, while the focus on machine learning (ML) may seem like an overnight development, according to a recent survey conducted by MIT Technology Review Custom and Google Cloud, the use of this technology has been growing steadily since the emergence of big data. ML is beginning to deliver on the potential created by big data and analytics by turning raw data into useful, predictive tools for business. Innovation-minded business leaders are embracing ML as the next big thing and have already crafted ML strategies and initiatives that promise real benefits and return on investment (ROI).
To help increase understanding about this important area of information technology and how it is being leveraged in solutions and platforms to provide business advantage, DBTA and Big Data Quarterly present the 2018 list of Cool Companies in Cognitive Computing starting below and continuing through the following three web pages.
ABBYY  A global leader of content intelligent solutions and services, ABBYY offers a complete range of AI-based technologies and solutions to help transform business documents and content into business value.
Accenture  Driving innovation to improve the way the world works and lives, Accenture provides a broad range of services and solutions in strategy, consulting, digital, technology and operations, and works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders.
Amazon AI services  With ML algorithms used in many of Amazons own internal systems and core to the capabilities its customers experienceincluding path optimization in fulfillment centers, Amazon.coms recommendations engine, and the retail experience Amazon Gothe companys goal with Amazon AI services is to  share its learnings and ML capabilities as fully managed services.
Attivio  A cognitive search and insight platform company that enables Fortune 500 enterprises to answer the most complex questions, Attivio puts search at the core of every enterprise, integrating every data source into a single always-learning platform.
BlueData  With BlueDatas container-based software platform, enterprises can quickly deploy multi-node environments for AI use cases with TensorFlow and other ML tools. BlueData makes it easier, faster, and more cost-effective to innovate with AI, ML, and big data analyticseither on-premises, in the cloud, or in a hybrid architecture.
BMC  A provider of IT solutions for the digital enterprise, BMC recently introduced the Helix Cognitive Service Management (CSM) offering, integrating cognitive technologies such as  AI and ML into traditional IT service management,  and enabling end-to-end CSM built for containerized microservices-based architectures multi-clouds.
C3 IoT  While delivering a comprehensive platform as a service (PaaS) for rapidly developing and operating big data, predictive analytics, AI/ML, and IoT software as a service (SaaS) applications, C3 IoT also offers a family of configurable and extensible SaaS products developed with and operating on its PaaS.


TECH:
Cognitive Computing Market
Cognitive computing can be defined as the simulation of human brain activity, to achieve highly complex and fast computing like a human brain. Cognitive computing systems are continuously learning systems. These systems learn using different processes such as pattern recognition, data mining and language processing. Cognitive computing is seeing massive breakthroughs as companies are now involved in developing Artificial Intelligence (AI) and are competing to gain the edge. Cognitive computing can help realize the perfect AI as it can help us mimicking human brain activity. These systems can be helpful in developing automated systems that can solve complex problems on their own.
Cognitive computers use many complex machine learning algorithms to continuously evolve by learning through the data provided. They can mine information from internet and as search for new patterns that can help them learn easily. Artificial Intelligence is one the primary use of cognitive computing, these systems can also be employed in neural network systems, robotics, natural learning and other technologies. IBM`s Watson is one the leading technology in this segment. Google Deepmind and other cognitive systems by Microsoft and Facebook are rapidly innovations to get the edge in this market. The systems could revolutionize the current generation of computing. They open door for multiple markets such as health care and space exploration to achieve higher data processing and power.
The study, besides estimating the cognitive computing market potential between till 2025, analyzes on who can be the market leaders and what partnerships would help them to capture the market share. The report gives an overview about the dynamics of the market, by discussing various aspects such as drivers, restraints, Porters 5 forces, value chain, customer acceptance and investment scenario.  
The study in brief deals with the product life cycle, comparing it to the relevant products from across industries that had already been commercialized. It approximates the time for innovation, in order for the industry to maintain a stable growth over a sustained period. The report also details the potential for various applications, discussing about recent product innovations and gives an overview on potential regional market shares.


TECH;
Custom Cognitive Computing Aims To Turn Every Developer Into A Data Scientist

Janakiram MSVContributori
I cover Cloud Computing, Machine Learning, and Internet of Things

The hype and buzz around artificial intelligence and machine learning are intimidating the developer community. Machine learning and deep learning are different from other technology waves of the recent past. The combination of mathematics, statistics, data, algorithms and programming is overwhelming to even those self-taught developers who take pride in embracing any new technology.

Computer VisionSOURCE: GRAPHICSTOCK
But, the traditional way of approaching machine learning and artificial intelligence is set to change. Developers will be able to infuse artificial intelligence into applications without having to deal with the complexity involved in implementing machine learning models. They will be able to train sophisticated models without transforming into a data scientist.
Google recently announced Cloud AutoML, a service that lets developers train machine learning algorithms with custom data without having to choose the algorithms and building a model. Google is not the first to bring this to the public cloud. Last year Microsoft announced CustomVision.ai, a computer vision API that delivers the same promise as Google Cloud AutoML service. Clarifai, a New York-based AI startup offers a similar service to its customers.
So, whats the big deal about custom cognitive computing?
Today, AI can be consumed in two forms  1) cognitive APIs, and 2) custom ML models. Cognitive APIs offer computer vision, speech, translation, text analytics as cloud-hosted APIs. IBM Watson, Google Cloud ML APIs, Microsoft Cognitive Services and Amazon AI APIs are examples of such hosted APIs. Any developer with the basic understanding of consuming REST API can invoke these services to add intelligence to her application. But these services come with a limitation  they are trained on generic datasets. That means while computer vision API can identify a car as an object, it cannot detect its make and model. If the developer is building an application that has to precisely determine the car make and model, these APIs won't work.

To build such a niche, precise machine learning model, the developer has to create a massive dataset of cars, label each of them with the make and model details and train a highly complex convolutional neural network. This also involves steps like pre-processing the dataset, cleaning the dataset and normalizing it before training the neural network. Depending on the size of the dataset, she will have to use GPU-based machines to accelerate the training process. Once the model is thoroughly trained and evaluated, it can be ported to mobile devices or edge computing appliances for inference.
Training such a complex machine learning model demands advanced data science skills along with expensive infrastructure. These are the key barriers to implementing machine learning and artificial intelligence.
Custom cognitive computing takes a middle path by bringing the ease of cognitive computing to training custom models. With this API, developers can upload labeled data to the cloud and iteratively train the model to achieve accuracy. Once the model is ready, it is exposed as REST API, which can be consumed like any other cognitive service. The drawback with this approach is that the trained model cannot be exported to different environments. Scenarios that need low-latency and offline inference cannot rely on custom cognitive APIs.
Microsoft and Google are competing to win the mindshare of ML developers. Apart from custom computer vision APIs, both the platforms are expected to launch additional cognitive services including custom video, text, translation, and speech services.
Public cloud vendors are moving aggressively to democratize machine learning, which is making the technology accessible to developers. This is also resulting in reducing the learning curve involved in acquiring AI and ML skills.

TECH:
A.I., Machine Learning, Cognitive Computing As Professions Top Technology Hard Trends For 2018  Part 1
ARTICLE MAY 3, 2018



 | SHARE  | SHARE  | SHARE  | SHARE
By Daniel Burrus, Ceo Of Burrus Research, And Tom Hood, CEO Of The Maryland Association Of CPAs And The Business Learning Institute.
Using futurist Daniel Burrus predictions as a baseline, MACPA surveys narrow technologys impact on the profession top trends.
Artificial intelligence, machine learning, and cognitive computing in audit and tax are the top trends that will impact the accounting and finance world over the next three years, according to research conducted by the Maryland Association of CPAs, the Business Learning Institute, and world-renowned futurist Daniel Burrus.
There has never been a shortage of trends, said Burrus. The real problem is figuring out which ones will happen so that you know where to place your strategic bets. I have been publishing a list of top trends since 1983, as well as speaking and writing about their future impact, and if you have read any of my seven books or thousands of articles over the decades, you know they have been highly accurate. The reason for this is the methodology I have developed that separates what I call Hard Trends  the trends that will happen  from Soft Trends, which might happen. Knowing their distinctions can make all the difference.
I have been writing about each one of these technology trends for many years, but for one to make it on my Top 20 list, it has to be developed enough that you can apply it to exponentially grow your business, Burrus added. Each is growing at an increasingly exponential rate. As such, they all will impact our lives, both personally and professionally, in the coming year and beyond. They highlight enormous, game- changing opportunities in a broad array of applications and industries.
As you read through them, look for opportunities for you to leverage them and become a positive disruptor.
One more word of advice, Tom Hood, CEO Of The Maryland Association Of CPAs And The Business Learning Institute, said. Dont stop at the trends impacting your firm or company. Stop and think about how these trends are impacting your clients and customers, both internally and externally. Think about the hard trends facing your industry and your customers industries. Read the entire list and think about robotics and 3D printing, augmented and virtual reality, the Internet of Things, location-based services, drones, and wearables.
These seemingly individual technologies are joining forces to disrupt the accounting and finance profession exponentially. The impact of artificial intelligence is huge, but when it is combined with blockchain, for instance, its impact increases tenfold ... at least. This 10x mindset will be vital going forward, Hood said.
Using Burrus annual list as a starting point, Hood asked more than 1,000 CPAs and finance and accounting professionals which of those trends will have the greatest impact on the profession over the next three years. The first 10 trends are in rank order from our surveys and research and are listed here in order:
1. Artificial intelligence (A.I.), advanced machine learning and cognitive computing applications
Cognitive computing applications grow rapidly. Advances in machine learning and artificial intelligence, such as Googles DeepMind and IBMs Watson, coupled with networked intelligent machines and sensors will create a giant leap forward, thanks to exponential advances in computing power, digital storage, and bandwidth. A.I. increasingly will become embedded in our applications and processes. Also, thanks to better sensors, increasing machine intelligence, and Siri-like voice communications, advanced automation and intelligent robotics increasingly will work with humans in new and productive ways. As A.I. is applied to vehicle-to-vehicle communications, we will see acceleration in the use of semi- autonomous and fully autonomous vehicles.


2.  Big Data and the use of high- speed data analytics
Big Data is a term that describes the technologies and techniques used to capture and utilize exponentially increasing streams of data. The goal is to bring enterprise-wide visibility and insights that enable making rapid, critical decisions. Using advanced cloud services, high-speed data analytics increasingly will be employed as a complement to existing information management systems and programs to identify actionable insights from a mass of Big Data.
Separating good data from bad data also will become a rapidly growing service.
3.  Adaptive and predictive cybersecurity systems
Business, government, and education have moved cybersecurity from an underfunded back-office activity to
a major initiative going forward. With the rapid growth of connected technologies such as the Internet of Things (IoT) and semi-autonomous as well as fully autonomous vehicles, security systems will move beyond reacting faster to include adaptive security systems using A.I. and other advanced tools, such as behavioral analytics. This will add a level of predict and prevent, allowing us to stop many, but sadly not all, attacks before they start.
4. Virtualization of processes and services (on-demand services)
The virtualization of processes and services will increasingly be accessed by companies needing to update and streamline existing services, and to deploy new services rapidly. The rapid growth of Collaboration-as-a-Service, Security-as-a-Service, Networking-
as-a-Service, and many more is giving birth to Everything-as-a-Service.
5. Mobile apps for business process innovation

As we increasingly transform business processes using mobility, the use of mobile apps for purchasing, supply chain, logistics, distribution, service, sales, and maintenance will grow rapidly. There will be an increasing focus on business app stores within companies, giving the company a competitive advantage and giving users access to the personalized information they need on their mobile devices anytime and anywhere.
6.  Blockchains and cryptocurrency
Introduced as a means of transferring bitcoins, blockchains are fast gaining traction in any number of areas. A system that enables secure, digital, direct transfers, blockchains decentralize transactions by eliminating the middleman, thereby allowing for direct connection among all involved parties. In addition to currency, blockchains can be used to transfer contracts, insurance policies, real estate titles, bonds, votes, and other items of value. They provide increased transparency and, as a result, distributed trust. Given their security and lower cost, blockchains create a platform that will impact limitless products and services, thereby enabling innovation and growth. Look for applications in health care, supply chain, and finance to grow rapidly. In 2017, the average person discovered bitcoin thanks to its meteoric rise in value, as well as other coins such as ethereum (used for initial coin offerings) and litecoin, to name a few. The crypto genie is now out of the bottle, and thanks to bitcoin trading, bitcoin ATMs, and bitcoin mania, we will see blockchains and cryptocurrency increasingly become part of our lives.
7.  Advanced cloud computing services
Businesses of all sizes will increasingly embrace new variations on public, private, hybrid, and personal mobile clouds. This represents a major shift in how organizations obtain and maintain software, hardware, and computing capacity to cut costs in IT, human resources, and sales management. Not all clouds are created equal. Some are optimized for IoT applications, while others are designed for different levels of security and speed.
8.  Smarter smartphones and tablets drive mobile process innovation
The vast majority of mobile phones sold globally have browsers, making a smartphone our primary computer. This signals a profound shift in global computing, allowing businesses of all sizes to transform the ways in which they market, sell, communicate, collaborate, educate, train, and innovate using mobility. An enterprise mobility strategy that puts mobile first is rapidly becoming mandatory for organizations of all sizes. The next phase is to embed a layer of A.I. in everything.


9.  Virtualization of storage, desktops, applications, and networking
The virtualization of hardware and software will see continued acceptance through growth in both large and small businesses as virtualization security improves. Hardware-as-a-Service (HaaS) is increasingly joining Software-as-a-Service (SaaS), creating what some have called IT as a Service. In addition to the rapid growth of virtual storage, virtualization of processing power will continue to grow, allowing mobile devices to access supercomputer capabilities and apply them to processes such as purchasing and logistics. These services will help companies cut costs, as they provide access to powerful software programs and the latest technology without the expense of a large IT staff and time- consuming, expensive upgrades.
 
1.	Social business applications
Social takes on a new level of urgency as organizations shift from an Information Age informing model to a Communication Age communicating and engagement model. Social software for business
will reach a new level of adoption, with applications to enhance relationships, collaboration, networking, social validation, and more. Augmented reality and virtual reality will increasi


TECH:
How the Enterprise Can Leverage Cognitive Computing
   
Arthur Cole | May 16, 2018
 
Source: nicescene/iStockphoto
Takeaway: Cognitive computing is the next evolutionary step for today's artificial intelligence and machine learning technologies. But what new capabilities will it bring to the enterprise?
The enterprise is just getting its feet wet when it comes to artificial intelligence and machine learning, but the real prize behind these technologies is the advent of fully cognitive computing.
But what does cognitive mean, and how will we know when weve achieved it? And most importantly, in what ways will it improve enterprise processes and capabilities beyond the already significant advancements of AI and ML?
The most well-known cognitive platform these days is IBM Watson. Not only is it a Jeopardy! champion, it is playing an increasingly crucial role in numerous data-intensive industries, such as health care, finance and high-tech manufacturing. But Watson is not the only cognitive solution for the enterprise. Companies like Enterra Solutions, Attivio and Diwo are putting cognitive to work on tasks like app development, search and even security. (For more on AI's potential, see Can Creativity Be Implemented in AI?)

Tech moves fast! Stay ahead of the curve with Techopedia!
Join nearly 200,000 subscribers who receive actionable tech insights from Techopedia.
Mixed Results
So far, however, initial results have been mixed. Even Watson sometimes has trouble distilling the truth from large, often conflicting, data sets. But like all intelligent systems, cognitive has the ability to learn from and adapt to changing environments, which allows them to steadily improve their own performance without manual coding. And this is leading to the very real possibility that before long knowledge work of all types will be largely managed by autonomous, self-learning platforms.
But if this is academic to all intelligent systems, what differentiates cognitive solutions from run-of-the-mill AI? According to RT Insights Joel Hans, the key difference lies in the way cognitive processes information. Standard intelligence is very effective at determining which action among a set of predefined options is the most appropriate in a given situation. So an intelligent assistant, for example, can parse the wording of a certain request and select a response from an existing menu. A cognitive solution, however, attempts to emulate human thought to engage in contextually aware problem-solving. This puts cognitive more on the level of an assistant that can give advice and ascertain the nuance of a problem rather than a simple program that can automatically perform a function.
A key example highlighting the difference between intelligence and cognitive can be found in the operating room. An intelligent system would be able to monitor heart rate, breathing and other factors to regulate the level of anesthesia or even guide a remote scalpel to the precise location. A cognitive assistant, on the other hand, would give advice on procedures and courses of treatment, pulling data from numerous sources that a doctor might not have ready access to.
How can this be applied to business? Frederic Laluyaux, president and CEO of Aera Technology, argues that the key application is digitizing the executive function by training machines to evaluate conflicting goals and data to then choose from a variety of options based on logic, rationality, causal analysis and experience. Leading neuroscientists are already mapping how this takes place in the human brain, so the next step is to apply the same learning process to AI.
For instance, an immature human brain may require some time to master a complex task, such as tying a shoe, but once the skill is learned it becomes automatic. As situations become more complex, the brain must rely on a larger data store, much of it external, in order to arrive at a conclusion. From that point, it must then devise sort of a threat meter to weigh the seriousness of a given situation and the amount of attention it deserves. Just as a human brain must undergo these developmental stages in order to achieve executive-level decision-making capability, so too must an intelligent platform. This is why most experts are not overly concerned when platforms like Watson cannot perform flawlessly right out of the box  it has to learn what needs to be done. (We need computers, but do they need us? Check out Another Look at Man-Computer Symbiosis.)
Getting Smarter
But this development is a two-way street. As cognitive evolves and changes, so too will the enterprise. Mobile Business Insights Rose de Fremery notes that a cognitive enterprise will have the capacity for exponential learning and continuous, self-directed optimization, which can be used to gain a competitive advantage by leveraging complex technologies like blockchain, the IoT and advanced 3D printing.
To successfully navigate this transition, however, the enterprise needs to adopt cognitive technologies with a clear plan in mind. While many organizations will undoubtedly use them to shore up positions in established markets, others are looking to pioneer new processes and business models to either remake existing industries along more digital, service-driven lines or create entirely new ones for an increasingly connected world.
At some point, however, cognitive technologies must generate practical applications that serve to improve or expand the way knowledge work is performed today. To Phanikishore Burre, vice president of cloud, infrastructure and security services at CSS Corp., the most pressing use cases for cognitive are:
	Predictive Maintenance  in which huge data sets can be leveraged to anticipate failures in both digital and mechanical systems;

	Interdependency Analytics  mapping the relationships between systems and events to ascertain existing and potential trouble spots and dynamically strive for optimal performance;

	Self-Healing/Autonomous Remediation  automatic restoration of critical infrastructure, applications and software using a combination of automated instrumentation, machine learning analytics and integrated remediation;

	Self-Learning Systems Management  ensuring that intelligence is always accessible in the context of a given task, making it easier to access relevant information, tools, templates and other resources, and;

	Smart Agents  intelligent, connected virtual assets that can detect and respond to internal and external environments to transition the enterprise from real-time control to predictive, autonomous control.
Cognitive technology is sometimes referred to as a thinking computer, but this is not entirely correct. The basic underpinnings of human thought and consciousness are still a mystery, while cognitive systems attempt to mimic the results of human intellect through highly advanced algorithmic processes. This means they can be broken down, analyzed and restructured on a finite level, giving humans ultimate control over how they behave.
In many ways, cognitive solutions can outperform the human brain, particularly when it comes to processing large, complex data sets. But ultimately, the brain wins out because it thinks for itself, not for someone else.

TECH:
Cognitive computing named as major trend driving the future of business
July 29, 2016 | Written by: Michael Belfiore
Categorized: AI for the Enterprise
Share this post:
For the fourth year running, the Future Today Institute, a leading forecasting and strategy firm, has named cognitive computing as a major technology trend driving whats next in business and government. Along with deep learning, big data, drones, virtual reality, robotics, spaceflight and others, cognitive computing is a major technology-driven disrupter, of both human behavior and society itself, according to the institutes 2016 Trend Report.
Cognitive Predicted to be Critical Tech Trend for 2017
The Future Today Institute uses its forecasting model to provide guidance to Fortune 500 companies as well as government agencies, non-profits and universities on important emerging trends, with the goal of helping them thrive in the face of rapid technological change.
Far from the latest fad or technology of the moment, trends singled out by the Institute are those that drive long-term changes within key industries, ultimately shaping how consumers behave and how businesses will evolve. They represent the intersection of human nature and technological breakthroughs.

What is Cognitive Computing?
Cognitive computing has gained prominence as a major disrupter by enabling computers to interact with people in human-like ways. These systems understand and communicate in natural language, and leverage artificial intelligence to present new insights that far surpass those gleaned by human intelligence alone.
At the forefront of this technology, and cited by the 2016 Tech Trends report as the single most important innovator to watch, is IBM Watson.
With cognitive systems like Watson, we are now able to unlock the value in data that was previously inaccessible because it existed in an unstructured format or was dispersed in any number of separate silos. Cognitive systems think more like humans, at immense scale, which redefines what is possible to discover and to do. Over time, these new technologies will serve to transform jobs, businesses, customer experiences and entire industries.
Cognitive technologies can help multiple teams at companies improve performance and ROI. From a customer service perspective, cognitive technologies help businesses personalize, humanize and automate customer experiences by finding real-time insights in both structured AND unstructured data including social media posts, emails, audio recordings, documents, manuals and more. This formerly dark datawhich lives in internal, external, cloud-based and even publicly-available sourceshas rarely been utilized by companies, even though they collect and store large volumes of unstructured content on a regular basis. With cognitive computing, companies can find valuable insights and significant opportunities hidden in this data.
In the mainstream, Watson may best be known as the system that defeated the reigning champion on the TV game show Jeopardyin 2011, deftly answering the hosts spoken questions and returning the correct answers more quickly than any human competitor.
Watson has evolved since then into a powerful, commercially-available range of services. Watson services combines search and analysis of unstructured content like emails, social media posts and audio recordings  data that was out of reach for conventional systems  with unique cognitive computing capabilities to scale human expertise.
For example, Watson Explorer helps companies connect, analyze and quickly find the information they need through a 360-degree view of all data to reveal critical insights and inform better decisions.
Watson is advancing industries from retail and telecommunications to healthcare and banking by quickly accessing large amounts of data and providing information and insights that would take human workers much longer to find and parse on their own.
Cognitive computing, however, does not operate in a vacuum. The key to its success is two other trends on the Future Today Institutes 2016 watch list: data and deep learning.
Harnessing Data Better With Cognitive Computing
One of the reasons data made the list is because it is a key driver of many other technologies, including cognitive computing. Major industries, governments and individuals increasingly rely on data from an ever-expanding number of sources to help them do everything from fixing machines to making smarter purchases.
Stakeholders are discovering new potential for how to harness data to improve business performance across departments. They are quickly finding, however, that each new opportunity comes with at least one major challenge in storing, managing and/or interpreting all this data. For example, one Watson client, a U.S. state government, was looking for a tool to manage hundreds of thousands of online documents and relay crucial insurance requirements to its citizens quickly and easily. This state government team is currently leveraging Watson to make sense of this data cache and keep its residents informed while reducing technology and data management costs. For this and many other reasons, the Future Today Institute has named IBM Watson as a technology to watch in this field.
Cognitive Computing and Deep Learning
The 2016 Trends report points to deep learning experiments from major companies like IBM, Google and Facebook, as well as neural network projects from university labs among its technologies to watch. These programs are teaching computers to recognize faces, see in the dark, translate spoken and written language and much more.
Watsons cognitive computing abilities utilize deep learning processes to analyze and draw insights from unstructured data in ways that only the smartest human minds could beforebut at a much, much faster rate. In the automotive industry, for example, Watson is helping a automobile manufacturing company identify not only potential safety issues with specific vehicle models, but also the potential causes of those issues, very quickly, so issues can proactively be fixed before they lead to a costly recall. Watson does this by gleaning information hidden in thousands of comments by consumers who report problems to the U.S. National Highway Traffic Safety Administration.
Solving Tomorrows Challenges with Cognitive Computing
The trends cited by the Future Today Institute impact nearly every major industry. In this report, cognitive computing alone has applications in financial services, travel, entertainment, education, infrastructure and transportation, marketing and PR, human resources, and medical and life sciences.
While trends cited by the institute have enormous potential for improving the lives of millions, they also provide a cautionary tale, says the 2016 report. Now more than ever, emerging technology breakthroughs are outpacing the evolution of our public policy and discussions regarding ethics, writes 2016 Trend Report author and Future Today Institute CEO and founder Amy Webb. We must continue to think ahead about how our actions (or lack of actions) today will impact the future of our societies, business and global communities.
Cognitive computing, with its ability to help businesses and policymakers gain insights from formerly hidden information, can and must be part of the solution.

TECH:
COGNITIVE COMPUTING: HOW TO TRANSFORM DIGITAL SYSTEMS TO THE NEXT LEVEL OF INTELLIGENCE


Cognitive Computing: Whats in a Name?
Cognitive computing might be one of the many buzzwords that you today hear and see alongside such terms asArtificial Intelligence, Machine Learning, Deep Learning and Big Data. However, quite opposite to these terms, Cognitive computing, as it seems, does not have a clear definition yet  and this is the fact commonly agreed on inthe industry.
The Cognitive Computing Consortium, which represents a crossdisciplinary group of researchers, developers, and practitioners of cognitive computing and its allied technologies, came up with the best (at present) definition of the term, so were going to use their description to depict how cognitive computing differs from traditional computing.
At the same time, our goal is not to explain the terminology, but rather to demonstrate how this concept can beapplied to real business needs, and implemented with technologies available on the market.
So, coming back to the consortiums definition, Cognitive computing addresses complex situations that arecharacterized by ambiguity and uncertainty; in other words it handles human kinds of problems. Cognitivecomputing systems often need to weigh conflicting evidence and suggest an answer that can be considered asbest rather than right.
This is a good start, which allows for better distinguishing of traditional computing systems from the cognitive ones, as well as helps understand what they are lacking.
Traditional Computing Systems versus Cognitive Computing
Here are some examples of what human kinds of problems might cover:
	Speech understanding
	Face detection
	Recommendations
	Medical diagnosis
	Risk assessment
	Sentiment analysis
	Psychometrics to identify psychological profiles
The last one was allegedly used in the recent US elections for manipulating voters preferences.
Some of these problems are almost intractable for traditional computing techniques although people have been successfully solving them for thousands of years. On the other hand, the majority of them is still challenging even for a human mind.
Your Personal Digital Assistant
Continuing with the Cognitive computing definition: Cognitive computing systems may play the role of an assistant or a coach for the user, and they may act virtually autonomously in many problem-solving situations. Their output may be prescriptive, suggestive, instructive, or simply entertaining.
For example, your personal assistant, should it be Siri, Google Assistant, Alexa or Cortana is a great example of a Cognitive computing system for personal use.
Implementation Approaches
But can an enterprise system have Cognitive computing capabilities? Of course, it can, and there are multiple examples: fraud detection in Finance, investment risk management, commerce recommender systems, predictive maintenance in manufacturing, detection of anomalies in Oil & Gas production cycle to prevent oil spills and so on.
As the Cognitive Computing Consortium suggests, in order to achieve the cognitive level of computing, the systems must be Adaptive, Interactive, Iterative and stateful, and Contextual.
Below we will analyze these requirements and map to possible implementation approaches which help achieve better intelligence for digital systems:
	Adaptive  as the information changes, and goals and requirements evolve, the systems must learn. They must resolve ambiguity and tolerate unpredictability. They must be engineered to feed on dynamic data in real time, or near real time.
This is a reasonable requirement, after all, cognitive systems are supposed to mimic the ability of humans to learn and improve from experience. This implies that one cant just write a program for solving a particular task, the program itself should be dynamic enough in order to adapt and improve with experience. From the implementation standpoint, this requirement falls well into the Machine Learning approach, which by definition gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959).
Over many years, Machine Learning community has been developing new techniques and algorithms, gradually increasing their accuracy. During this time, performance of the majority of AI applications remained to be sub-human, i.e. it was worse than average human performance. But starting from 2012, the situation has started to change, and the changes were quite drastic. Although there were several factors that made this progress possible, perhaps the most important one was the emerging of the Deep Learning field, which is a family of Machine Learning techniques inspired by cognitive and neuroscience that are now considered state-of-the-art in Artificial Intelligence.
	Interactive  the systems must easily interact with users so that those users can define their needs in a comfortable and natural way. They may also interact with other processors, devices, and Cloud services, as well as with people.
User Experience and Design Thinking methodologies in particular are a great foundation to start designing the interactivity. Modern human interfaces may vary a lot, starting from traditional Mobile, Web or Wearable to innovative speech, gestures or even mind controlled systems. Moreover, this problem is bi-directional. Cognitive systems should not only understand human input; they also should act and provide their results in the way people will find natural and easy to understand. A recent breakthrough in Deep Learning and AI particularly addresses these challenges giving a way to achieve a near human-level of experience in interacting with cognitive systems.
System interfaces between devices often require fast and lightweight M2M protocols such as MQTT or CoAP. These are de-facto standards in modern IoT solutions, often supported by Cloud platforms with IoT capabilities such as AWS or MS Azure or more specialized like C3 IoT or PTC ThingWorx.
	Iterative and stateful  the systems must aid in defining a problem by asking questions or finding an additional source input if a problem statement is ambiguous or incomplete. They must remember previous interactions in a process and return information that is suitable for the specific application at that point in time.
Mitigating todays financial, market, and even social risks makes reliability of the Cognitive systems the highest priority for businesses, as they are supposed to provide highly trusted and consistent results. From the design and architectural standpoints, meeting this requirement requires careful application of the data quality and validation methodologies in order to ensure that the system is always provided with enough information, and that the data sources it operates on deliver reliable and up-to-date input.
	Contextual  the systems must understand, identify, and extract contextual elements such as meaning, syntax, time, location, appropriate domain, regulations, users profile, process, task, and goal. They should draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided).
As Cognitive Systems aim to handle real world problems, which are highly uncertain and may be influenced by potentially unlimited number of different factors, quality and consistency of their results highly depends on the number of factors they consider while making the decision. That brings yet another technological trade-off as the complexity of the problem grows tremendously with the number of the data sources. Aggregating and integrating the data from different data sources and processing it in a unified way is also challenging. Here is where Apache Spark project comes into play providing distributed and highly efficient tool that covers most of the present data processing and analytics routines. It also addresses another important requirement of having the most important data available for the real-time ad-hoc access while being able to reach a long track of historical data for better insight. A complex Cognitive system usually is a combination of multiple technologies that are glued together. As an example, Kafka, Spark, Elasticsearch, Cassandra, Hadoop, and other technologies can coexist in a single solution to handle in-memory hot contextual data and a batch processing of massive cold historical data. The science and art of designing such complex solutions now is a subject of cooperation between practitioners and academia. The SmartDecisions game, which simulates the software architecture design process in a format that makes the learning process more enjoyable for students, is a good example of such a cooperation that introduces gamification to solving comprehensive technological challenges in a fun way.
Conclusion
To sum up, Cognitive Computing doesnt bring a drastic novelty into the AI and Big Data industry. Rather it urges digital solutions to meet  human-centric requirements: act, think, and behave like a human in order to achieve maximum synergy from human-machine interaction.


TECH:
Cognitive Computing Market 2018 Global Trend, Segmentation And Opportunities Forecast To 2023
Posted: Jul 16, 2018 7:09 PM IST
Cognitive Computing
Cognitive Computing -Market Demand, Growth, Opportunities and Analysis Of Top Key Player Forecast To 2023
Cognitive Computing Industry
Description
Wiseguyreports.Com Adds Cognitive Computing -Market Demand, Growth, Opportunities and Analysis Of Top Key Player Forecast To 2023 To Its Research Database
Cognitive computing like many IT terms is defined both by its technical capability and applications and by marketing statements. From a technical standpoint, cognitive computing is a system that uses machine learning algorithms to continually increase its knowledge from the data it receives. The learning aspect is characterized by the system continuously refining patterns in the data as well as the way data is processed. This learning allows cognitive computing systems to anticipate new problems and model possible solutions in an adaptive and iterative manner. The system also works within a context and can identify and extract contextual elements such as meaning, syntax, time, location, regulation, user profiles, tasks and goals.
Applications of cognitive computing are derived from data mining, pattern recognition and natural language processing. These applications automate manual processes or augment more basic computing processes. The marketing definition of cognitive computing is to solve problems as well as or better than humans without human assistance. However, cognitive computing may best be applied as an enabler in a process that involves both machines and humans rather than one replacing the other.
As with the field of artificial intelligence in general, cognitive computing is now coming into practical application because of a convergence of underlying technologies. These include high performance processors and advanced algorithms, which combine to enable computing systems to leverage machine learning, natural language processing and expert systems to provide new value in dealing with consumer and business applications.
Request for Sample Report @ https://www.wiseguyreports.com/sample-request/3249400-cognitive-computing-applications-and-global-markets
While cognitive computing represents the continued evolution of artificial intelligence to practical commercial applications, it is currently benefiting from accelerated adoption in the market caused by its ability to make sense of unstructured data. According to some estimates, the amount of data being generated globally is growing at a rate of REDACTED% per year. Most of this growth is from unstructured data types (e.g., data that does not fit neatly into a database, including documents, videos, photos, audio files, presentations, web pages, etc.). Conventional database management systems are not designed toorganize and interpret this data, limiting the applicability of IT organization to make use of it. Entercognitive computing, which can apply new processes to discern patterns that can be used to manage,interpret and act on unstructured data.
The demand for this kind of deep data analysis and autonomous response is driving the cognitive computing market to grow to $REDACTED billion by 2022 at a compound annual growth rate (CAGR) of REDACTED% for 2017-2022. The market was $REDACTED billion in 2016 and grew to $REDACTED billion in 2017. With a foundation in expert systems the market can be segmented into expert systems, machine learning and natural language processing.
North America is the leading market with a REDACTED% share, followed by Europe at REDACTED% and Asia-Pacific at REDACTED%. While enterprise is a wide adopter of cognitive computing, governments with the highest growth rates and applications are pioneering cognitive computing in education and healthcare. Within enterprise applications and financial services, operations and marketing are gaining traction.
Report Scope
The scope of this report covers the overall Cognitive Computing technologies market with market sizing and trends analysis for the most recently completed actuals for 2016 as well as forecasts, trends and compound annual growth rates (CAGRs) for 2017 through 2022.
The market is segmented by end user, technology and region. End user segments include education, government and enterprise. Applications include the military, healthcare and manufacturing sectors.
Report Includes
 An overview of the global market for cognitive computing 
 Analyses of global market trends, with data from 2017, 2018, and projections of compound annual growth rates (CAGRs) through 2023 
 Insight into the types of cognitive computing technologies, including natural language processing, neural networks, and expert systems 
 Descriptions of key patents related to the market 
 Information on key components, applications, and market trends 
 Profiles of major players and companies in the market, including Google Deepmind AI Solution, Intel Saffron, Numenta, Welltok Analytics, Cisco Cognitive Threat Analysis and Abbyy

TECH:
	WHAT IS COGNITIVE COMPUTING? FEATURES, SCOPE & LIMITATIONS

Human thinking is beyond imagination. Can a computer develop such ability to think and reason without human intervention? This is something programming experts at IBM Watson are trying to achieve. Their goal is to simulate human thought process in a computerized model. The result is cognitive computing  a combination of cognitive science and computer science. Cognitive computing models provide a realistic roadmap to achieve artificial intelligence.
Reference  https://www.datanami.com/2016/09/13/sas-goes-back-future-cognitive-computing-viya/
Cognitive computing represents self-learning systems that utilize machine learning models to mimic the way brain works. Eventually, this technology will facilitate the creation of automated IT models which are capable of solving problems without human assistance
Cognition comes from the human brain. So whats the brain of cognitive systems?
Cognitive computing represents the third era of computing. In the first era, (19th century) Charles Babbage, also known as father of the computer introduced the concept of a programmable computer. Used in the navigational calculation, his computer was designed to tabulate polynomial functions. The second era (1950) experienced digital programming computers such as ENIAC and ushered an era of modern computing and programmable systems. And now to cognitive computing which works on deep learning algorithms and big data analytics to provide insights. Thus the brain of a cognitive system is the neural network, fundamental concept behind deep learning. The neural network is a system of hardware and software mimicked after the central nervous system of humans, to estimate functions that depend on the huge amount of unknown inputs.
What are the features of a cognitive computing solution?
With the present state of cognitive function computing, basic solution can play an excellent role of an assistant or virtual advisor. Siri, Google assistant, Cortana, and Alexa are good examples of personal assistants. Virtual advisor such as Dr. AI by HealthTap is a cognitive solution. It relies on individual patients medical profiles and knowledge gleaned from 105,000 physicians. It compiles a prioritized list of the symptoms and connects to a doctor if required. Now, experts are working on implementing cognitive solutions in enterprise systems. Some use cases are fraud detection using machine learning, predictive analytics solution, predicting oil spills in Oil and Gas production cycle etc.
The purpose of cognitive computing is the creation of computing frameworks that can solve complicated problems without constant human intervention. In order to implement cognitive function computing in commercial and widespread applications, Cognitive Computing consortium has recommended the following features for the computing systems 
1. Adaptive
This is the first step in making a machine learning based cognitive system. The solutions should mimic the ability of human brain to learn and adapt from the surroundings. The systems cant be programmed for an isolated task. It needs to be dynamic in data gathering, understanding goals, and requirements.
2. Interactive
Similar to brain the cognitive solution must interact with all elements in the system  processor, devices, cloud services and user. Cognitive systems should interact bi-directionally. It should understand human input and provide relevant results using natural language processing and deep learning. Some skilled intelligent chatbots such as Mitsuku have already achieved this feature.
3. Iterative and stateful
The system should remember previous interactions in a process and return information that is suitable for the specific application at that point in time. It should be able to define the problem by asking questions or finding an additional source. This feature needs a careful application of the data quality and validation methodologies in order to ensure that the system is always provided with enough information and that the data sources it operates on to deliver reliable and up-to-date input.
4. Contextual
They must understand, identify, and extract contextual elements such as meaning, syntax, time, location, appropriate domain, regulations, users profile, process, task, and goal. They may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided).

What is the scope of cognitive computing?
While computers have been faster at calculations and processing than humans for decades. But they have failed miserably to accomplish tasks that humans take for granted, like understanding the natural language or recognizing unique objects in an image. Thus cognitive technology makes such new class of problems computable. They can respond to complex situations characterized by ambiguity and have far-reaching impacts on our private lives, healthcare, business, etc.
According to a study by the IBM Institute for Business Value, Your Cognitive Future, scope of cognitive computing consists of engagement, decision, and discovery. These 3 capabilities are related to ways people think and demonstrate their cognitive abilities in everyday life.
1. Engagement
The cognitive systems have vast repositories of structured and unstructured data. These have the ability to develop deep domain insights and provide expert assistance. The models build by these systems include the contextual relationships between various entities in a systems world that enable it to form hypotheses and arguments. These can reconcile ambiguous and even self-contradictory data. Thus these systems are able to engage in deep dialogue with humans. The chatbot technology is a good example of engagement model. Many of the AI chatbots are pre-trained with domain knowledge for quick adoption in different business-specific applications.
2. Decision
A step ahead of engagement systems, these have decision-making capabilities. These systems are modeled using reinforcement learning. Decisions made by cognitive systems continually evolve based on new information, outcomes, and actions. Autonomous decision making depends on the ability to trace why the particular decision was made and change the confidence score of a systems response. A popular use case of this model is the use of IBM Watson in healthcare. The system can collate and analyze data of patient including his history and diagnosis. The solution bases recommendations on its ability to interpret the meaning and analyze queries in the context of complex medical data and natural language, including doctors notes, patient records, medical annotations and clinical feedback. As the solution learns, it becomes increasingly more accurate. Providing decision support capabilities and reducing paperwork allows clinicians to spend more time with patients.
3. Discovery
Discovery is the most advanced scope of cognitive computing. Discovery involves finding insights and understanding vast amount of information and developing skills. These models are built on deep learning and unsupervised machine learning. With ever-increasing volumes of data, there is a clear need for systems that help exploit information more effectively than humans could on their own. While still in the early stages, some discovery capabilities have already emerged, and the value propositions for future applications are compelling. Cognitive Information Management (CIM) shell at Louisiana State University (LSU) is one of the cognitive solutions. The distributed intelligent agents in the model collect streaming data, like text and video, to create an interactive sensing, inspection, and visualization system that provides real-time monitoring and analysis. The CIM Shell not only sends an alert but reconfigures on the fly in order to isolate a critical event and fix the failure.
Cognitive computing landscape
Present cognitive computing landscape is dominated by larger players  IBM, Microsoft, and Google. IBM being the pioneer of this technology has invested $26 billion dollars in big data and analytics and now spends close to one-third of its R&D budget in developing cognitive computing technology. Many other companies and organizations are developing products and services that are as good, if not better than Watson. IBM and Google have acquired some of the rivals and the market is moving towards consolidation. Lets take a look at the prominent players in this market 
1. IBM Watson
Originally Watson is an IBM supercomputer that combines artificial intelligence (AI) and sophisticated analytical software for optimal performance as a question answering machine famously featured in show Jeopardy. Now it uses a set of transformational technologies such as natural language processing, image recognition, text analytics and virtual agents. IBM Watson leverages deep content analysis and evidence-based reasoning. Combined with massive probabilistic processing techniques, Watson can improve decision making, reduce cost and optimize outcomes.
2. Microsoft Cognitive Services
Microsoft cognitive services previously known as Project Oxford are a set of APIs, SDKs and cognitive services which the developers can use to make their applications more intelligent. With Cognitive Services, developers can easily add intelligent features  such as emotion and sentiment detection, vision and speech recognition, knowledge, search and language understanding  into their applications. We have made a chatbot Specter using Microsoft Bot Framework to improve the efficiency of our marketing team.
3. Google DeepMind
DeepMind was acquired by Google in 2014 and considered to be a leading player in AI research. The team consists of many renowned experts in the field of deep neural networks, reinforcement learning, and systems neuroscience-inspired models. DeepMind became popular with AlphaGo, a narrow AI to play Go, a Chinese strategy board game for two players. AlphaGo became the first AI program to beat a professional human player in October 2015, on a full-sized board.
4. CognitiveScale
CognitiveScale founded by former members of IBM Watson team provides cognitive cloud software for enterprises. Cognitive Scales augmented intelligence platform delivers insights-as-a-service and accelerates the creation of cognitive applications in healthcare, retail, travel, and financial services. They help businesses make sense from dark data  messy, disparate, first and third party data and drive actionable insights and continuous learning.
5. SparkCognition
SparkCognition is an Austin-based startup formed in 2014. SparkCognition develops AI-Powered cyber-physical software for the safety, security, and reliability of IT, OT, and the IIoT. The technology is more inclined towards manufacturing. It is capable of harnessing real-time sensor data and learning from it continuously, allowing for more accurate risk mitigation and prevention policies to intervene and avert disasters.
Watson and DeepMinds success has inspired other companies to develop cognitive platforms using open source tools. Other leading technology companies like Qualcomm and Intel are taking cautious steps to include cognitive solutions for specialized industries. Uber has established a research arm dedicated to AI and machine learning and acquired Geometric Intelligence and Otto. Otto is an autonomous truck and transportation startup and Geometric Intelligence is focused on generating insights from fewer data using machine learning. Gamalon has developed an AI technique using Bayesian Program Synthesis. It requires only a few pieces to train the system to achieve same levels of accuracy as neural networks.
Healthcare is the most popular sector to adopt cognitive solutions. Startups such as Lumiata and Enlitic have developed small and powerful analytic solutions that assist healthcare providers in diagnosis and prediction of disease conditions.Other companies in this market are Cisco cognitive threat analytics, CustomerMatrix, Digital Reasoning and Narrative Science.
Limitations of cognitive computing
Limited analysis of risk
The cognitive systems fail at analyzing the risk which is missing in the unstructured data. This includes socio-economic factors, culture, political environments, and people. For example, a predictive model discovers a location for oil exploration. But if the country is undergoing a change in government, the cognitive model should take this factor into consideration. Thus human intervention is necessary for complete risk analysis and final decision making.
Meticulous training process
Initially, the cognitive systems need training data to completely understand the process and improve. The laborious process of training cognitive systems is most likely the reason for its slow adoption. WellPoints financial management is facing a similar situation with IBM Watson. The process of training Watson for use by the insurer includes reviewing the text on every medical policy with IBM engineers. The nursing staff keeps feeding cases until the system completely understands a particular medical condition. Moreover, the complex and expensive process of using cognitive systems makes it even worse.
More intelligence augmentation rather than artificial intelligence
The scope of present cognitive technology is limited to engagement and decision. Cognitive computing systems are most effective as assistants which are more like intelligence augmentation instead of artificial intelligence. It supplements human thinking and analysis but depends on humans to take the critical decisions. Smart assistants and chatbots are good examples. Rather than enterprise-wide adoption, such specialized projects are an effective way for businesses to start using cognitive systems.
Cognitive computing is definitely the next step in computing started by automation. It sets a benchmark for computing systems to reach the level of the human brain. But it has some limitations which make AI difficult to apply in situations with a high level of uncertainty, rapid change or creative demands. The complexity of problem grows with the number of data sources. It is challenging to aggregate, integrate and analyze such unstructured data. A complex cognitive solution should have many technologies that coexist to give deep domain insights.
Thus, besides AI, ML and NLP, technologies such as NoSQL, Hadoop, Elasticsearch, Kafka, Spark etc should form a part of the cognitive system. This complete solution would be capable of handling dynamic real-time data and static historical data. The enterprises looking to adopt cognitive solutions should start with a specific business segment. These segments should have strong business rules to guide the algorithms, and large volumes of data to train the machines.


TECH:
016, 03:28am
What Everyone Should Know About Cognitive ComputingBernard Marr
	
	
	
Artificial intelligence has been a far-flung goal of computing since the conception of the computer, but we may be getting closer than ever with new cognitive computing models.
Cognitive computing comes from a mashup of cognitive science  the study of the human brain and how it functions  and computer science, and the results will have far-reaching impacts on our private lives, healthcare, business, and more.
What is cognitive computing?
The goal of cognitive computing is to simulate human thought processes in a computerized model. Using self-learning algorithms that use data mining, pattern recognition and natural language processing, the computer can mimic the way the human brain works.
While computers have been faster at calculations and processing than humans for decades, they havent been able to accomplish tasks that humans take for granted as simple, like understanding natural language, or recognizing unique objects in an image.
Some people say that cognitive computing represents the third era of computing: we went from computers that could tabulate sums (1900s) to programmable systems (1950s), and now to cognitive systems.
These cognitive systems, most notably IBM IBM -1.09%s Watson, rely on deep learning algorithms and neural networks to process information by comparing it to a teaching set of data.  The more data the system is exposed to, the more it learns, and the more accurate it becomes over time, and the neural network is a complex tree of decisions the computer can make to arrive at an answer.
What can cognitive computing do?
For example, according to this TED Talk video from IBM, Watson could eventually be applied in a healthcare setting to help collate the span of knowledge around a condition, including patient history, journal articles, best practices, diagnostic tools, etc., analyze that vast quantity of information, and provide a recommendation.

Cognitive computing and artificial intelligence (source: Shutterstock)
The doctor is then able to look at evidence-based treatment options based on a large number of factors including the individual patients presentation and history, to hopefully make better treatment decisions.
In other words, the goal (at this point) is not to replace the doctor, but expand the doctors capabilities by processing the humongous amount of data available that no human could reasonably process and retain, and provide a summary and potential application.
This sort of process could be done for any field in which large quantities of complex data need to be processed and analyzed to solve problems, including finance, law, and education.
These systems will also be applied in other areas of business including consumer behavior analysis, personal shopping bots, customer support bots, travel agents, tutors, security, and diagnostics.  Hilton Hotels recently debuted the first concierge robot, Connie, which can answer questions about the hotel, local attractions, and restaurants posed to it in natural language.
The personal digital assistants we have on our phones and computers now (Siri and Google GOOGL -0.82% among others) are not true cognitive systems; they have a pre-programmed set of responses and can only respond to a preset number of requests.  But the time is coming in the near future when we will be able to address our phones, our computers, our cars, or our smart houses and get a real, thoughtful response rather than a pre-programmed one.
As computers become more able to think like human beings, they will also expand our capabilities and knowledge. Just as the heroes of science fiction movies rely on their computers to make accurate predictions, gather data, and draw conclusions, so we will move into an era when computers can augment human knowledge and ingenuity in entirely 

TECH:
Planning for 2017? Top 3 ways you can add business value with cognitive solutions
Share this post:
IT departments are currently plagued by gaps between data collection, insights and action. They constantly find themselves hand-holding non-technical teams like marketing, sales and HR. And often times, they have a limited number of information sources at their disposal. That is, if they havent empowered their company with cognitive technologies.
Many business leaders already understand that cognitive computing is going to be a major disrupter. Whether you choose to embrace it to your advantage or not is probably going to determine the outcome of how your business performs and survives going forward.
Today, there are more data multipliers than ever beforeincluding humans, machines, and business processesand the volume of structured and unstructured data (social media posts, documents, emails, images, videos, audio recordings, customer feedback, manuals, industry reports etc.) is growing exponentially. For example, by 2017, health data is expected to grow 99% (88% of it unstructured), insurance data by 94% (84% of it unstructured) and manufacturing data by 99% (82% of it unstructured).
Until recently, businesses were blind to the valuable insights hidden in unstructured content. Then businesses started implementing cognitive solutions through their IT departments, and trends, patterns, insights, and relationships became readily apparent. Below is just a snapshot of the many ways cognitive computing is empowering IT departments to boost performance across teams and improve their bottom line.
What can you do with cognitive solutions like Watson?
In todays data-driven economy, more than 30% of all companies are expected to pursue advanced analytics and cognitive computing to stay competitive and drive revenue. The result is CMOs, CIOs, CTOs and CDOs working together to bridge gaps between teams and connect the data, tools and insights they use to improve decision-making, leverage employee expertise, gain a 360-degree customer view and implement faster data-to-execution business practices.
VIDEO: How IT and Watson are partnering in the cognitive era
1. Put every employee behind the wheel
In the automotive industry, the only thing more unique than the make, model, and color of every car is the financing package for every customer. Each driver has a unique mix of income sources and savings that are used to determine their loan offer. This poses a huge challenge for financial service agents who must look through all of this data and determine the most beneficial price and payment plan for both the buyer and the manufacturer.
With the customers consent, agents can use cognitive technologies to search and analyze the customers data sources ranging from their driving history to their credit score, to then create a transparent 360-degree view of the customer. Empowered by this information, agents can make justifiable offers that align with the customers finances and needs. This cognitive vetting process prevents agents from having to break the customer interaction in order to manually dig through structured and unstructured data or get a supervisor or tech support on the phone. From the sales frontlines to the back office and every other department in between, cognitive systems are able to empower teams to improve customer satisfaction and drive business growth.
Cognitive systems are very good at doing the heavy liftingpulling data together, analyzing the information and then presenting the relevant answers to users so that they can make more confident and effective business decisions that impact performance and revenue.
Vincent Thomas, Client Engagement Leader, IBM Watson
2. Detect product safety issues proactively to avoid costly recalls
When it comes to assessing product performance, cognitive analytics are incomparable. By monitoring usage data and crash reports, cognitive technologies can quickly identify the source of a device malfunction and determine the best course of corrective actionbe it a recall, software update or assembly line adjustment.
Product issues can have a serious impact on businesses. This is especially true when these problems manifest themselves as product safety issues causing injury, or even death. Whether its a car or a childrens toy or an advanced medical device, product safety issues dont just result in negative publicity about your products and services, they can also lead to millions of dollars in lawsuits and liability.
Companies need to leverage all their internal, external and publicly available data to identify issues as quickly as possible to get ahead of and avoid negative press, huge penalties by industry regulators, and most importantly, to protect the safety of their customers. Identifying emerging issues before they escalate helps protect reputations, customer loyalty and money. This also helps avoid significant time-to-market delays that occur when devices fail to perform as intended.
VIDEO: How auto manufacturers are using cognitive solutions to prevent product recalls and even save lives
3. Cognitive Solutions Can Improve Everyones Performance
Nothing hinders someones performance like poor health, and cognitive technologies are helping mankind combat some of the worlds most difficult diseases by pairing decades of clinical research and patient data with cognitive analytics. This allows doctors to better identify potential risk factors for their patients and put them on the best treatment plan for their specific health profile. As for researchers, these insights could reveal patterns that lead to new, more effective pharmaceuticals. The result is workplaces with healthier and productive employees.
How Cognitive Computing Works
Cognitive computing systems typically run on multiple technologies including:
 Natural language queries and processing
 Machine learning algorithms
 Real-time computing
These new technologies allow cognitive systems to sift through stockpiles of structured and unstructured data to derive valuable insights that would take even the smartest humans more than a lifetime of reading and research to discover. And like humans, cognitive systems continue to learn as more information is made available to them. Thats great news, considering that the digital universe is growing by 40% a year.
While many competitors are investing in their own cognitive and Artificial Intelligence solutions, Watson is the only cognitive computing platform thats made specifically to support a broad range of enterprise operations like the ones mentioned above. Other Watson highlights include a 300-million word English vocabulary, a grasp of seven other languages, a thorough understanding of idioms and nuances, impeccable interview skills and a Jeopardy championship.
Weve already seen some of the amazing things cognitive computing can bring to multiple departments in your company including sales, marketing, and product design, and were just beginning to scratch the surface. To learn more about how you can unlock productivity gains with cognitive computing click on one of the topics below:


TECH:
Cognitive Computing: Benefits and Challenges of the Next Tech Revolution
by BarcelonaTech UPC

Posted on 13/Apr/2015
Cognitive Computing is going to transform and improve our lives. But it also presents challenges that we need to be conscious of in order to make the best use of this technology.
Co-authored by: Mateo Valero and Jordi Torres
Big Data technology allows companies to gain the edge over their business competitors and, in many ways, to increase customer benefits. For customers, the influences of big data are far reaching, but the technology is often so subtle that consumers have no idea that big data is actually helping make their lives easier.
For instance, in the online shopping arena, Amazons recommendation engine uses big data and its database of around 250 million customers to suggest products by looking at previous purchases, what other people looking at similar things have purchased, and other variables.
They are also developing a new technology which predicts what items you might want based on the factors mentioned above and sends it to your nearest delivery hub, meaning faster deliveries for us.
To do so, they are using predictive models, a collection of mathematical and programming techniques used to determine the probability of future events, analyzing historic and current data to create a model to predict future outcomes.
Today, predictive models form the basis of many of the things that we do online: search engines, computer translation, voice recognition systems, etc. Thanks to the advent of Big Data these models can be improved, or trained, by exposing them to large data sets that were previously unavailable.
And it is for this reason that we are now at a turning point in the history of computing. Throughout its short history, computing has undergone a number of profound changes with different computing waves.
In its first wave, computing made numbers computable.
The second wave has made text and rich media computable and digitally accessible. Nowadays, we are experiencing the next wave that will also make context computable with systems that embed predictive capabilities, providing the right functionality and content at the right time, for the right application, by continuously learning about them and predicting what they will need.
For example identify and extract context features such as hour, location, task, history or profile to present an information set that is appropriate for a person at a specific time and place.
The general idea is that instead of instructing a computer what to do, we are going to simply throw data at the problem and tell the computer to figure it out itself.
We changed the nature of the problem from one in which we tried to explain to the computer how to drive, to one in which we say, Heres a lot of data, figure out how to drive yourself. For this purpose the computer software takes functions from the brain like: inference, prediction, correlation, abstraction,  giving to the systems to possibility to do this by themselves. And here it comes the use of cognitive word to describe this new computing.
These reasoning capabilities, data complexity, and time to value expectations are driving the need for a new class of supercomputer systems such as those investigated in our research group in Barcelona.
It is required a continuous development of supercomputing systems enabling the convergence of advanced analytic algorithms and big data technologies driving new insights based on the massive amounts of available data. We will use the term Cognitive Computing (others use Smart Computing, Intelligent Computing, etc.) to label this new type of computing research.
We can find different examples of the strides made by cognitive computing in industry. The accuracy of Googles voice recognition technology, for instance, improved from 84 percent in 2012 to 98 percent less than two years later. DeepFace technology from Facebook can now recognize faces with 97 percent accuracy.
IBM was able to double the precision of Watsons answers in the few years leading up to its famous victory in the quiz show Jeopardy. This is a very active scenario.
From 2011 through to May 2014, over 100 companies in the area merged or were acquired. During this same period, over $2 billion dollars in venture capital funds have been given to companies building cognitive computing products and services.
Cognitive Computing will improve our lives. Healthcare organizations are using predictive modeling to assist diagnosing patients and identifying risks associated with care. Or farmers are using predictive modeling to manage and protect crops from planting through harvest.
But there are problems that we need to be conscious of. The first, is the idea that we may be controlled by algorithms that are likely to predict what we are about to do.
Privacy was the central challenge in the second wave era. In the next wave of Cognitive Computing, the challenge will be safeguarding free will. After Snowden revelations we realize its easy to abuse access to data.
There is another problem. Cognitive Computing is going to challenge white collar, professional knowledge work in the 21st century in the same way that factory automation and the assembly line challenged blue collar labor in the 20th century.
For instance, one of the Narrative Sciences co-founder estimates that 90 percent of news could be algorithmically generated by the mid-2020s, much of it without human intervention. Or researchers at Oxford published a study estimating that 47 percent of total US employment is at risk due to the automation of cognitive tasks.
Cognitive Computing is going to transform how we live, how we work and how we think, and thats why Cognitive Computing will be a big deal. Cognitive computing is a powerful tool, but a tool nevertheless  and the humans wielding the tool must decide how to best use it.


TECH:
Cognitive computing for all? Think about it
	2
	
Machines like IBM's Watson are becoming as smart as we are. And soon the technology may become elementary to many more than just Big Blu
Stephanie Neil
The health care industry suffers from an ailment known as information overload. It's a condition that makes it difficult for doctors and insurance providers to quickly determine the procedures that will be required and covered for each patient. The individual's history, years of case files and clinical evidence must be considered -- no small feat for a human. But it's as easy as saying "aah" for Watson -- IBM's artificial intelligence computer system that processes natural language questions against a deep well of data to compute evidence-based answers in a matter of seconds.
Watson can sift through the data equivalent of about 1 million books, analyze the information and provide precise responses to complicated questions in less than three seconds. Insurance provider WellPoint Inc. doesn't have to imagine what that could mean to the health care industry. Through a partnership with IBM that began two years ago, the Indianapolis-based company is working with more than 3,000 physician offices in its network to provide patient coverage and treatment options almost instantaneously.alth IT strategy at WellPoint Inc.
"We trained Watson to think like a nurse or physician on staff," said Elizabeth Bigham, WellPoint's vice president of health IT strategy. "It receives requests from providers, finds medical policies, compares what the provider said in the request, determines who the patient is, what they want to do and why, and renders a recommendation to our staff."
Watson, Bigham said, is a game changer in the medical field. Other companies are also developing Watson-powered applications for health care providers and consumers -- for example, software startup Welltok Inc.'s CafeWell Concierge, which will offer personalized, location-based guidance on diet, exercise and preventive services.
And it's not just health care. In January, IBM launched a Watson Group business unit to ignite new commercialization efforts in a range of industries, including financial services, travel, telecom and retail. To help fuel its efforts, the new unit was given $100 million to invest in third-party software developers; it made an initial investment in Welltok in February. IBM also is making Watson services available in the cloud and providing software developers with access to its Watson application programming interface to build new kinds of cognitive apps. Welltok's project is one of those efforts; another is a Watson-powered "smart adviser" self-service applications, which can understand natural language, read and interpret text and learn from other types of smart technology, such as virtual personal assistants. Also, Fluid Inc., which makes software designed to improve online shopping, is developing an app that makes product recommendations based on information provided through natural dialogue. So talking to a smart device, like an iPad, could deliver the same kind of experience a shopper would have with an in-store sales associate.
A bountiful mind
It is this cognitive capacity -- the ability to mimic the human brain, to learn and to understand in context and to be more assistant than tool -- that will revolutionize computing as we know it, IBM officials said.
That's according to IBM, of course. But Dan Miller, founder and senior analyst at Opus Research, doesn't disagree, calling the technology "transformative."
"IBM was out to demonstrate deep computing's ability to do things like understand recognize a topic quickly, discern irony and satire," he said. "It also helped uncover some of the constraints of the brute force approach. Scientists have long known that the answers get better and quicker if the systems address a specific topic or domain. And that's where IBM Watson is taking it."

In health care, for example, where the volume of data is doubling every five years, it could take a nurse 20 minutes to collect the data needed to make a treatment assessment. Cognitive technology, coupled with big data, is delivering that same evidence-based information in a matter of seconds.
"We know Watson makes us more efficient and is helping us turn around requests faster," Bigham said. "It also ensures we are consistent in our application of medical policies and guidelines."
IBM is also working with pharmaceutical companies to understand drug interactions. Using IBM's new Watson Discovery Advisor service, which makes connections across millions of articles, journals and studies, drug researchers can formulate conclusions that today can take months in just hours.  
Of course, cognitive technology is not a new concept. Artificial intelligence emerged as a hot topic in the 1960s, when computer scientists set out to build systems that were as intelligent as humans. The 1980s saw a flowering of "expert systems" from companies like Symbolics and Lisp Machines as well as the highly publicized development of a massively parallel processing supercomputer aimed at AI applications by Thinking Machines -- but those efforts quickly ran out of steam. Other supercomputer makers -- even IBM -- also dangled the idea of intelligent machines in front of government agencies and research laboratories, but their focus was on solving grand scientific problems, such as modeling the global climate and mapping the human genome. Those things didn't require cognitive capabilities, just colossal computing power.
Today, processing power and storage are not the big issues they once were. When Watson was introduced in 2011, it ran on 90 servers and 20 terabytes of disk. But the current system is 90% smaller and 20 times faster, said Steve Gold, IBM's vice president of marketing and sales operations for Watson systems.
And the cognitive technology has evolved as well.
Watson is an amalgamation of artificial intelligence, machine learning and natural language technologies. But it does not follow a logic-based set of rules as the supercomputers of the past did. Instead, it decomposes questions from natural language to understand the context of what is being asked. Then it analyzes the corpus of available information in research and articles and comes up with candidate answers. It is not deterministic; it is probabilistic -- producing a set of best answers with ranking and supporting evidence. For example, a simple question about the color of the sky depends on the circumstances: It could be blue, gray or white. Watson tries to comprehend the question and then uses thousands of algorithms to score the answer.
Consumers, too, are ready for intelligent systems. Conditioned by the prevalent use of intelligent personal assistant application, like Siri on Apple's iPhone, people expect computers to recognize natural language and respond to complex questions.
"What IBM is doing at the high end, as well as companies like Google and Apple that are working natural language understanding into machine learning, we, as humans, are being conditioned to feel more comfortable talking to some form of artificial intelligence," Miller said.
Miller expects there will be a ripple effect that will eventually bring high-end Watson-powered apps to the masses. "But it doesn't take a Watson-like investment to get an interface that is conversational and human-like in nature," he said
Watson isn't simple or inexpensive. While Bigham wouldn't disclose WellPoint's financial arrangement with IBM, the process of training Watson for use by the insurer includes reviewing the wording on every medical policy with IBM engineers, who define keywords to help Watson draw relationships between data. The nursing staff together with IBM engineers must keep feeding cases to Watson until it gets it. Teaching Watson about nasal surgery, for example, means going through policies and inputting definitions specific to the nose and conditions that affect it. Test cases then need to be created with all of the variations of what could happen and fed to Watson.
And things change, so it is an ongoing process. Bigham said the company can now teach Watson new things over a period of several weeks. It is a significant time and money investment, but WellPoint is bearing the brunt of the work to develop an affordable commercial app that it can license to other health insurance companies.
This painstaking process of training Watson is most likely the reason cognitive technology is not catching on like wildfire, Bigham said. According to Miller, there are subtler things at work.
"Generally, solutions like Watson are put in the 'emerging technologies' category, and the processes involved with building a business plan to make an investment in Watson are just now being defined," Miller said. So though cutting-edge technologies like cognitive aren't typically associated with objectives like return on investment, they are starting to be. 
Similarly, other companies are working on their own strategies with different forms of cognitive technology. Enterra Solutions, based in Newtown, Pa., has developed a cognitive reasoning platform that combines big data and artificial intelligence to find insights that can improve performance in areas like the supply chain and consumer marketing. Google, Facebook and Yahoo have all recently hired AI researchers or acquired startup vendors to lead machine learning development efforts. And NuLogix Labs, recently relocated from Princeton, N.J., to San Jose, Calif., is using a complex event processing technology, called the Cognitive Information Management Shell, to develop an agro-intelligence platform designed to help increase food supplies -- starting in India.  
Total recognition
The CIM Shell, developed at Louisiana State University, can drill down into complex events and activities to adapt rapidly to evolving situations. The work being done with a university in India is focused on increasing specific crop productivity by using sensors to collect data on ground activity and a synthesis program to dynamically reconfigure in real time to adjust to environmental changes. The agriculture app provides actionable intelligence to scientists who use it to direct experiments and inform decision making.
The technology can be put to use in other industries as well. Oil and gas company BP gave LSU a $250,000 grant to develop a prototype of the technology that could be used to help prevent future oil spills.
The CIM Shell's distributed intelligent agents fuse disparate streaming data, like text and video, to create an interactive sensing, inspection and visualization system that provides real-time monitoring and analysis. If there are any changes in data patterns -- temperature or pressure of equipment, for example -- it sends an alert, noted Dr. S.S. Iyengar, a computer science professor at Florida International University and chief scientist at NuLogix who co-invented the technology. It's not the first technology that detects changes in conditions -- abnormal situation management applications can, but they only flag things out of the ordinary. The CIM Shell not only sends an alert but reconfigures on the fly in order to isolate a critical event and fix the failure.
"The goal of CIM is that nobody should have to write the program," said co-inventor Supratik Mukhopadhyay, an assistant professor in the department of computer science at LSU. "You tell the computer what it needs to do and it writes a program itself that will solve the problem in real time."
The human factor
The CIM Shell and Watson take different approaches to understanding complex events, but they both are built to respond, learn and continue processing, just like the human brain.
What these cognitive systems can't do is analyze the risk that might not be represented anywhere in the unstructured data. That includes factoring in cultures, environments, people and accountability.
"You have to be able to analyze risk," said Jose Bravo, a chief scientist at oil company Shell Global. Shell is looking at big data systems and is considering a variety of artificial intelligence products, but, according to Bravo, there are still limitations to what a deep learning machine can do. For example, if a predictive model says buy oil in the Middle East, but a leader in the region is at risk of being deposed by a revolution, it must be factored into the decision. "If you could predict how the future will develop, that would be great, but you can't," Bravo said. "And you can't hold a machine accountable if it makes a disastrous decision."
But that's why there will always be a human element in the cognitive machine mix. At WellPoint, the staff ultimately chooses whether to accept Watson's recommendations. The value of Watson is the speed, efficiency and consistency for responding to a doctor's request and for complying with medical policies and guidelines, Bigham said. For the doctor's office, users are typing a natural language question into a browser on demand. There is no calling and waiting to submit a request. And every day Watson gets smarter, drawing connections between concepts based on things it's already learned. 
As time goes on, Opus Research's Miller predicted, cognitive technology will evolve for the masses, getting less expensive and easier to use. So Watson and other artificial intelligence systems won't fade away like the cognitive fads of the past, Bigham said. "In my opinion, this is one of the next big things."


TECH:


	2
	
Machines like IBM's Watson are becoming as smart as we are. And soon the technology may become elementary to many more than just Big
The health care industry suffers from an ailment known as information overload. It's a condition that makes it difficult for doctors and insurance providers to quickly determine the procedures that will be required and covered for each patient. The individual's history, years of case files and clinical evidence must be considered -- no small feat for a human. But it's as easy as saying "aah" for Watson -- IBM's artificial intelligence computer system that processes natural language questions against a deep well of data to compute evidence-based answers in a matter of seconds.
Watson can sift through the data equivalent of about 1 million books, analyze the information and provide precise responses to complicated questions in less than three seconds. Insurance provider WellPoint Inc. doesn't have to imagine what that could mean to the health care industry. Through a partnership with IBM that began two years ago, the Indianapolis-based company is working with more than 3,000 physician offices in its network to provide patient coverage and treatment options almost instantaneously.
"We trained Watson to think like a nurse or physician on staff," said Elizabeth Bigham, WellPoint's vice president of health IT strategy. "It receives requests from providers, finds medical policies, compares what the provider said in the request, determines who the patient is, what they want to do and why, and renders a recommendation to our staff."
Watson, Bigham said, is a game changer in the medical field. Other companies are also developing Watson-powered applications for health care providers and consumers -- for example, software startup Welltok Inc.'s CafeWell Concierge, which will offer personalized, location-based guidance on diet, exercise and preventive services.
And it's not just health care. In January, IBM launched a Watson Group business unit to ignite new commercialization efforts in a range of industries, including financial services, travel, telecom and retail. To help fuel its efforts, the new unit was given $100 million to invest in third-party software developers; it made an initial investment in Welltok in February. IBM also is making Watson services available in the cloud and providing software developers with access to its Watson application programming interface to build new kinds of cognitive apps. Welltok's project is one of those efforts; another is a Watson-powered "smart adviser" self-service applications, which can understand natural language, read and interpret text and learn from other types of smart technology, such as virtual personal assistants. Also, Fluid Inc., which makes software designed to improve online shopping, is developing an app that makes product recommendations based on information provided through natural dialogue. So talking to a smart device, like an iPad, could deliver the same kind of experience a shopper would have with an in-store sales associate.
A bountiful mind
It is this cognitive capacity -- the ability to mimic the human brain, to learn and to understand in context and to be more assistant than tool -- that will revolutionize computing as we know it, IBM officials said.
That's according to IBM, of course. But Dan Miller, founder and senior analyst at Opus Research, doesn't disagree, calling the technology "transformative."
"IBM was out to demonstrate deep computing's ability to do things like understand recognize a topic quickly, discern irony and satire," he said. "It also helped uncover some of the constraints of the brute force approach. Scientists have long known that the answers get better and quicker if the systems address a specific topic or domain. And that's where IBM Watson is taking it."

In health care, for example, where the volume of data is doubling every five years, it could take a nurse 20 minutes to collect the data needed to make a treatment assessment. Cognitive technology, coupled with big data, is delivering that same evidence-based information in a matter of seconds.
"We know Watson makes us more efficient and is helping us turn around requests faster," Bigham said. "It also ensures we are consistent in our application of medical policies and guidelines."
IBM is also working with pharmaceutical companies to understand drug interactions. Using IBM's new Watson Discovery Advisor service, which makes connections across millions of articles, journals and studies, drug researchers can formulate conclusions that today can take months in just hours.  
Of course, cognitive technology is not a new concept. Artificial intelligence emerged as a hot topic in the 1960s, when computer scientists set out to build systems that were as intelligent as humans. The 1980s saw a flowering of "expert systems" from companies like Symbolics and Lisp Machines as well as the highly publicized development of a massively parallel processing supercomputer aimed at AI applications by Thinking Machines -- but those efforts quickly ran out of steam. Other supercomputer makers -- even IBM -- also dangled the idea of intelligent machines in front of government agencies and research laboratories, but their focus was on solving grand scientific problems, such as modeling the global climate and mapping the human genome. Those things didn't require cognitive capabilities, just colossal computing power.
Today, processing power and storage are not the big issues they once were. When Watson was introduced in 2011, it ran on 90 servers and 20 terabytes of disk. But the current system is 90% smaller and 20 times faster, said Steve Gold, IBM's vice president of marketing and sales operations for Watson systems.
And the cognitive technology has evolved as well.
Watson is an amalgamation of artificial intelligence, machine learning and natural language technologies. But it does not follow a logic-based set of rules as the supercomputers of the past did. Instead, it decomposes questions from natural language to understand the context of what is being asked. Then it analyzes the corpus of available information in research and articles and comes up with candidate answers. It is not deterministic; it is probabilistic -- producing a set of best answers with ranking and supporting evidence. For example, a simple question about the color of the sky depends on the circumstances: It could be blue, gray or white. Watson tries to comprehend the question and then uses thousands of algorithms to score the answer.
Consumers, too, are ready for intelligent systems. Conditioned by the prevalent use of intelligent personal assistant application, like Siri on Apple's iPhone, people expect computers to recognize natural language and respond to complex questions.
"What IBM is doing at the high end, as well as companies like Google and Apple that are working natural language understanding into machine learning, we, as humans, are being conditioned to feel more comfortable talking to some form of artificial intelligence," Miller said.
Miller expects there will be a ripple effect that will eventually bring high-end Watson-powered apps to the masses. "But it doesn't take a Watson-like investment to get an interface that is conversational and human-like in nature," he said.
More on cognitive computing
Podcast: A crash course on cognitive computing
Cognitive computing may serve in governance, risk and compliance
IBM's $1b dollar investment in cognitive computing
Watson isn't simple or inexpensive. While Bigham wouldn't disclose WellPoint's financial arrangement with IBM, the process of training Watson for use by the insurer includes reviewing the wording on every medical policy with IBM engineers, who define keywords to help Watson draw relationships between data. The nursing staff together with IBM engineers must keep feeding cases to Watson until it gets it. Teaching Watson about nasal surgery, for example, means going through policies and inputting definitions specific to the nose and conditions that affect it. Test cases then need to be created with all of the variations of what could happen and fed to Watson.
And things change, so it is an ongoing process. Bigham said the company can now teach Watson new things over a period of several weeks. It is a significant time and money investment, but WellPoint is bearing the brunt of the work to develop an affordable commercial app that it can license to other health insurance companies.
This painstaking process of training Watson is most likely the reason cognitive technology is not catching on like wildfire, Bigham said. According to Miller, there are subtler things at work.
"Generally, solutions like Watson are put in the 'emerging technologies' category, and the processes involved with building a business plan to make an investment in Watson are just now being defined," Miller said. So though cutting-edge technologies like cognitive aren't typically associated with objectives like return on investment, they are starting to be. 
Similarly, other companies are working on their own strategies with different forms of cognitive technology. Enterra Solutions, based in Newtown, Pa., has developed a cognitive reasoning platform that combines big data and artificial intelligence to find insights that can improve performance in areas like the supply chain and consumer marketing. Google, Facebook and Yahoo have all recently hired AI researchers or acquired startup vendors to lead machine learning development efforts. And NuLogix Labs, recently relocated from Princeton, N.J., to San Jose, Calif., is using a complex event processing technology, called the Cognitive Information Management Shell, to develop an agro-intelligence platform designed to help increase food supplies -- starting in India.  
Total recognition
The CIM Shell, developed at Louisiana State University, can drill down into complex events and activities to adapt rapidly to evolving situations. The work being done with a university in India is focused on increasing specific crop productivity by using sensors to collect data on ground activity and a synthesis program to dynamically reconfigure in real time to adjust to environmental changes. The agriculture app provides actionable intelligence to scientists who use it to direct experiments and inform decision making.
The technology can be put to use in other industries as well. Oil and gas company BP gave LSU a $250,000 grant to develop a prototype of the technology that could be used to help prevent future oil spills.
The CIM Shell's distributed intelligent agents fuse disparate streaming data, like text and video, to create an interactive sensing, inspection and visualization system that provides real-time monitoring and analysis. If there are any changes in data patterns -- temperature or pressure of equipment, for example -- it sends an alert, noted Dr. S.S. Iyengar, a computer science professor at Florida International University and chief scientist at NuLogix who co-invented the technology. It's not the first technology that detects changes in conditions -- abnormal situation management applications can, but they only flag things out of the ordinary. The CIM Shell not only sends an alert but reconfigures on the fly in order to isolate a critical event and fix the failure.
"The goal of CIM is that nobody should have to write the program," said co-inventor Supratik Mukhopadhyay, an assistant professor in the department of computer science at LSU. "You tell the computer what it needs to do and it writes a program itself that will solve the problem in real time."
The human factor
The CIM Shell and Watson take different approaches to understanding complex events, but they both are built to respond, learn and continue processing, just like the human brain.
What these cognitive systems can't do is analyze the risk that might not be represented anywhere in the unstructured data. That includes factoring in cultures, environments, people and accountability.
"You have to be able to analyze risk," said Jose Bravo, a chief scientist at oil company Shell Global. Shell is looking at big data systems and is considering a variety of artificial intelligence products, but, according to Bravo, there are still limitations to what a deep learning machine can do. For example, if a predictive model says buy oil in the Middle East, but a leader in the region is at risk of being deposed by a revolution, it must be factored into the decision. "If you could predict how the future will develop, that would be great, but you can't," Bravo said. "And you can't hold a machine accountable if it makes a disastrous decision."
But that's why there will always be a human element in the cognitive machine mix. At WellPoint, the staff ultimately chooses whether to accept Watson's recommendations. The value of Watson is the speed, efficiency and consistency for responding to a doctor's request and for complying with medical policies and guidelines, Bigham said. For the doctor's office, users are typing a natural language question into a browser on demand. There is no calling and waiting to submit a request. And every day Watson gets smarter, drawing connections between concepts based on things it's already learned. 
As time goes on, Opus Research's Miller predicted, cognitive technology will evolve for the masses, getting less expensive and easier to use. So Watson and other artificial intelligence systems won't fade away like the cognitive fads of the past, Bigham said. "In my opinion, this is one of the next big things."


TECH:

How Cognitive Computing can Revolutionize Businesses

	
	
Cognitive Computing simply means tapping into the power of a computing system that can work smarter than us. It is the next step forward in digital evolution where we move further from programmatic computing to self-learning systems.
In the initial days, tabulating data was regarded as computing. Then came the age of programming languages with which software programs were written and deployed.
Now, cognitive computing is pushing the envelope towards an era where systems can learn, think, decide and act on their own without human intervention.
The Brain Power that Propels Cognitive Computing
Cognitive Computing borrows its brain power from Machine Learning algorithms and Artificial Intelligence. Using these technologies, they learn continuously from data received as input during the regular course of operations.
Pattern recognition systems, Natural Language Processing, Data Mining add to the efficiency of these systems. With intelligence that is continuously built up, they are able to foresee patterns and arrive at proactive decisions by anticipating problems and deriving possible solutions. With time, they become completely autonomous and can handle operations without human interference, thus, making real the much awaited future of complete automation.
Cognitive Computing Systems (CCSs) will forge a new winning partnership between machines and humans that will cut costs and improve service models.
It will help:
	Enhance the current level of efficiency by quickening decision making
	Scale the quantum of processes rapidly and consistently
	Accelerate the level of performance by capturing knowledge from real-world
This automation can contribute massive reforms to existing businesses practices that are error-prone or inefficient.
91 percent of retail industry executives familiar with cognitive computing believe it will play a disruptive role in the industry, and 94 percent are likely to invest in cognitive capabilities in the near future. IBM Thinking Like a Customer Report.
Industries where cognitive computing can add machine intelligence include:
	Health Care - Deep analytics of historical patient data and clinical workflow
	Retail - Analysing customer behaviour and refining product suggestions at POS
	Finance - Suggesting stock market movements based on social media behaviour
	Customer service - Chatbots with Artificial Intelligence & Machine Learning that treat customers with proactive suggestions
	Insurance - Usage based insurance, faster claim processing, historical data based underwriting, etc.
	Manufacturing - solves everyday manufacturing challenges ranging from managing production cycles, equipment maintenance to labor safety.
#1. Cognitive Healthcare
Hand written notes, long gestation period to identify disease symptoms, and lack of information have remained primary causes that cripple the efficiency of healthcare professionals. Cognitive Computing can sweep away all those inefficiencies in a single shot by providing insightful information that was impossible with programmatic computing.
Proactive Diagnosis
Cognitive computing can give a computerized approach to search past patient records, prescriptions and disease statistics to predict disease patterns. Big Data analytics helps in improving clinical workflows thus improving healthcare services on a global scale.
Real-life example:
Researchers at University of California, Los Angeles (UCLA), were able to quickly identify people with changes of diabetes by mining thousands of patient records in digital form. The data mining also revealed patterns which helped identify the chances of disease patterns that were previously unknown.
#2. Cognitive Retail
Omnichannel Retail will be one of the biggest benefactors of cognitive computing. From analyzing and understanding fluctuating customer behaviour to forecasting inventory and making possible smart shelves, cognitive retail will change the way business is conducted in the retail industry.
Omni-channel Intelligence
Cognitive retail will help retailers to conquer their customers through providing services that help customers find the right product when they want it and where they want it. It will bring about a paradigm shift in omnichannel eCommerce where data from multiple points like warehouses, vendors, logistics, customer point of sale, etc. should be analyzed and broken down into meaningful information on a real-time basis.
Real-life examples:
Coop Danmark was able to profit by using Cognitive abilities to identify SKUs that can be marked down for a specific period of time for maximum profitability. Similar to Coop Danmark, Lindt - the global chocolate brand and City Beach - the Australian based retailer were also able to boost their retail administration prowess using Cognitive Computing.
#3. Cognitive Finance
The finance industry is one of the toughest industries to decipher real-time information. Currency exchange rates, stock market indices, valuation norms, stock values - there is a massive volume of information which an investor has to digest before making a rational decision.
Predictive Analysis of  Markets
With the coming of cognitive finance, investors, CFOs and everybody else relating to BFSI (Banking, Financial Services and Insurance) industry will be able to keep a steady pace with changing financial environment. Thus, it will reduce the amount of risk taken in a financially volatile economy and help improve business profitability.
 Some practical applications that will 
Real-life example:
Trump and Dump bot is a computer program that will estimate the increase or decrease in a companys stock value based on President Donald Trumps tweets. Although the idea might seem a bit too far fetched, it is already proven to be realistic.
For instance, on 5th Jan the President tweeted a criticism against Toyotas plan to build a plant in Mexico. Toyotas stocks fell by 3.1% immediately after the tweet.
#4. Cognitive Customer Service
Routine and uncertainty alternate wildly in the customer service domain. Customer service agents have to stay updated with the product changes, understand customer perspective and deliver assistance without letting human inefficiencies get in the way. At the same time, the cost should also be maintained at a bare minimum to upkeep business profitability.
Chatbots and Self-service
Cognitive customer service helps achieve all these objectives by providing a cognition based conversation based self-service for customers. Chatbots, driven by conversational technology, is a classic example of such automated customer service. Just like other technology trends, conversational technology will give a new impetus to the Ecommerce sector.
Chatbots with pre-trained customer service skills and industry skills can provide the same, if not better customer service than humans. They offer a personalized shopping experience online, just like how a salesperson in a brick and mortar textile store aid you in selecting the products of your choice. Secondly, chatbots are also empowered with analytic skills and machine learning which improves their efficiency with time.
Real-life example:
VentureBeat surveyed the globe for popular chatbots and zeroed in on some chatbots that were surpassing human customer service.
Instalocate is one such chatbot. Instalocate makes lives easier for travelers by simplifying the task of journey-tracking, resolving compensation issues, fetching best deals from the Internet and so on. These tasks otherwise require numerous calls to the customer service, which often does not yield the best results.
#5. Cognitive Insurance
Cognitive computing in insurance industry is helping insurers like Metlife, USAA, etc. to reduce risks in underwriting, inaccuracies in insurance valuation and reduce claim costs. Predictive capabilities of cognitive computing helps estimate futuristic claim amounts with accuracy based on which financial provisions can be made.
Data-driven Business Models
Data-driven models like Usage Based Insurance are also expected to become mainstream in the near future with the help of cognitive computing. Forbes expects the insurance scene in the United States to improve a lot where legal and claim documents can be closely linked to State law and order using cognitive computing helping insurers assess risk and estimate premiums easily.
Real-life example:
USAA was able to accelerate its policy approval process rapidly based on inputs like applicants past clinical data and current medical policies and guidelines. the USAA deployed IBM Watson to check whether a policy application deserved approval or needs to be rejected if it is not in tune with the policies.
#6. Cognitive Manufacturing
Cognitive Manufacturing will heighten the level of interaction between manufacturing equipments and humans. CCS will help workers to know real-time the vital stats of equipment that are difficult to inspect from close quarters or are spread across a large area.
Some emerging applications of Cognitive Computing in manufacturing include:
	Deep search for critical patterns to predict downtimes
	Robotic technicians that can access past data for improving repair quality
	Weather, logistics, context data-driven parts planning
Combined with Internet of Things and the data streams that it creates, Cognitive Computing can deliver data-based strategic inputs for executives to maximize their manufacturing productivity.



